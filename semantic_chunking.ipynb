{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c5d882",
   "metadata": {},
   "source": [
    "Hugging face maintains a central hub of datasets, can load popular datasets like IMDB\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "Access to evaluation metrics\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "print(metric.compute(preictions=[0,1,0], references = [0,1,1]))\n",
    "\n",
    "\n",
    "works with pytorch\n",
    "converts datasets into  torch.util.data.Dataset\n",
    "tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90f90a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54d3376d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'content', 'references'],\n",
       "    num_rows: 2673\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"jamescalam/ai-arxiv2\",split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcfdddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "encoder = OpenAIEncoder(name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14485d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2 0 2\n",
      "c e D 8 2 ] G L . s c [\n",
      "1 v 8 3 2 7 1 . 2 1 3 2 : v i X r a\n",
      "# Fast Inference of Mixture-of-Experts Language Models with Offloading\n",
      "Artyom Eliseev Moscow Institute of Physics and Technology Yandex School of Data Analysis lavawolfiee@gmail.com\n",
      "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n",
      "# Abstract\n",
      "With the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n",
      "# Introduction\n",
      "Many recent advances in natural language processing rely on large pre-trained language models, such as GPT-3 and 4 Brown et al. (2020); OpenAI (2023), Palm & Gemini Chowdhery et al. (2022); Team et al. (2023) and many others. However, the rapid scientific progress in this area would be impossible without open-access LLMs such as LLaMA 1 and 2 (Touvron et al., 2023), Falcon (TII UAE, 2023), BLOOM (Scao et al., 2022), OPT (Zhang et al., 2022), or NeoX/Pythia (Biderman et al., 2023). The key advantage of open-access LLMs is that researchers can deploy them locally and modify them in ways that would be impossible with proprietary APIs.\n",
      "Even though LLM parameters are openly available, it is still difficult to use these models due to their sheer size. State-of-the-art open-access language models require multiple high-end GPUs 1 even for basic inference workloads. To use these LLMs on more affordable hardware setups, one must either compress model parameters (Dettmers et al., 2022; Frantar et al., 2022) or offload parameters to a cheaper storage, be it RAM or SSD (Pudipeddi et al., 2020; Sheng et al., 2023).\n",
      "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n",
      "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\n",
      "In this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\n",
      "we observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\n",
      "â¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2\n",
      "# 2 Background & Related Work\n",
      "# 2.1 Mixture-of-Experts\n",
      "The recent surge in MoE language models builds on a relatively old idea (Jacobs et al., 1991; Jordan & Jacobs, 1994) of training ensembles of specialized models (âexpertsâ) and a gating function to select the right expert for the task. To achieve specialization, Mixture-of-Experts learn by simultaneously i) training the gating function to choose the best experts and ii) training the experts themselves on samples assigned to them by the gating function. Since then, many different MoE variants emerged, including mixture of SVM models (Collobert et al., 2002), Dirichlet processes (Shahbaba & Neal, 2009) and various neural networks.\n",
      "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).\n",
      "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n",
      "# 2.2 Post-training Quantization of LLMs\n",
      "A natural way to circumvent this is to reduce the model size through quantization (Nagel et al., 2020; Gholami et al., 2021; Frantar et al., 2022), sparsification Frantar & Alistarh (2023a); Ma et al. (2023),\n",
      "2https://github.com/dvmazur/mixtral-offloading 3https://huggingface.co/google/switch-c-2048, released in November 15th, 2022\n",
      "2\n",
      "factorization Hsu et al. (2022), or a combination thereof. These compression types are not specific to LLMs and are based on much older methods outside the scope of our work4. However, recent works found that there are unique challenges to quantizing very large transformer-based language models due to emergent outliersDettmers et al. (2022); Lin et al. (2023); Dettmers et al. (2023).\n",
      "Generally speaking, the optimal compression rate for most LLMs is 4 bits per parameter Dettmers & Zettlemoyer (2022). While there are more extreme algorithms for 3- and even 2-bit compression Chee et al. (2023); Lin et al. (2023); Dettmers et al. (2023), they are typically inferior to choosing a smaller model and quantizing it to around 4 bits. Most recently, there has been several concurrent works for quantizing Mixture-of-Experts models (Kim et al., 2023; Frantar & Alistarh, 2023b).\n",
      "# Inference with Parameter Offloading\n",
      "A recent line of work explores inferencing and training large models with limited accelerator mem- ory by âoffloadingâ their parameters to another, cheaper memory, such as system RAM or even SSD (Pudipeddi et al., 2020; Ren et al., 2021). This technique works by loading model parameters just-in-time when they are needed for computation. Since most deep learning models use layers in a fixed order, offloading can pre-dispatch the next layer parameters in the background, ahead of time.\n",
      "This technique works particularly well when processing large batches of data, during train- ing Pudipeddi et al. (2020); Ren et al. (2021) or large-batch non-interactive inference Aminabadi et al. (2022); Sheng et al. (2023), where each layer processes a lot of tokens each time the layer is loaded from RAM. In turn, when doing interactive inference (e.g. as a chat assistants), offloading works significantly slower than on-device inference. This is because interactive inference generates tokens autoregressively, from left to right. This way, the inference system processes one or few tokens at a time, and therefore spends most of the time waiting for next layerâs parameters to be loaded.\n",
      "# 2.4 Hardware Setup\n",
      "While our analysis is not specific to any hardware setup, we target the hardware specifications of cheap / free-tier cloud instances Google (2023) and the upper half of gaming computers Steam (2023): i) enough system memory to hold model parameters, ii) a GPU with 11-16GB VRAM and iii) host-to-device communication at 8-16GB/s (PCIe Gen.3). If we examine popular open-access MoE models (Mixtral-8x7B and switch-c-2048), we find that all non-experts can fit a fraction of available GPU memory. In turn, the experts that constitute vast majority of model parameters do not fit even with quantization. Finally, even if we could fit the model parameters in memory, running generative inference requires additional memory for layer activations and past attention keys & values.\n",
      "# 3 Method\n",
      "In this work, we aim to systematically find the optimal way to inference modern Mixture-of-Experts LLMs on desktop or low-end cloud instances. More specifically, we focus on the task of generating tokens interactively, i.e. generate multiple tokens per second at batch size 15.\n",
      "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\n",
      "Below, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n",
      "4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n",
      "3\n",
      "Selected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture\n",
      "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n",
      "# 3.1 Expert Locality and LRU caching\n",
      "As we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\n",
      "To take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.\n",
      "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n",
      "# 3.2 Speculative Expert Loading\n",
      "While LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.\n",
      "For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads the next transformer layer ahead of time, while the previous layer is still running. Unfortunately, this schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose which experts to load just-in-time for computation. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. Modern open-access MoE language models choose active experts using the final outputs of the previous layer, which means they cannot be pre-fetched them in parallel with previous layer. While it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could still try to guess the likely next experts and load them speculatively, while processing the previous layer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual next layerâs experts later. In other words, this type of speculative loading does not change the final model predictions, but may reduce latency if the guess is accurate enough.\n",
      "6More specifically, not possible without changing the model architecture, which would require re-training\n",
      "4\n",
      "While analyzing modern MoE models, we found that it is possible to get an accurate guess of next layerâs experts by applying next layerâs gating function to previous layerâs hidden states â or, more specifically, to the same hidden states that are used by previous MoE layerâs gating function. This heuristic relies on the fact that transformer layers are residual, i.e. each layer adds to the previous hidden states instead of re-computing them from scratch. This architecture introduces an inductive bias such that any layerâs hidden states into a decent estimate of next layerâs hidden states.\n",
      "# 3.3 System Design & Implementation Details\n",
      "In this section, we describe practical design considerations and implementation details that we used for inferencing MoE language models on consumer and low-end cloud hardware. Our system design combines the caching & prefetching techniques and a mixed MoE quantization scheme .\n",
      "MoE quantization. As we described earlier in Section 2.2, there are multiple weight quantization algorithms optimized for LLMs. Model compression has a natural synergy with offloading because compressed models take less time to load onto GPU. In our experitments, we also observed that MoE models get better quality-size trade-offs when quantizing experts to a lower bitwidth, while keeping all non-expert layers at 4-bit.\n",
      "We use Half Quadratic Quantization (HQQ) (Badri & Shaji, 2023) â a data-free quantization algorithm that supports a variety of bit rates. However, we chose this algorithm only for convenience, because it was already well tested for Mixtral models. Since our analysis does not rely on any specific choice of quantization, we believe that if we chose another quantization algorithm (e.g. GPTQ or AWQ) our conclusions would be similar. In our early experiments, we also tried the sub-1-bit quantization from QMoE Frantar & Alistarh (2023b) that worked well on the Switch-c-2048 model. However, we found that sub-1-bit compression caused too significant a loss in perplexity for Mixtral-8x7B models.\n",
      "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\n",
      "Many consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.\n",
      "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n",
      "# 4 Experiments\n",
      "In this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.\n",
      "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n",
      "5\n",
      "iy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\n",
      "Figure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n",
      "# 4.1 Expert LRU Cache and Speculative Loading\n",
      "In this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.\n",
      "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n",
      "# 4.2 Mixed MoE Quantization\n",
      "Next, we test how different Quantization schemes affect MoE performance and size. We also use Mixtral-8x7B, but this time, we use non-instruction-tuned variant since it fits better with the available benchmarks. We measure WikiText2 perpliexity Merity et al. (2016), C4 perplexity Raffel et al. (2020), as well as 5-shot MMLU accuracy Hendrycks et al. (2021). Our objective for this section is to find the best trade off between size and performance for offloading with the target setups. Note that out of 46.7B total parameters in the Mixtral-8x7B model, the experts constitute 45.1B (96.6%). The rest of the model parameters are allocated to embeddings, self-attention layers, MoE gates and minor layers such as LayerNorm.\n",
      "Experts quant Model size, GB Wiki2 C4 MMLU Attn quant Experts quant Model size, GB FP16 4-bit 3-bit 2-bit 86.99 25.82 23.21 19.33 3.59 3.67 3.96 4.52 6.52 70.51% 6.58 70.3% 6.78 69.32% 7.31 66.66% 3-bit FP16 4-bit 3-bit 2-bit 85.08 23.92 21.31 17.46 3.99 4.06 4.34 4.90 FP16 4-bit 3-bit 2-bit 85.16 23.99 21.37 17.54 3.68 3.76 4.05 4.61 6.59 â 6.66 69.11% 6.87 68.47% 7.42 65.58% 2-bit FP16 4-bit 3-bit 2-bit 84.96 23.79 21.18 17.30 4.98 5.08 5.36 5.97\n",
      "Table 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared attention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit quantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section 4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\n",
      "6\n",
      "Algorithm 2-bit Experts 3-bit Experts A100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud) 3.061 Full algorithm 2.918 W/o expert pre-loading 2.265 W/o LRU cache & pre-loading Naive offloading (accelerate) 1.392 2.655 2.227 1.758 1.059 2.278 2.051 1.547 0.919 2.092 1.567 1.168 0.661 2.845 2.683 2.055 1.246 2.475 2.024 1.595 0.914 2.038 1.857 1.346 1.791 1.603 1.365 1.061 0.580\n",
      "Table 2: Inference speed for Mixtral-8x7B in low-tier , measured in tokens per second.\n",
      "As discussed earlier, we use HQQ Badri & Shaji (2023) data-free quantization algorithm and consider the following quantization schemes:\n",
      "1. FP16 (no quantization) 2. HQQ 4-bit with group size 64, scale group size 256 3. HQQ 3-bit with group size 64, scale group size 128 4. HQQ 2-bit with group size 16, scale group size 128\n",
      "Note that the actual model size with n-bit quantization is larger than n bits per parameter. This is because the quantized data format also stores quantization scale and zero point for each group of weights. Notably, the above 2-bit quantization scheme uses, on average, 2.6 bits per parameter due to a large number of quantization schemes. We also keep embeddings, logits, MoE gates and normalization layers in 16-bit format.\n",
      "Table 1 summarizes our results: overall, it seems advantageous to quantize experts to 3 or 2 bits while keeping attention layers to a higher bitwidth (16 or 4 bits). Based on these evaluations, we chose two quantization schemes (highlighted in green) that offer favourable performance-size trade-offs within the target hardware constraints.\n",
      "# 4.3 Practical offloading performance\n",
      "Finally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech- niquesproposed throughout this report. Based on the perplexity evaluations from the previous section, we chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for experts. We evaluate this system by generating tokens via sampling on OpenAssistant (KÃ¶pf et al., 2023) conversations and measuring the average number of tokens generated per second with batch size 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without temperature or nucleus sampling.\n",
      "We consider four hardware configurations: a free-tier Colab instance with a T4 GPU (16GB VRAM, PCIe Gen.3), a past generation gaming laptop with RTX 3080 Mobile (16GB, PCIe Gen.4), a mid- range gaming desktop with RTX 3060 (12GB, PCIe Gen.3) and a high-end data-center server with A100-80GB-SXM. Note that the A100 server could run the model without offloading. We use offloading on A100 mostly to provide a reference for other setups. Finally, when evaluating 3-bit models, we use a cloud T4 from Microsoft Azure because the free-tier colab instances did not have enough RAM for this specific configuration. We use k = 2 for RTX 3060 and k = 4 for all other GPUs.\n",
      "As shown in Table 2, all evaluated setups can generate 2-4 tokens per second with the full algorithm. Using pre-loading appears to be most beneficial on RTX 3060, possibly due to lower LRU cache size. Cursiously, RTX 3060 (desktop) performs nearly equally with a much higher end 3080 Mobile. We attribute this to the fact that both GPUs are still bottlenecked by host-to-device bandwidth, limited by the PCIe architecture. Finally, all schemes significantly outperform naive offloading that loads the entire MoE layer.\n",
      "# 5 Conclusion and Future Work\n",
      "In this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n",
      "7\n",
      "and explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\n",
      "Our method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n",
      "# Acknowledgements\n",
      "Authors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n",
      "# References\n",
      "Aminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.\n",
      "Badri, H. and Shaji, A. Half-quadratic quantization of large machine learning models, November 2023. URL https://mobiusml.github.io/hqq_blog/.\n",
      "Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., OâBrien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.\n",
      "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In Conference on Neural Information Processing Systems (NeurIPS), 2020.\n",
      "Chee, J., Cai, Y., Kuleshov, V., and Sa, C. D. Quip: 2-bit quantization of large language models with guarantees, 2023.\n",
      "Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n",
      "Collobert, R., Bengio, S., and Bengio, Y. A parallel mixture of svms for very large scale problems. In Advances in Neural Information Processing Systems, pp. 633â640, 2002.\n",
      "Dettmers, T. and Zettlemoyer, L. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.\n",
      "Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.\n",
      "Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. Spqr: A sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023.\n",
      "Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., Zoph, B., Fedus, L., Bosma, M., Zhou, Z., Wang, T., Wang, Y. E., Webster, K., Pellat, M., Robinson, K., Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q. V., Wu, Y., Chen, Z., and Cui, C. Glam: Efficient scaling of language models with mixture-of-experts, 2022.\n",
      "Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.\n",
      "8\n",
      "Frantar, E. and Alistarh, D. SparseGPT: Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a.\n",
      "Frantar, E. and Alistarh, D. Qmoe: Practical sub-1-bit compression of trillion-parameter models, 2023b.\n",
      "Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n",
      "Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.\n",
      "Google. Google colaboratory, 2023. URL https://colab.research.google.com/.\n",
      "Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021.\n",
      "Hsu, Y.-C., Hua, T., Chang, S., Lou, Q., Shen, Y., and Jin, H. Language model compression with weighted low-rank factorization. arXiv preprint arXiv:2207.00112, 2022.\n",
      "Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural Computation, 3(1):79â87, March 1991. ISSN 0899-7667. doi: 10.1162/neco.1991.3.1.79. URL https://doi.org/10.1162/neco.1991.3.1.79.\n",
      "Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181â214, 1994.\n",
      "Kim, Y. J., Fahim, R., and Awadalla, H. H. Mixture of quantized experts (moqe): Complementary effect of low-bit quantization and robustness, 2023.\n",
      "KÃ¶pf, A., Kilcher, Y., von RÃ¼tte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and Mattick, A. Openassistant conversations â democratizing large language model alignment, 2023.\n",
      "Lample, G., Sablayrolles, A., Ranzato, M. A., Denoyer, L., and Jegou, H. Large memory layers with product keys. In Wallach, H., Larochelle, H., Beygelzimer, A., dÃlchÃ©-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 8546â8557. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9061-large-memory-layers- with-product-keys.pdf.\n",
      "Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n",
      "Lewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettlemoyer, L. Base layers: Simplifying training of large, sparse models. arXiv preprint arXiv:2103.16716, 2021.\n",
      "Liang, T., Glossner, J., Wang, L., and Shi, S. Pruning and quantization for deep neural network accel- eration: A survey. CoRR, abs/2101.09671, 2021. URL https://arxiv.org/abs/2101.09671.\n",
      "Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\n",
      "Ma, X., Fang, G., and Wang, X. Llm-pruner: On the structural pruning of large language models, 2023.\n",
      "Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.\n",
      "Mixtral AI team. Mixtral of experts a high quality sparse mixture of experts, 2023. URL https: //mistral.ai/news/mixtral-of-experts/.\n",
      "9\n",
      "Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020.\n",
      "OpenAI. Gpt-4 technical report. arXiv, 2023.\n",
      "Pudipeddi, B., Mesmakhosroshahi, M., Xi, J., and Bharadwaj, S. Training large neural networks with constant memory using a new execution algorithm. CoRR, abs/2002.05645, 2020. URL https://arxiv.org/abs/2002.05645.\n",
      "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1â67, 2020.\n",
      "Ren, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang, S., Zhang, M., Li, D., and He, Y. Zero-offload: Democratizing billion-scale model training. CoRR, abs/2101.06840, 2021. URL https://arxiv.org/abs/2101.06840.\n",
      "Scao, T. L., Fan, A., Akiki, C., Pavlick, E., IliÂ´c, S., Hesslow, D., CastagnÃ©, R., Luccioni, A. S., Yvon, F., GallÃ©, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n",
      "Shahbaba, B. and Neal, R. Nonlinear models using dirichlet process mixtures. Journal of Machine Learning Research, 10(Aug):1829â1850, 2009.\n",
      "Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outra- geously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\n",
      "Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen, B., Liang, P., RÃ©, C., Stoica, I., and Zhang, C. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, pp. 31094â31116. PMLR, 2023.\n",
      "Steam. Steam hardware & software survey: October 2023, accessed on 2023.11.02, 2023. URL https://store.steampowered.com/hwsurvey/videocard/.\n",
      "Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T., Lazaridou, A., Firat, O., Molloy, J., Isard, M., Barham, P. R., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E., Meyer, C., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Tucker, G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati, L., Kazemi, M., Gonzalez, L., Khalman, M., Sygnowski, J., Frechette, A., Smith, C., Culp, L., Proleev, L., Luan, Y., Chen, X., Lottes, J., Schucher, N., Lebron, F., Rrustemi, A., Clay, N., Crone, P., Kocisky, T., Zhao, J., Perz, B., Yu, D., Howard, H., Bloniarz, A., Rae, J. W., Lu, H., Sifre, L., Maggioni, M., Alcober, F., Garrette, D., Barnes, M., Thakoor, S., Austin, J., Barth-Maron, G., Wong, W., Joshi, R., Chaabouni, R., Fatiha, D., Ahuja, A., Liu, R., Li, Y., Cogan, S., Chen, J., Jia, C., Gu, C., Zhang, Q., Grimstad, J., Hartman, A. J., Chadwick, M., Tomar, G. S., Garcia, X., Senter, E., Taropa, E., Pillai, T. S., Devlin, J., Laskin, M., de Las Casas, D., Valter, D., Tao, C., Blanco, L., Badia, A. P., Reitter, D., Chen, M., Brennan, J., Rivera, C., Brin, S., Iqbal, S., Surita, G., Labanowski, J., Rao, A., Winkler, S., Parisotto, E., Gu, Y., Olszewska, K., Zhang, Y., Addanki, R., Miech, A., Louis, A., Shafey, L. E., Teplyashin, D., Brown, G., Catt, E., Attaluri, N., Balaguer, J., Xiang, J., Wang, P., Ashwood, Z., Briukhov, A., Webson, A., Ganapathy, S., Sanghavi, S., Kannan, A., Chang, M.-W., Stjerngren, A., Djolonga, J., Sun, Y., Bapna, A., Aitchison, M., Pejman, P., Michalewski, H., Yu, T., Wang, C., Love, J., Ahn, J., Bloxwich, D., Han, K., Humphreys, P., Sellam, T., Bradbury, J., Godbole, V., Samangooei, S., Damoc, B., Kaskasoli, A., Arnold, S. M. R., Vasudevan, V., Agrawal, S., Riesa, J., Lepikhin, D., Tanburn, R., Srinivasan, S., Lim, H., Hodkinson, S., Shyam, P., Ferret, J., Hand, S., Garg, A., Paine, T. L., Li, J., Li, Y., Giang, M., Neitz, A., Abbas, Z., York, S., Reid, M., Cole, E., Chowdhery, A., Das, D., RogoziÂ´nska, D., Nikolaev, V., Sprechmann, P., Nado, Z., Zilka, L., Prost, F., He, L., Monteiro, M., Mishra, G., Welty, C., Newlan, J., Jia, D., Allamanis, M., Hu, C. H., de Liedekerke, R., Gilmer, J., Saroufim, C., Rijhwani, S., Hou, S., Shrivastava, D., Baddepudi, A., Goldin, A., Ozturel, A., Cassirer, A., Xu, Y., Sohn,\n",
      "10\n",
      "D., Sachan, D., Amplayo, R. K., Swanson, C., Petrova, D., Narayan, S., Guez, A., Brahma, S., Landon, J., Patel, M., Zhao, R., Villela, K., Wang, L., Jia, W., Rahtz, M., GimÃ©nez, M., Yeung, L., Lin, H., Keeling, J., Georgiev, P., Mincu, D., Wu, B., Haykal, S., Saputro, R., Vodrahalli, K., Qin, J., Cankara, Z., Sharma, A., Fernando, N., Hawkins, W., Neyshabur, B., Kim, S., Hutter, A., Agrawal, P., Castro-Ros, A., van den Driessche, G., Wang, T., Yang, F., yiin Chang, S., Komarek, P., McIlroy, R., LuËciÂ´c, M., Zhang, G., Farhan, W., Sharman, M., Natsev, P., Michel, P., Cheng, Y., Bansal, Y., Qiao, S., Cao, K., Shakeri, S., Butterfield, C., Chung, J., Rubenstein, P. K., Agrawal, S., Mensch, A., Soparkar, K., Lenc, K., Chung, T., Pope, A., Maggiore, L., Kay, J., Jhakra, P., Wang, S., Maynez, J., Phuong, M., Tobin, T., Tacchetti, A., Trebacz, M., Robinson, K., Katariya, Y., Riedel, S., Bailey, P., Xiao, K., Ghelani, N., Aroyo, L., Slone, A., Houlsby, N., Xiong, X., Yang, Z., Gribovskaya, E., Adler, J., Wirth, M., Lee, L., Li, M., Kagohara, T., Pavagadhi, J., Bridgers, S., Bortsova, A., Ghemawat, S., Ahmed, Z., Liu, T., Powell, R., Bolina, V., Iinuma, M., Zablotskaia, P., Besley, J., Chung, D.-W., Dozat, T., Comanescu, R., Si, X., Greer, J., Su, G., Polacek, M., Kaufman, R. L., Tokumine, S., Hu, H., Buchatskaya, E., Miao, Y., Elhawaty, M., Siddhant, A., Tomasev, N., Xing, J., Greer, C., Miller, H., Ashraf, S., Roy, A., Zhang, Z., Ma, A., Filos, A., Besta, M., Blevins, R., Klimenko, T., Yeh, C.-K., Changpinyo, S., Mu, J., Chang, O., Pajarskas, M., Muir, C., Cohen, V., Lan, C. L., Haridasan, K., Marathe, A., Hansen, S., Douglas, S., Samuel, R., Wang, M., Austin, S., Lan, C., Jiang, J., Chiu, J., Lorenzo, J. A., SjÃ¶sund, L. L., Cevey, S., Gleicher, Z., Avrahami, T., Boral, A., Srinivasan, H., Selo, V., May, R., Aisopos, K., Hussenot, L., Soares, L. B., Baumli, K., Chang, M. B., Recasens, A., Caine, B., Pritzel, A., Pavetic, F., Pardo, F., Gergely, A., Frye, J., Ramasesh, V., Horgan, D., Badola, K., Kassner, N., Roy, S., Dyer, E., Campos, V., Tomala, A., Tang, Y., Badawy, D. E., White, E., Mustafa, B., Lang, O., Jindal, A., Vikram, S., Gong, Z., Caelles, S., Hemsley, R., Thornton, G., Feng, F., Stokowiec, W., Zheng, C., Thacker, P., ÃaËglar ÃnlÃ¼, Zhang, Z., Saleh, M., Svensson, J., Bileschi, M., Patil, P., Anand, A., Ring, R., Tsihlas, K., Vezer, A., Selvi, M., Shevlane, T., Rodriguez, M., Kwiatkowski, T., Daruki, S., Rong, K., Dafoe, A., FitzGerald, N., Gu-Lemberg, K., Khan, M., Hendricks, L. A., Pellat, M., Feinberg, V., Cobon-Kerr, J., Sainath, T., Rauh, M., Hashemi, S. H., Ives, R., Hasson, Y., Li, Y., Noland, E., Cao, Y., Byrd, N., Hou, L., Wang, Q., Sottiaux, T., Paganini, M., Lespiau, J.-B., Moufarek, A., Hassan, S., Shivakumar, K., van Amersfoort, J., Mandhane, A., Joshi, P., Goyal, A., Tung, M., Brock, A., Sheahan, H., Misra, V., Li, C., RakiÂ´ceviÂ´c, N., Dehghani, M., Liu, F., Mittal, S., Oh, J., Noury, S., Sezener, E., Huot, F., Lamm, M., Cao, N. D., Chen, C., Elsayed, G., Chi, E., Mahdieh, M., Tenney, I., Hua, N., Petrychenko, I., Kane, P., Scandinaro, D., Jain, R., Uesato, J., Datta, R., Sadovsky, A., Bunyan, O., Rabiej, D., Wu, S., Zhang, J., Vasudevan, G., Leurent, E., Alnahlawi, M., Georgescu, I., Wei, N., Zheng, I., Chan, B., Rabinovitch, P. G., Stanczyk, P., Zhang, Y., Steiner, D., Naskar, S., Azzam, M., Johnson, M., Paszke, A., Chiu, C.-C., Elias, J. S., Mohiuddin, A., Muhammad, F., Miao, J., Lee, A., Vieillard, N., Potluri, S., Park, J., Davoodi, E., Zhang, J., Stanway, J., Garmon, D., Karmarkar, A., Dong, Z., Lee, J., Kumar, A., Zhou, L., Evens, J., Isaac, W., Chen, Z., Jia, J., Levskaya, A., Zhu, Z., Gorgolewski, C., Grabowski, P., Mao, Y., Magni, A., Yao, K., Snaider, J., Casagrande, N., Suganthan, P., Palmer, E., Irving, G., Loper, E., Faruqui, M., Arkatkar, I., Chen, N., Shafran, I., Fink, M., CastaÃ±o, A., Giannoumis, I., Kim, W., RybiÂ´nski, M., Sreevatsa, A., Prendki, J., Soergel, D., Goedeckemeyer, A., Gierke, W., Jafari, M., Gaba, M., Wiesner, J., Wright, D. G., Wei, Y., Vashisht, H., Kulizhskaya, Y., Hoover, J., Le, M., Li, L., Iwuanyanwu, C., Liu, L., Ramirez, K., Khorlin, A., Cui, A., LIN, T., Georgiev, M., Wu, M., Aguilar, R., Pallo, K., Chakladar, A., Repina, A., Wu, X., van der Weide, T., Ponnapalli, P., Kaplan, C., Simsa, J., Li, S., Dousse, O., Yang, F., Piper, J., Ie, N., Lui, M., Pasumarthi, R., Lintz, N., Vijayakumar, A., Thiet, L. N., Andor, D., Valenzuela, P., Paduraru, C., Peng, D., Lee, K., Zhang, S., Greene, S., Nguyen, D. D., Kurylowicz, P., Velury, S., Krause, S., Hardin, C., Dixon, L., Janzer, L., Choo, K., Feng, Z., Zhang, B., Singhal, A., Latkar, T., Zhang, M., Le, Q., Abellan, E. A., Du, D., McKinnon, D., Antropova, N., Bolukbasi, T., Keller, O., Reid, D., Finchelstein, D., Raad, M. A., Crocker, R., Hawkins, P., Dadashi, R., Gaffney, C., Lall, S., Franko, K., Filonov, E., Bulanova, A., Leblond, R., Yadav, V., Chung, S., Askham, H., Cobo, L. C., Xu, K., Fischer, F., Xu, J., Sorokin, C., Alberti, C., Lin, C.-C., Evans, C., Zhou, H., Dimitriev, A., Forbes, H., Banarse, D., Tung, Z., Liu, J., Omernick, M., Bishop, C., Kumar, C., Sterneck, R., Foley, R., Jain, R., Mishra, S., Xia, J., Bos, T., Cideron, G., Amid, E., Piccinno, F., Wang, X., Banzal, P., Gurita, P., Noga, H., Shah, P., Mankowitz, D. J., Polozov, A., Kushman, N., Krakovna, V., Brown, S., Bateni, M., Duan, D., Firoiu, V., Thotakuri, M., Natan, T., Mohananey, A., Geist, M., Mudgal, S., Girgin, S., Li, H., Ye, J., Roval, O., Tojo, R., Kwong, M., Lee-Thorp, J., Yew, C., Yuan, Q., Bagri, S., Sinopalnikov, D., Ramos, S., Mellor, J., Sharma, A., Severyn, A., Lai, J., Wu, K., Cheng, H.-T., Miller, D., Sonnerat, N., Vnukov, D., Greig, R., Beattie, J., Caveness, E., Bai, L., Eisenschlos, J., Korchemniy, A., Tsai, T., Jasarevic,\n",
      "11\n",
      "M., Kong, W., Dao, P., Zheng, Z., Liu, F., Yang, F., Zhu, R., Geller, M., Teh, T. H., Sanmiya, J., Gladchenko, E., Trdin, N., Sozanschi, A., Toyama, D., Rosen, E., Tavakkol, S., Xue, L., Elkind, C., Woodman, O., Carpenter, J., Papamakarios, G., Kemp, R., Kafle, S., Grunina, T., Sinha, R., Talbert, A., Goyal, A., Wu, D., Owusu-Afriyie, D., Du, C., Thornton, C., Pont-Tuset, J., Narayana, P., Li, J., Fatehi, S., Wieting, J., Ajmeri, O., Uria, B., Zhu, T., Ko, Y., Knight, L., HÃ©liou, A., Niu, N., Gu, S., Pang, C., Tran, D., Li, Y., Levine, N., Stolovich, A., Kalb, N., Santamaria-Fernandez, R., Goenka, S., Yustalim, W., Strudel, R., Elqursh, A., Lakshminarayanan, B., Deck, C., Upadhyay, S., Lee, H., Dusenberry, M., Li, Z., Wang, X., Levin, K., Hoffmann, R., Holtmann-Rice, D., Bachem, O., Yue, S., Arora, S., Malmi, E., Mirylenka, D., Tan, Q., Koh, C., Yeganeh, S. H., PÃµder, S., Zheng, S., Pongetti, F., Tariq, M., Sun, Y., Ionita, L., Seyedhosseini, M., Tafti, P., Kotikalapudi, R., Liu, Z., Gulati, A., Liu, J., Ye, X., Chrzaszcz, B., Wang, L., Sethi, N., Li, T., Brown, B., Singh, S., Fan, W., Parisi, A., Stanton, J., Kuang, C., Koverkathu, V., Choquette-Choo, C. A., Li, Y., Lu, T., Ittycheriah, A., Shroff, P., Sun, P., Varadarajan, M., Bahargam, S., Willoughby, R., Gaddy, D., Dasgupta, I., Desjardins, G., Cornero, M., Robenek, B., Mittal, B., Albrecht, B., Shenoy, A., Moiseev, F., Jacobsson, H., Ghaffarkhah, A., RiviÃ¨re, M., Walton, A., Crepy, C., Parrish, A., Liu, Y., Zhou, Z., Farabet, C., Radebaugh, C., Srinivasan, P., van der Salm, C., Fidjeland, A., Scellato, S., Latorre-Chimoto, E., Klimczak-PluciÂ´nska, H., Bridson, D., de Cesare, D., Hudson, T., Mendolicchio, P., Walker, L., Morris, A., Penchev, I., Mauger, M., Guseynov, A., Reid, A., Odoom, S., Loher, L., Cotruta, V., Yenugula, M., Grewe, D., Petrushkina, A., Duerig, T., Sanchez, A., Yadlowsky, S., Shen, A., Globerson, A., Kurzrok, A., Webb, L., Dua, S., Li, D., Lahoti, P., Bhupatiraju, S., Hurt, D., Qureshi, H., Agarwal, A., Shani, T., Eyal, M., Khare, A., Belle, S. R., Wang, L., Tekur, C., Kale, M. S., Wei, J., Sang, R., Saeta, B., Liechty, T., Sun, Y., Zhao, Y., Lee, S., Nayak, P., Fritz, D., Vuyyuru, M. R., Aslanides, J., Vyas, N., Wicke, M., Ma, X., Bilal, T., Eltyshev, E., Balle, D., Martin, N., Cate, H., Manyika, J., Amiri, K., Kim, Y., Xiong, X., Kang, K., Luisier, F., Tripuraneni, N., Madras, D., Guo, M., Waters, A., Wang, O., Ainslie, J., Baldridge, J., Zhang, H., Pruthi, G., Bauer, J., Yang, F., Mansour, R., Gelman, J., Xu, Y., Polovets, G., Liu, J., Cai, H., Chen, W., Sheng, X., Xue, E., Ozair, S., Yu, A., Angermueller, C., Li, X., Wang, W., Wiesinger, J., Koukoumidis, E., Tian, Y., Iyer, A., Gurumurthy, M., Goldenson, M., Shah, P., Blake, M., Yu, H., Urbanowicz, A., Palomaki, J., Fernando, C., Brooks, K., Durden, K., Mehta, H., Momchev, N., Rahimtoroghi, E., Georgaki, M., Raul, A., Ruder, S., Redshaw, M., Lee, J., Jalan, K., Li, D., Perng, G., Hechtman, B., Schuh, P., Nasr, M., Chen, M., Milan, K., Mikulik, V., Strohman, T., Franco, J., Green, T., Hassabis, D., Kavukcuoglu, K., Dean, J., and Vinyals, O. Gemini: A family of highly capable multimodal models, 2023.\n",
      "TII UAE. The Falcon family of large language models. https://huggingface.co/tiiuae/ falcon-40b, May 2023.\n",
      "Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., RoziÃ¨re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"content\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "612590ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n",
      "# Albert Gu*1 and Tri Dao*2\n",
      "1Machine Learning Department, Carnegie Mellon University 2Department of Computer Science, Princeton University agu@cs.cmu.edu, tri@tridao.me\n",
      "# Abstract\n",
      "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformersâ computational ineï¬ciency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities\n"
     ]
    }
   ],
   "source": [
    "content = dataset[3][\"content\"]\n",
    "print(content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fb1ac61",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_router.splitters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msplitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RollingWindowSplitter\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_router\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[32m      3\u001b[39m logger.setLevel(\u001b[33m\"\u001b[39m\u001b[33mWarning\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'semantic_router.splitters'"
     ]
    }
   ],
   "source": [
    "from semantic_router.splitters import RollingWindowSplitter\n",
    "from semantic_router.utils.logger import logger\n",
    "logger.setLevel(\"Warning\")\n",
    "splitter = RollingWindowSplitter(\n",
    "    encoder = encoder,\n",
    "    min_split_tokens = 100,\n",
    "    max_split_tokens = 500,\n",
    "    window_size=2,\n",
    "    plot_splits = True, #set this to true to visualize chunking\n",
    "    enable_statistics = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90d35db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed version: 0.1.2\n",
      "Installed at: d:\\agenticai\\.venv\\Lib\\site-packages\\semantic_router\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import semantic_router\n",
    "print(\"Installed version:\", semantic_router.__version__)\n",
    "print(\"Installed at:\", semantic_router.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea30a48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-31 20:01:26 INFO semantic_chunkers.utils.logger Single document exceeds the maximum token limit of 300. Splitting to sentences before semantically merging.\u001b[0m\n",
      "100%|██████████| 49/49 [01:39<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from semantic_chunkers import StatisticalChunker\n",
    "chunker = StatisticalChunker(encoder=encoder)   \n",
    "chunks = chunker(docs=[content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c8c38f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1, tokens 300, triggered by: token limit\n",
      "\u001b[31m# Mamba: Linear-Time Sequence Modeling with Selective State Spaces # Albert Gu*1 and Tri Dao*2 1Machine Learning Department, Carnegie Mellon University 2Department of Computer Science, Princeton University agu@cs.cmu.edu, tri@tridao.me # Abstract Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformersâ computational ineï¬ ciency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of eï¬ cient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpliï¬ ed end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5Ã higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 2, tokens 300, triggered by: token limit\n",
      "\u001b[32mAs a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. # 1 Introduction Foundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an eï¬ ective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The eï¬ cacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 3, tokens 112, triggered by: 0.32\n",
      "\u001b[34mHowever, this property brings fundamental drawbacks: an inability to model anything outside of a ï¬ nite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more eï¬ cient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it eï¬ ective. As of yet, none of these variants have been shown to be empirically eï¬ ective at scale across domains.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 4, tokens 121, triggered by: 0.21\n",
      "\u001b[35mRecently, structured state space sequence models (SSMs) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very eï¬ ciently as either a recurrence or convolution, with linear or near-linear scaling in sequence length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 5, tokens 256, triggered by: 0.26\n",
      "\u001b[31mAdditionally, they have principled Equal contribution. 1 mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Range Arena (Tay, Dehghani, Abnar, et al. 2021). Many ï¬ avors of SSMs (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less eï¬ ective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 6, tokens 240, triggered by: 0.28\n",
      "\u001b[32mSelection Mechanism. First, we identify a key limitation of prior models: the ability to eï¬ ciently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to ï¬ lter out irrelevant information and remember relevant information indeï¬ nitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally eï¬ cient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between diï¬ erent levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3Ã faster on A100 GPUs).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 7, tokens 236, triggered by: 0.26\n",
      "\u001b[34mArchitecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and eï¬ ciency together yield performance improvements on real data up to sequence length 1M. We empirically validate Mambaâ s potential as a general sequence FM backbone, in both pretraining quality and domain-speciï¬ c task performance, on several types of modalities and settings:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 8, tokens 139, triggered by: 0.33\n",
      "\u001b[35mâ ¢ Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indeï¬ nitely long (>1M tokens). â ¢ Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform- ers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. â ¢ Language Modeling.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 9, tokens 165, triggered by: 0.21\n",
      "\u001b[31mMamba is the ï¬ rst linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5Ã generation throughput compared to Transformers of similar size, and Mamba-3Bâ s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 10, tokens 138, triggered by: 0.34\n",
      "\u001b[32m2 # Selective State Space Model # with Hardware-aware State Expansion # A vuvy GPU SRAM Selection Mechanism es Selection Mechanism Figure 1: (Overview.) Structured SSMs independently map each channel (e.g. ð · = 5) of an input ð ¥ to output ð ¦ through a higher dimensional latent state â (e.g. ð = 4). Prior SSMs avoid materializing this large effective state (ð ·ð , times batch size ð µ and sequence length ð ¿) through clever alternate computation paths requiring time-invariance: the (â , A, B, C) parameters are constant across time.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 11, tokens 100, triggered by: 0.26\n",
      "\u001b[34mOur selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. # 2 State Space Models Structured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a 1-dimensional function or sequence ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 12, tokens 188, triggered by: 0.46\n",
      "\u001b[35m¥(ð ¡) â â â ¦ ð ¦(ð ¡) â â through an implicit latent state â (ð ¡) â â ð . Concretely, S4 models are deï¬ ned with four parameters (â , A, B, C), which deï¬ ne a sequence-to-sequence trans- formation in two stages. â â ²(ð ¡) = Aâ (ð ¡) + Bð ¥(ð ¡) ð ¦(ð ¡) = Câ (ð ¡) (1a) (1b) â ð ¡ = Aâ ð ¡â 1 + Bð ¥ð ¡ ð ¦ð ¡ = Câ ð ¡ (2a) (2b) ð ð ² = (Cð ©, Cð ¨ð ©, â ¦ , Cð ¨ ð ¦ = ð ¥ â ð ² ð ©, â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 13, tokens 112, triggered by: 0.41\n",
      "\u001b[31m¦ ) (3a) (3b) Discretization. The ï¬ rst stage transforms the â continuous parametersâ (â , A, B) to â discrete parametersâ (A, B) through ï¬ xed formulas A = ð ð ´(â , A) and B = ð ð µ(â , A, B), where the pair (ð ð ´, ð ð µ) is called a discretization rule. Various rules can be used such as the zero-order hold (ZOH) deï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 14, tokens 109, triggered by: 0.24\n",
      "\u001b[32mned in equation (4). A = exp(â A) B = (â A)â 1(exp(â A) â I) â â B (4) Discretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly normalized (Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 15, tokens 274, triggered by: 0.29\n",
      "\u001b[34mIt also has connections to gating mechanisms of RNNs (Gu, Gulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5. However, from a mechanical point of view discretization can simply be viewed as the ï¬ rst step of the computation graph in the forward pass of an SSM. Alternate ï¬ avors of SSMs can bypass the discretization step and parameterize (A, B) directly instead (Zhang et al. 2023), which may be easier to reason about. Computation. After the parameters have been transformed from (â , A, B, C) â ¦ (A, B, C), the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3). 3 Commonly, the model uses the convolutional mode (3) for eï¬ cient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for eï¬ cient autoregressive inference (where the inputs are seen one timestep at a time). Linear Time Invariance (LTI). An important property of equations (1) to (3) is that the modelâ s dynamics are constant through time.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 16, tokens 182, triggered by: 0.33\n",
      "\u001b[35mIn other words (â , A, B, C), and consequently (A, B) as well, are ï¬ xed for all time-steps. This property is called linear time invariance (LTI), which is deeply connected to recurrence and convolutions. Informally, we think of LTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these classes of models. Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental eï¬ ciency constraints, discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint while overcoming the eï¬ ciency bottlenecks.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 17, tokens 141, triggered by: 0.24\n",
      "\u001b[31mStructure and Dimensions. Finally, we note that structured SSMs are so named because computing them eï¬ ciently also requires imposing structure on the A matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use. In this case, the A â â ð Ã ð , B â â ð Ã 1, C â â 1Ã ð matrices can all be represented by ð numbers. To operate over an input sequence ð ¥ of batch size ð µ and length ð ¿ with ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 18, tokens 236, triggered by: 0.26\n",
      "\u001b[32m· channels, the SSM is applied independently to each channel. Note that in this case, the total hidden state has dimension ð ·ð per input, and computing it over the sequence length requires ð (ð µð ¿ð ·ð ) time and memory; this is the root of the fundamental eï¬ ciency bottleneck addressed in Section 3.3. General State Space Models. We note that the term state space model has a very broad meaning which simply represents the notion of any recurrent process with a latent state. It has been used to refer to many disparate concepts in diï¬ erent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman ï¬ lters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes convolutional) models at large (deep learning). Throughout this entire paper we use the term â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 19, tokens 229, triggered by: 0.22\n",
      "\u001b[34mSSMâ to refer exclusively to the class of structured SSMs or S4 models (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington, and Linderman 2023) and use these terms interchangeably. For convenience we may also include derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y. Li et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 20, tokens 158, triggered by: 0.32\n",
      "\u001b[35mâ ¢ Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. â ¢ H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. â ¢ Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 21, tokens 106, triggered by: 0.26\n",
      "\u001b[31m¢ RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. 4 â ¢ RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation (attention-free Transformer (S. Zhai et al. 2021)). Its main â WKVâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 22, tokens 172, triggered by: 0.25\n",
      "\u001b[32mmechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. # 3 Selective State Space Models We motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 23, tokens 254, triggered by: 0.33\n",
      "\u001b[34mciently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). # 3.1 Motivation: Selection as a Means of Compression We argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoï¬ s of popular sequence models from this point of view. For example, attention is both eï¬ ective and ineï¬ cient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are eï¬ cient because they have a ï¬ nite state, implying constant-time inference and linear-time training. However, their eï¬ ectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 24, tokens 283, triggered by: 0.35\n",
      "\u001b[35mâ ¢ The Selective Copying task modiï¬ es the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and ï¬ lter out the irrelevant ones (white). â ¢ The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the (A, B) transitions in (2)) cannot let them select the correct information from their context, or aï¬ ect the hidden state passed along the sequence an in input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have diï¬ culty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 25, tokens 183, triggered by: 0.18\n",
      "\u001b[31mIn summary, the eï¬ ciency vs. eï¬ ectiveness tradeoï¬ of sequence models is characterized by how well they compress their state: eï¬ cient models must have a small state, while eï¬ ective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or ï¬ lter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). # Improving SSMs with Selection One method of incorporating a selection mechanism into models is by letting their parameters that aï¬ ect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN) be input-dependent.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 26, tokens 190, triggered by: 0.38\n",
      "\u001b[32m5 Copying Output noo am > mt HE nee Tt Solution # Tetons | # oO S lective Copying # aoe # i) # [coe # Induction Heads # EES > # fo Perfectly solved by LTI (e.g. convolutional) models that do not need to look at the actual inputs Hi i Hl ] Bw H a H > BH Figure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 27, tokens 123, triggered by: 0.37\n",
      "\u001b[34mAlgorithm 2 SSM + Selection (S6) Input: ð ¥ â ¶ (ð ±, ð », ð ³) Output: ð ¦ â ¶ (ð ±, ð », ð ³) 1: A â ¶ (ð ³, ð ½) â ð ¯ð ºð ð ºð ð ¾ð ð ¾ð â ³ Represents structured ð Ã ð matrix â ³ Represents structured ð Ã ð matrix 2: B â ¶ (ð ³, ð ½) â ð ¯ð ºð ð ºð ð ¾ð ð ¾ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 28, tokens 115, triggered by: 0.49\n",
      "\u001b[35m3: C â ¶ (ð ³, ð ½) â ð ¯ð ºð ð ºð ð ¾ð ð ¾ð 4: â â ¶ (ð ³) â ð â (ð ¯ð ºð ð ºð ð ¾ð ð ¾ð ) 5: A, B â ¶ (ð ³, ð ½) â ð ½ð ð ð ¼ð ð ¾ð ð ð ð ¾(â , A, B) 6: ð ¦ â ð ²ð ²ð ¬(A, B, C)(ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 29, tokens 131, triggered by: 0.42\n",
      "\u001b[31m¥) 2: B â ¶ (ð ±, ð », ð ½) â ð ð µ(ð ¥) 3: C â ¶ (ð ±, ð », ð ½) â ð ð ¶(ð ¥) 4: â â ¶ (ð ±, ð », ð ³) â ð â (ð ¯ð ºð ð ºð ð ¾ð ð ¾ð +ð â (ð ¥)) 5: A, B â ¶ (ð ±, ð », ð ³, ð ½) â ð ½ð ð ð ¼ð ð ¾ð ð ð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 30, tokens 114, triggered by: 0.44\n",
      "\u001b[32m¾(â , A, B) 6: ð ¦ â ð ²ð ²ð ¬(A, B, C)(ð ¥) â ³ Time-invariant: recurrence or convolution â ³ Time-varying: recurrence (scan) only 7: return ð ¦ 7: return ð ¦ Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main diï¬ erence is simply making several parameters â , B, C functions of the input, along with the associated changes to tensor shapes throughout.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 31, tokens 192, triggered by: 0.34\n",
      "\u001b[34mIn particular, we highlight that these parameters now have a length dimension ð ¿, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2). This loses the equivalence to convolutions (3) with implications for its eï¬ ciency, discussed next. We speciï¬ cally choose ð ð µ(ð ¥) = ð «ð ð ð ¾ð ºð ð (ð ¥), ð ð ¶(ð ¥) = ð «ð ð ð ¾ð ºð ð (ð ¥), ð â (ð ¥) = ð ¡ð ð ð ºð ½ð ¼ð ºð ð ð ·(ð «ð ð ð ¾ð ºð 1(ð ¥)), and ð â = ð ð ð ¿ð ð ð ð ð , where ð «ð ð ð ¾ð ºð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 32, tokens 108, triggered by: 0.33\n",
      "\u001b[35mis a parameterized projection to dimension ð . The choice of ð â and ð â is due to a connection to RNN gating mechanisms explained in Section 3.5. # 3.3 Efficient Implementation of Selective SSMs Hardware-friendly architectures such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and Transform- ers (Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 33, tokens 104, triggered by: 0.39\n",
      "\u001b[31mcient on modern hardware (GPU) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting â vary over time in recurrent SSMs (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational eï¬ ciency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 34, tokens 100, triggered by: 0.41\n",
      "\u001b[32m# 3.3.1 Motivation of Prior Models We ï¬ rst revisit this motivation and overview our approach to overcome limitations of prior methods. â ¢ At a high level, recurrent models such as SSMs always balance a tradeoï¬ between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more eï¬ ective but slower. Thus 6 we want to maximize hidden state dimension without paying speed and memory costs. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 35, tokens 129, triggered by: 0.41\n",
      "\u001b[34m¢ Note that the recurrent mode is more ï¬ exible than the convolution mode, since the latter (3) is derived from expanding the former (2) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state â with shape (ð ±, ð », ð ³, ð ½), much larger (by a factor of ð , the SSM state dimension) than the input ð ¥ and output ð ¦ of shape (ð ±, ð », ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 36, tokens 109, triggered by: 0.31\n",
      "\u001b[35m³). Thus the more eï¬ cient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of only (ð ±, ð », ð ³). â ¢ Prior LTI SSMs leverage the dual recurrent-convolutional forms to increase the eï¬ ective state dimension by a factor of ð (â 10 â 100), much larger than traditional RNNs, without eï¬ ciency penalties. # 3.3.2 Overview of Selective Scan:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 37, tokens 293, triggered by: token limit\n",
      "\u001b[31mHardware-Aware State Expansion The selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations: â ¢ The naive recurrent computation uses ð (ð µð ¿ð ·ð ) FLOPs while the convolutional computation uses ð (ð µð ¿ð · log(ð ¿)) FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension ð , the recurrent mode can actually use fewer FLOPs. â ¢ The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state â . The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state â only in more eï¬ cient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a signiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 38, tokens 173, triggered by: 0.29\n",
      "\u001b[32mcant speedup compared to a standard implementation. Concretely, instead of preparing the scan input (A, B) of size (ð ±, ð », ð ³, ð ½) in GPU HBM (high-bandwidth memory), we load the SSM parameters (â , A, B, C) directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the ï¬ nal outputs of size (ð ±, ð », ð ³) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-eï¬ cient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 39, tokens 264, triggered by: 0.29\n",
      "\u001b[34mFinally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. # 3.4 A Simplified SSM Architecture As with structured SSMs, selective SSMs are standalone sequence transformations that can be ï¬ exibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension ð · by a controllable expansion factor ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 40, tokens 193, triggered by: 0.18\n",
      "\u001b[35m¸. For each block, most of the parameters (3ð ¸ð ·2) are in the linear projections (2ð ¸ð ·2 for input projections, ð ¸ð ·2 for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for â , B, C, and 7 Linear projection Sequence transformation Nonlinearity (activation multiplication) H3 Â®@ Gated MLP â Mamba # or Figure 3: (Architecture.) Our simplified block design combines the H3 block, which is the basis of most SSM architectures, with the ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba block homogenously. Compared to the H3 block, Mamba replaces the first multiplicative gate with an activation function. Compared to the MLP block, Mamba adds an SSM to the main branch.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 41, tokens 266, triggered by: 0.27\n",
      "\u001b[31mFor ð we use the SiLU / Swish activation (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017). the matrix A) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always ï¬ x to ð ¸ = 2 in our experiments and use two stacks of the block to match the 12ð ·2 parameters of a Transformerâ s interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular â SwiGLUâ variant (Chowdhery et al. 2023; Shazeer 2020; Touvron et al. 2023). Finally, we additionally use an optional normalization layer (we choose LayerNorm (J. L. Ba, Kiros, and Hinton 2016)), motivated by RetNetâ s usage of a normalization layer in a similar location (Y. Sun et al. 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 42, tokens 259, triggered by: 0.25\n",
      "\u001b[32m# 3.5 Properties of Selection Mechanisms The selection mechanism is a broader concept that can be applied in diï¬ erent ways, such as to more traditional RNNs or CNNs, to diï¬ erent parameters (e.g. A in Algorithm 2), or using diï¬ erent transformations ð (ð ¥). # 3.5.1 Connection to Gating Mechanisms We highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selection mechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time systems is well established (Funahashi and Nakamura 1993; Tallec and Ollivier 2018). In fact, Theorem 1 is an improvement of Gu, Johnson, Goel, et al. (2021, Lemma 3.1) generalizing to the ZOH discretization and input-dependent gates (proof in Appendix C). More broadly, â in SSMs can be seen to play a generalized role of the RNN gating mechanism. In line with prior work, we adopt the view that discretization of SSMs is the principled foundation of heuristic gating mechanisms. Theorem 1.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 43, tokens 116, triggered by: 0.33\n",
      "\u001b[34mWhen ð = 1, A = â 1, B = 1, ð â = ð «ð ð ð ¾ð ºð (ð ¥), and ð â = ð ð ð ¿ð ð ð ð ð , then the selective SSM recurrence (Algorithm 2) takes the form ð ð ¡ = ð (ð «ð ð ð ¾ð ºð (ð ¥ð ¡)) â ð ¡ = (1 â ð ð ¡)â ð ¡â 1 + ð ð ¡ð ¥ð ¡. (5)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 44, tokens 106, triggered by: 0.31\n",
      "\u001b[35mAs mentioned in Section 3.2, our speciï¬ c choices of ð â , ð â is from this connection. In particular, note that if a given input ð ¥ð ¡ should be completely ignored (as necessary in the synthetic tasks), all ð · channels should ignore it, and so we project the input down to 1 dimension before repeating/broadcasting with â . 8 # Interpretation of Selection Mechanisms We elaborate on two particular mechanistic eï¬ ects of selection.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 45, tokens 116, triggered by: 0.29\n",
      "\u001b[31mVariable Spacing. Selectivity allows ï¬ ltering out irrelevant noise tokens that may occur between inputs of interest. This is exempliï¬ ed by the Selective Copying task, but occurs ubiquitously in common data modalities, particularly for discrete data â for example the presence of language ï¬ llers such as â umâ . This property arises because the model can mechanistically ï¬ lter out any particular input ð ¥ð ¡, for example in the gated RNN case (Theorem 1) when ð ð ¡ â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 46, tokens 117, triggered by: 0.29\n",
      "\u001b[32m0. It has been empirically observed that many sequence models do not improve with longer Filtering Context. context (F. Shi et al. 2023), despite the principle that more context should lead to strictly better performance. An explanation is that many sequence models cannot eï¬ ectively ignore irrelevant context when necessary; an intuitive example are global convolutions (and general LTI models). On the other hand, selective models can simply reset their state at any time to remove extraneous history, and thus their performance in principle improves monotonicly with context length (e.g.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 47, tokens 295, triggered by: token limit\n",
      "\u001b[34mSection 4.3.2). In settings where multiple independent sequences are stitched together, Transformers Boundary Resetting. can keep them separate by instantiating a particular attention mask, while LTI models will bleed information between the sequences. Selective SSMs can also reset their state at boundaries (e.g. â ð ¡ â â or Theorem 1 when ð ð ¡ â 1). These settings may occur artiï¬ cially (e.g. packing documents together to improve hardware utilization) or naturally (e.g. episode boundaries in reinforcement learning (Lu et al. 2023)). Additionally, we elaborate on eï¬ ects of each selective parameter. In general, â controls the balance between how much to focus or ignore the current input Interpretation of â . ð ¥ð ¡. It generalizes RNN gates (e.g. ð ð ¡ in Theorem 1), mechanically, a large â resets the state â and focuses on the current input ð ¥, while a small â persists the state and ignores the current input. SSMs (1)-(2) can be interpreted as a continuous system discretized by a timestep â , and in this context the intuition is that large â â â represents the system focusing on the current input for longer (thus â selectingâ it and forgetting its current state) while a small â â 0 represents a transient input that is ignored. Interpretation of A.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 48, tokens 175, triggered by: 0.28\n",
      "\u001b[35mWe remark that while the A parameter could also be selective, it ultimately aï¬ ects the model only through its interaction with â via A = exp(â A) (the discretization (4)). Thus selectivity in â is enough to ensure selectivity in (A, B), and is the main source of improvement. We hypothesize that making A selective in addition to (or instead of) â would have similar performance, and leave it out for simplicity. Interpretation of B and C. As discussed in Section 3.1, the most important property of selectivity is ï¬ ltering out irrelevant information so that a sequence modelâ s context can be compressed into an eï¬ cient state. In an SSM, modifying B and C to be selective allows ï¬ ner-grained control over whether to let an input ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 49, tokens 208, triggered by: 0.23\n",
      "\u001b[31m¥ð ¡ into the state â ð ¡ or the state into the output ð ¦ð ¡. These can be interpreted as allowing the model to modulate the recurrent dynamics based on content (input) and context (hidden states) respectively. 3.6 Additional Model Details Real vs. Complex. Most prior SSMs use complex numbers in their state â , which is necessary for strong performance on many tasks (Gu, Goel, and RÃ© 2022). However, it has been empirically observed that completely real-valued SSMs seem to work ï¬ ne, and possibly even better, in some settings (Ma et al. 2023). We use real values as the default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeoï¬ is related to the continuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous modalities (e.g. audio, video) but not discrete (e.g. text, DNA).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 50, tokens 121, triggered by: 0.35\n",
      "\u001b[32m9 Initialization. Most prior SSMs also suggest special initializations, particularly in the complex-valued case, which can help in several settings such as low-data regimes. Our default initialization for the complex case is S4D-Lin and for the real case is S4D-Real (Gu, Gupta, et al. 2022), which is based on the HIPPO theory (Gu, Dao, et al. 2020). These deï¬ ne the ð -th element of A as â 1â 2 + ð ð and â (ð + 1) respectively.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 51, tokens 103, triggered by: 0.34\n",
      "\u001b[34mHowever, we expect many initializations to work ï¬ ne, particularly in the large-data and real-valued SSM regimes; some ablations are considered in Section 4.6. Parameterization of â . We deï¬ ned the selective adjustment to â as ð â (ð ¥) = ð ¡ð ð ð ºð ½ð ¼ð ºð ð ð ·(ð «ð ð ð ¾ð ºð 1(ð ¥)), which was motivated by the mechanics of â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 52, tokens 104, triggered by: 0.34\n",
      "\u001b[35m(Section 3.5). We observe that it can be generalized from dimension 1 to a larger dimension ð . We set this to be a small fraction of ð ³, which uses a negligible number of parameters compared to the main Linear projections in the block. We additionally note that the broadcasting operation can instead be viewed as another Linear projection, initialized to a speciï¬ c pattern of 1â s and 0â s; if this projection is trainable, this leads to the alternative ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 53, tokens 279, triggered by: token limit\n",
      "\u001b[31mâ (ð ¥) = ð «ð ð ð ¾ð ºð ð ·(ð «ð ð ð ¾ð ºð ð (ð ¥)), which can be viewed as a low-rank projection. In our experiments, the â parameter (which can be viewed as a bias term) is initialized to ð â 1 â following prior work on SSMs (Gu, Johnson, Timalsina, et al. 2023). Remark 3.1. For brevity in our experimental results, we sometimes abbreviate selective SSMs as S6 models, because they are S4 models with a selection mechanism and computed with a scan. # 4 Empirical Evaluation In Section 4.1 we test Mambaâ s ability to solve the two synthetic tasks motivated in Section 3.1. We then evaluate on three domains, each evaluated on autoregressive pretraining as well as downstream tasks. Section 4.2: language model pretraining (scaling laws), and zero-shot downstream evaluation. Section 4.3: DNA sequence pretraining, and ï¬ ne-tuning on a long-sequence classiï¬ cation task. Section 4.4: audio waveform pretraining, and the quality of autoregressively generated speech clips. Finally, Section 4.5 shows Mambaâ s computational eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 54, tokens 283, triggered by: token limit\n",
      "\u001b[32mciency at both training and inference time, and Section 4.6 ablates various components of the architecture and selective SSMs. # 4.1 Synthetic Tasks Full experiment details for these tasks including task details and training protocol are in Appendix E.1. # 4.1.1 Selective Copying The Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test the memorization abilities of recurrent models. As discussed in Section 3.1, LTI SSMs (linear recurrences and global convolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for example, by constructing a convolution kernel of exactly the right length (Figure 2). This was explicitly validated in earlier work on global convolutions (Romero et al. 2021). The Selective Copying task prevents this shortcut by randomizing the spacing between tokens. Note that this task has been introduced before as the Denoising task (Jing et al. 2019). Note that many previous works argue that adding architecture gating (multiplicative interactions) can endow models with â data-dependenceâ and solve related tasks (Dao, Fu, Saab, et al. 2023; Poli et al. 2023). However, we ï¬ nd this explanation insuï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 55, tokens 265, triggered by: 0.25\n",
      "\u001b[34mcient intuitively because such gating does not interact along the sequence axis, and cannot aï¬ ect the spacing between tokens. In particular architecture gating is not an instance of a selection mechanism (Appendix A). Table 1 conï¬ rms that gated architectures such as H3 and Mamba only partially improve performance, while the selection mechanism (modifying S4 to S6) easily solves this task, particularly when combined with these more powerful architectures. 10 Model Arch. Layer Acc. S4 - No gate No gate S4 S6 18.3 97.0 H3 Hyena - H3 H3 H3 S4 Hyena S6 57.0 30.1 99.7 - - Mamba Mamba Mamba Mamba Hyena S4 S6 56.4 28.4 99.8 Induction Heads Extrapolation Extrapolation 1.05 ' â â Mua-Absotute 08] ; â â MHA-RoPE i =~ MHA-xPos 6) i â HB oa = byena ' Random 1 ran benath 0.0 , ; ; : , 10Â° 10Â° 108 10Â° 10Â° Test Sequence Length > g 8\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 56, tokens 153, triggered by: 0.10\n",
      "\u001b[35mTable 1: (Selective Copying.) Accuracy for combinations of architectures and inner sequence layers. Table 2: (Induction Heads.) Models are trained on sequence length 28 = 256, and tested on increasing sequence lengths of 26 = 64 up to 220 = 1048576. Full numbers in Table 11. # 4.1.2 Induction Heads Induction heads (Olsson et al. 2022) is a simple task from the mechanistic interpretability lens (Elhage et al. 2021) that is surprisingly predictive of the in-context learning ability of LLMs. It requires models to perform associative recall and copy: for example, if the model has seen a bigram such as â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 57, tokens 287, triggered by: token limit\n",
      "\u001b[31mHarry Potterâ in the sequence, then the next time â Harryâ appears in the same sequence, the model should be able to predict â Potterâ by copying from history. Dataset. We train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of 16, which is comparable to prior work on this task (Dao, Fu, Saab, et al. 2023) but with longer sequences. We additionally investigate generalization and extrapolation abilities by evaluating on a range of sequence lengths from 26 = 64 up to 220 = 1048576 at test time. Models. Following established work on induction heads, we use 2 layer models, which allows attention to mechanistically solve the induction heads task (Olsson et al. 2022). We test both multi-head attention (8 heads, with various positional encodings) and SSM variants. We use a model dimension ð · of 64 for Mamba and 128 for the other models. Results. Table 2 shows that Mambaâ or more precisely, its selective SSM layerâ has the ability to solve the task perfectly because of its ability to selectively remember the relevant token while ignoring everything else in between. It generalizes perfectly to million-length sequences, or 4000Ã longer than it saw during training, while no other method goes beyond 2Ã .\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 58, tokens 141, triggered by: 0.20\n",
      "\u001b[32mOut of positional encoding variants for attention models, xPos (which was designed for length extrapolation) is slightly better than the others; also note that all attention models were only tested up to sequence length 214 = 16384 due to memory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the ï¬ ndings in Poli et al. (2023). # 4.2 Language Modeling We evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on both pretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to mirror GPT3 speciï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 59, tokens 104, triggered by: 0.19\n",
      "\u001b[34mcations. We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training recipe described in Brown et al. (2020). All training details are in Appendix E.2. # 4.2.1 Scaling Laws For baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the strongest Transformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 60, tokens 214, triggered by: 0.27\n",
      "\u001b[35m11 Scaling Laws on The Pile (Sequence Length 2048) Scaling Laws on The Pile (Sequence Length 8192) 2x10\" 2x10 Hyena Hyena RWKV s RWKV â â Transformer Fy â â Transformer fd RetNet 2 â â RetNet 3+ 2 â HH wd â = Transformers |, | â â Transformert+ â â Mamba zg â â Mamba 2 2 S a 6x 10Â° 1 7 6x 10Â° 1 7 10\"? 102 10 107Â° FLOPs (log scale) FLOPs (log scale) s 8 fd 2 2 > 3 2 2 S a Figure 4: (Scaling Laws.) Models of size â 125ð to â 1.3ð µ parameters, trained on the Pile. Mamba scales better than all other attention-free models and is the first to match the performance of a very strong â Transformer++â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 61, tokens 119, triggered by: 0.21\n",
      "\u001b[31mrecipe that has now become standard, particularly as the sequence length grows. architectures (e.g. rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher learning rates). We also compare against other recent subquadratic architectures (Figure 4). All model details are in Appendix E.2. Figure 4 shows scaling laws under the standard Chinchilla (Hoï¬ mann et al. 2022) protocol, on models from â 125ð to â 1.3ð µ parameters. Mamba is the ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 62, tokens 268, triggered by: 0.28\n",
      "\u001b[32mrst attention-free model to match the performance of a very strong Transformer recipe (Transformer++) that has now become standard, particularly as the sequence length grows. We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior strong recurrent models that can also be interpreted as SSMs, due to a lack of eï¬ cient implementation leading to out-of-memory or unrealistic computation requirements. # 4.2.2 Downstream Evaluations Table 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We compare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al. 2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our models. (Note that Mamba and Pythia are trained with context length 2048, while RWKV was trained with context length 1024.) # 4.3 DNA Modeling Motivated by the success of large language models, there has been recent exploration into using the foundation model paradigm for genomics. DNA has been likened to language in that it consists of sequences of discrete tokens with a ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 63, tokens 288, triggered by: 0.20\n",
      "\u001b[34mnite vocab. It is also known for requiring long-range dependencies to model (Avsec et al. 2021). We investigate Mamba as a FM backbone for pretraining and ï¬ ne-tuning in the same setting as recent works on long-sequence models for DNA (Nguyen, Poli, et al. 2023). In particular, we focus on two explorations of scaling laws across model size and sequence length (Figure 5), and a diï¬ cult downstream synthetic classiï¬ cation task requiring long context (Figure 6). For pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training and model details (see also Appendix E.2). For the dataset, we largely follow the setup of HyenaDNA (Nguyen, Poli, et al. 2023), which uses the HG38 dataset for pretraining consisting of a single human genome with about 4.5 billion tokens (DNA base pairs) in the training split. # 4.3.1 Scaling: Model Size In this experiment, we investigate the scaling properties of genomics foundation models with various model backbones (Figure 5 Left). Training. To advantage the baselines, we train on a short sequence length of 1024; as shown in Section 4.3.2, we expect results to favor Mamba even more at longer sequence lengths.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 64, tokens 116, triggered by: 0.24\n",
      "\u001b[35mWe ï¬ x a global batch size of 1024, for a 12 Table 3: (Zero-shot Evaluations.) Best results for each size in bold. We compare against open source LMs with various tokenizers, trained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and tokenizer (GPT-NeoX-20B). For each model size, Mamba is best-in-class on every single evaluation result, and generally matches baselines at twice the model size. Model Token.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 65, tokens 168, triggered by: token limit\n",
      "\u001b[31mPile ppl â LAMBADA LAMBADA HellaSwag ppl â acc â acc â acc â acc â acc â acc â Hybrid H3-130M GPT2 â Pythia-160M Mamba-130M NeoX NeoX 29.64 10.56 89.48 38.10 16.07 25.77 33.0 44.3 31.7 30.2 35.3 64.2 61.4 64.5 44.4 43.2 48.0 24.2 24.1 24.3 50.6 51.9 51.9 40.1 40.6 44.7 Hybrid H3-360M GPT2 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 66, tokens 260, triggered by: token limit\n",
      "\u001b[32mPythia-410M Mamba-370M NeoX NeoX 9.95 8.28 12.58 10.84 8.14 48.0 51.4 55.6 41.5 40.6 46.5 68.1 66.9 69.5 51.4 52.1 55.1 24.7 24.6 28.0 54.1 53.8 55.3 48.0 48.2 50.0 Pythia-1B Mamba-790M NeoX NeoX 7.82 7.33 7.92 6.02 56.1 62.7 47.2 55.1 70.7 72.1 57.0 61.2 27.1 29.5 53.5 56.1 51.9 57.1 GPT-Neo 1.3B Hybrid H3-1.3B OPT-1.3B Pythia-1.4B RWKV-1.5B Mamba-1.4B GPT2 â GPT2 â â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 67, tokens 263, triggered by: token limit\n",
      "\u001b[34mOPT 7.51 NeoX 7.70 NeoX NeoX 6.80 7.50 11.25 6.64 6.08 7.04 5.04 57.2 49.6 58.0 61.7 56.4 64.9 48.9 52.6 53.7 52.1 52.5 59.1 71.1 71.3 72.4 71.0 72.4 74.2 56.2 59.2 56.7 60.5 60.5 65.5 25.9 28.1 29.6 28.5 29.4 32.8 54.9 56.9 59.5 57.2 54.6 61.5 52.4 53.0 55.0 55.2 54.3 59.7 GPT-Neo 2.7B Hybrid H3-2.7B OPT-2.7B Pythia-2.8B RWKV-3B Mamba-2.8B GPT2 â GPT2 â â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 68, tokens 248, triggered by: token limit\n",
      "\u001b[35mOPT 6.73 NeoX 7.00 NeoX NeoX 6.22 5.63 7.92 5.12 5.04 5.24 4.23 62.2 55.7 63.6 64.7 63.9 69.2 55.8 59.7 60.6 59.3 59.6 66.1 72.1 73.3 74.8 74.0 73.7 75.2 61.1 65.6 60.8 64.1 67.8 69.7 30.2 32.3 31.3 32.9 33.1 36.3 57.6 61.4 61.0 59.7 59.6 63.5 56.5 58.0 58.7 59.1 59.6 63.3 GPT-J-6B OPT-6.7B Pythia-6.9B RWKV-7.4B GPT2 OPT NeoX NeoX â â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 69, tokens 277, triggered by: token limit\n",
      "\u001b[31m6.51 6.31 4.10 4.25 4.45 4.38 68.3 67.7 67.1 67.2 66.3 67.2 64.0 65.5 75.4 76.3 75.2 76.1 67.0 65.6 67.3 67.8 36.6 34.9 35.5 37.5 64.1 65.5 61.3 61.0 63.0 62.9 61.7 62.5 total of 220 â 1ð tokens per batch. Models were trained for 10ð ¾ gradient steps for a total of 10ð µ tokens. Results. Figure 5 (Left) shows that Mambaâ s pretraining perplexity improves smoothly with model size, and that Mamba scales better than both HyenaDNA and Transformer++. For example, at the largest model size of â 40ð parameters, the curve shows that Mamba can match the Transformer++ and HyenaDNA models with roughly 3Ã to 4Ã fewer parameters. # 4.3.2 Scaling: Context Length In the next DNA experiment, we investigate the scaling properties of models with respect to sequence length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 70, tokens 259, triggered by: 0.17\n",
      "\u001b[32mWe only compare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at longer sequence lengths. We pretrain models on sequence lengths 210 = 1024, 212 = 4096, 214 = 16384, 216 = 65536, 218 = 262144, 220 = 1048576. We ï¬ x a model size of 6 layers by width 128 (about 1.3M-1.4M parameters). Models were trained for 20ð ¾ gradient steps for a total of â 330ð µ tokens. The longer sequence lengths used sequence length warmup similar to (Nguyen, Poli, et al. 2023). Results. Figure 5 (Right) shows that Mamba is able to make use of longer context even up to extremely long sequences of length 1M, and its pretraining perplexity improves as the context increases. On the other hand, the HyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section 3.5 on properties of the selection mechanism. In particular, LTI models cannot selectively ignore information; from a convolutional perspective, a very long convolution kernel is aggregating all information across a long sequence\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 71, tokens 300, triggered by: token limit\n",
      "\u001b[34m13 Scaling Laws on the Human Genome (HG38) Scaling Laws - Sequence Length (HG38) â â HyenaDNa 1.4m â = Mamba 1.4M â â Mamba 7M ae â â HyenaDNA 3.00 4 â Mamba â â Transformert+ 2.98 | Perplexity Perplexity 2.80 4 284 2.754 274 r T r r r ; 10Â° 107 103 10 105 10Â° Parameters (log scale) Sequence Length Figure 5: (DNA Scaling Laws.) Pretraining on the HG38 (human genome) dataset. (Left) Fixing short context length 210 = 1024 and increasing size from â 200ð ¾ to â 40ð parameters, Mamba scales better than baselines. (Right) Fixing model size and increasing sequence lengths while keeping tokens/batch and total training tokens fixed. Unlike baselines, the selection mechanism of Mamba facilitates better performance with increasing context length. Finetuning Accuracy (Species DNA Classification) 0.8] â â HyenaDNA1.4M 0.7-| â â Mamba 1.4m â â Mamba 7M mag] â â Random g 5 os 3 â 8 oA 034 024 --------------------------------- T T T T 103 10Â¢ 108 10 Sequence Length\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 72, tokens 295, triggered by: token limit\n",
      "\u001b[35mScaling Laws - Sequence Length (YouTubeMix) 1.475 â â SA+FEN 1.450 4 â â Mamba @ 1.4254 2 1.400 4 5 o 1.375 4 Â© 1.3504 1.325 4 1.300 T T T 10* 10Â° 10 Sequence Length Figure 6: (Great Apes DNA Classification.) Accuracy after fine-tuning on sequences of length 210 = 1024 up to 220 = 1048576 using pretrained models of the same context length. Nu- merical results in Table 13. Figure 7: (Audio Pretraining.) Mamba improves performance over prior state-of-the-art (Sashimi) in autoregressive audio mod- eling, while improving up to minute-long context or million- length sequences (controlling for computation). which may be very noisy. Note that while HyenaDNA claims to improve with longer context, their results do not control for computation time. # 4.3.3 Synthetic Species Classification We evaluate models on a downstream task of classifying between 5 diï¬ erent species by randomly sampling a contigu- ous segment of their DNA. This task is adapted from HyenaDNA, which used the species {human, lemur, mouse, pig, hippo}. We modify the task to be signiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 73, tokens 212, triggered by: 0.18\n",
      "\u001b[31mcantly more challenging by classifying between the ï¬ ve great apes species {human, chimpanzee, gorilla, orangutan, bonobo}, which are known to share 99% of their DNA. # 4.4 Audio Modeling and Generation For the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols (Goel et al. 2022). This model comprises 1. a U-Net backbone with two stages of pooling by a factor ð that doubles the model dimension ð · per stage, 2. alternating S4 and MLP blocks in each stage. We consider replacing the S4+MLP blocks with Mamba blocks. Experiment details are in Appendix E.4. # 4.4.1 Long-Context Autoregressive Pretraining We evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound 2017), a standard piano music dataset used by prior work consisting of 4 hours of solo piano music, sampled at a rate of\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 74, tokens 115, triggered by: 0.20\n",
      "\u001b[32m14 16000 Hz Pretraining details largely follow the standard language modeling setup (Section 4.2). Figure 7 evaluates the eï¬ ect of increasing training sequence lengths from 213 = 8192 to 220 â 106, while keeping computation ï¬ xed. (There are some slight edge cases to the way the data is curated, which may lead to kinks in the scaling curves. For example, only minute-long clips were available so the maximum sequence length is actually bounded by 60ð â 16000ð »ð § = 960000.)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 75, tokens 180, triggered by: 0.24\n",
      "\u001b[34mBoth Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba is better throughout, and the gap widens at longer lengths. The main metric is bits per byte (BPB), which is a constant factor log(2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities. We note one important detail: this is the only experiment in this paper in which we switched from the real parameterization to complex (Section 3.6). We show additional ablations in Appendix E.4. # 4.4.2 Autoregressive Speech Generation SC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting of 1-second clips sampled at 16000 Hz of the digits â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 76, tokens 296, triggered by: token limit\n",
      "\u001b[35mzeroâ through â nineâ with highly variable characteristics. We largely follow the autoregressive training setup and generation protocol of Goel et al. (2022). Table 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al. (2022): WaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette 2019), Diï¬ Wave (Z. Kong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art (and much larger) GAN- and diï¬ usion- based models. A larger model parameter-matched to the baselines further improves on ï¬ delity metrics dramatically. Table 5 takes the small Mamba model and investigates combinations of diï¬ erent architectures for the outer stages and center stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba > S4+MLP > MHA+MLP in the center blocks. Table 4: (SC09) Automated metrics for unconditional generation on a challenging dataset of fixed-length speech clips. (Top to Bottom) Autoregressive baselines, non-autoregressive baselines, Mamba, and dataset metrics.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 77, tokens 131, triggered by: 0.27\n",
      "\u001b[31mTable 5: (SC09 Model Ablations) Models with 6M parameters. In SaShiMiâ s U-Net backbone, there are 8 center blocks operat- ing on sequence length 1000, sandwiched on each side by 8 outer blocks on sequence length 4000, sandwiched by 8 outer blocks on sequence length 16000 (40 blocks total). The architecture of the 8 center blocks are ablated independently of the rest. Note that Transformers (MHA+MLP) were not tested in the more im- portant outer blocks because of efficiency constraints. Model Params NLL â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 78, tokens 261, triggered by: token limit\n",
      "\u001b[32mFID â IS â mIS â AM â SampleRNN WaveNet SaShiMi 35.0M 4.2M 5.8M 2.042 1.925 1.873 8.96 5.08 1.99 1.71 2.27 5.13 3.02 5.80 42.57 1.76 1.47 0.74 WaveGAN DiffWave + SaShiMi Mamba Mamba Train Test 19.1M 24.1M 23.0M 6.1M 24.3M - - - - - 1.852 1.860 - - 2.03 1.92 1.42 0.94 0.67 0.00 0.02 4.90 5.26 5.94 6.26 7.33 8.56 8.33 36.10 51.21 69.17 88.54 144.9 292.5 257.6 0.80 0.68 0.59 0.52 0.36 0.16 0.19\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 79, tokens 285, triggered by: token limit\n",
      "\u001b[34mOuter Center S4+MLP MHA+MLP S4+MLP S4+MLP Mamba Mamba Mamba Mamba S4+MLP MHA+MLP S4+MLP Mamba NLL â 1.859 1.867 1.859 1.850 1.853 1.852 FID â 1.45 1.43 1.42 1.37 1.07 0.94 IS â 5.06 5.42 5.71 5.63 6.05 6.26 mIS â 47.03 53.54 56.51 58.23 73.34 88.54 AM â 0.70 0.65 0.64 0.62 0.55 0.52 4.5 Speed and Memory Benchmarks We benchmark the speed of the SSM scan operation (state expansion ð = 16), as well as the end-to-end inference throughput of Mamba, in Figure 8. Our eï¬ cient SSM scan is faster than the best attention implementation that we know of (FlashAttention-2 (Dao 2023)) beyond sequence length 2K, and up to 20-40Ã faster than a standard scan implementation in PyTorch. Mamba achieves 4-5Ã\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 80, tokens 138, triggered by: 0.33\n",
      "\u001b[35mhigher inference throughput than a Transformer of similar size, since without the KV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained) would have higher inference throughput than a 5Ã smaller Transformer-1.3B. Details in Appendix E.5, which additionally includes a benchmark of memory consumption. 15 Scan vs Convolution vs Attention time (A100 80GB PCle) Inference throughput on A100 80GB (prompt length 2048) â Flashattention-2 ame ee ES 1000-1 â convolution @ 1500] mm Mamba 6.98 wwe â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 81, tokens 241, triggered by: 0.31\n",
      "\u001b[31mâ Scan (PyTorch) Py mmm Transformer 6.78 100 4 â â Scan (ours) Ei % 00M 2 a tod S 1000 B us Ff = 2 500 â = pad oid r S12 1k 2k Â«= 4k BKK 32K GK 128k 256K 512k 1 2 Hi A 16 32 oa 128 Sequence length Batch size @ = ~ Â£ Figure 8: (Efficiency Benchmarks.) (Left) Training: our efficient scan is 40Ã faster than a standard implementation. (Right) Inference: as a recurrent model, Mamba can achieve 5Ã higher throughput than Transformers. # 4.6 Model Ablations We perform a series of detailed ablations on components of our model, focusing on the setting of language modeling with size â 350M models at Chinchilla token counts (same setting as Figure 4). # 4.6.1 Architecture Table 6 investigates the eï¬ ects of the architecture (block) and its inner SSM layer (Figure 3).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 82, tokens 126, triggered by: 0.33\n",
      "\u001b[32mWe ï¬ nd that â ¢ Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very similar. â ¢ Replacing the complex-valued S4 variant from previous work with a real-valued one does not aï¬ ect performance much, suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware eï¬ ciency. â ¢ Replacing any of these with a selective SSM (S6) signiï¬ cantly improves performance, validating the motivation of Section 3. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 83, tokens 289, triggered by: token limit\n",
      "\u001b[34m¢ The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a selective layer). We also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA (a hybrid attention architecture) in Appendix E.2.2. # 4.6.2 Selective SSM Table 7 ablates the selective SSM layer by considering diï¬ erent combinations of selective â , B, and C param- eters (Algorithm 2), showing that â is the most important parameter due to its connection to RNN gating (Theorem 1). Table 8 considers diï¬ erent initializations of the SSM, which have been shown to make a large diï¬ erence in some data modalities and settings (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022). On language modeling, we ï¬ nd that simpler real-valued diagonal initializations (S4D-Real, row 3) instead of more standard complex-valued parameterizations (S4D-Lin, row 1) perform better. Random initializations also work well, consistent with ï¬ ndings from prior work (Mehta et al. 2023). Table 9 and Table 10 consider varying the dimension of the â and (B, C) projections respectively.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 84, tokens 297, triggered by: token limit\n",
      "\u001b[35mChanging them from static to selective provides the most beneï¬ t, while increasing the dimensions further generally improves performance modestly with a small increase in parameter count. Of particular note is the dramatic improvement of the selective SSM when the state size ð is increased, with over a 1.0 perplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in Sections 3.1 and 3.3. 16 Table 6: (Ablations: Architecture and SSM layer.) The Mamba block performs similarly to H3 while being simpler. In the inner layer, there is little difference among different parameterizations of LTI models, while selective SSMs (S6) provide a large improvement. More specifically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin. Model Arch. SSM Layer Perplexity Model Arch. SSM Layer Perplexity Hyena H3 H3 H3 H3 - H3 - Hyena S4 (complex) S4 (real) S6 10.24 10.30 10.34 8.95 Mamba Hyena - Mamba - - Mamba Mamba Mamba S4 (complex) S4 (real) S6 10.75 10.54 10.56 8.69 Table 7: (Ablations:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 85, tokens 139, triggered by: 0.19\n",
      "\u001b[31mSelective parameters.) â is the most im- portant parameter (Theorem 1), but using multiple selective pa- rameters together synergizes. Table 8: (Ablations: Parameterization of A.) The more standard initializations based on S4D-Lin (Gu, Gupta, et al. 2022) perform worse than S4D-Real or a random initializa- tion, when the SSM is selective. Selective A Selective B SelectiveC Perplexity \\Qx& xX Qk *Â®QX Qk Q&X 1093 10.15 9.98 9.81 8.71\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 86, tokens 272, triggered by: token limit\n",
      "\u001b[32mAð Initialization Að = â 1 Complex Real Að = â 1â 2 Að = â (ð + 1) Real Að â ¼ exp(ð ©(0, 1)) Real Field + ð ð 2 9.16 8.85 8.71 8.71 Table 9: (Ablations: Expressivity of â .) The selection mechanism of â constructs it with a projection of the input. Project- ing it even to dim. 1 provides a large in- crease in performance; increasing it fur- ther provides further improvements at the cost of a modest increase in parameters. State size fixed to ð = 16. Size of â proj. - 1 2 4 8 16 32 64 Params (M) 358.9 359.1 359.3 359.7 360.5 362.1 365.2 371.5 9.12 8.97 8.97 8.91 8.83 8.84 8.80 8.71 # Perplexity Table 10: (Ablations: SSM state dimension.) (Top) Constant B and C (Bottom) Selective B and C. Increasing the SSM state dimension ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 87, tokens 165, triggered by: 0.31\n",
      "\u001b[34m, which can be viewed as an expansion factor on the dimension of the recurrent state, can significantly improve performance for a negligible cost in parameters/FLOPs, but only when B and C are also selective. Size of â projection fixed to 64. State dimension ð Params (M) Perplexity 1 2 4 8 16 1 2 4 8 16 367.1 367.4 368.0 369.1 371.5 367.1 367.4 368.0 369.1 371.5 9.88 9.86 9.82 9.82 9.81 9.73 9.40 9.09 8.84 8.71 # 5 Discussion\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 88, tokens 141, triggered by: 0.15\n",
      "\u001b[35mWe discuss related work, limitations, and some future directions. Related Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an extended related work of SSMs and other related models. No Free Lunch: Continuous-Discrete Spectrum. Structured SSMs were originally deï¬ ned as discretizations of continuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as perceptual signals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes their weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 89, tokens 244, triggered by: 0.32\n",
      "\u001b[31m17 on data that LTI SSMs excel on. Our ablations on audio waveforms examine this tradeoï¬ in more detail. Downstream Affordances. Transformer-based foundation models (particularly LLMs) have a rich ecosystem of properties and modes of interaction with pretrained models, such as ï¬ ne-tuning, adaptation, prompting, in-context learning, instruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer alternatives such as SSMs have similar properties and aï¬ ordances. Scaling. Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source LLMs (e.g. Llama (Touvron et al. 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023) and RetNet (Y. Sun et al. 2023), which have been evaluated at the 7B parameter scale and beyond. It remains to assess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve further engineering challenges and adjustments to the model that are not discussed in this paper.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 90, tokens 126, triggered by: 0.25\n",
      "\u001b[32m# 6 Conclusion We introduce a selection mechanism to structured state space models, allowing them to perform context-dependent reasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture, Mamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance of strong Transformer models. We are excited about the broad applications of selective state space models to build foundation models for diï¬ erent domains, especially in emerging modalities requiring long context such as genomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model backbone.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 91, tokens 138, triggered by: 0.28\n",
      "\u001b[34m# Acknowledgments We thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft. # References [1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. â Unitary Evolution Recurrent Neural Networksâ . In: The International Conference on Machine Learning (ICML). 2016, pp. 1120â 1128. iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 92, tokens 170, triggered by: 0.22\n",
      "\u001b[35mEffective Gene Expression Prediction from Sequence by Integrating Long-range Interactionsâ . In: Nature Methods 18.10 (2021), pp. 1196â 1203. Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. â Using Fast Weights to Attend to the Recent Pastâ . In: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016). Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. â Layer Normalizationâ . In: arXiv preprint arXiv:1607.06450 (2016). [2] [3] [4] [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 93, tokens 144, triggered by: 0.27\n",
      "\u001b[31mNeural Machine Translation by Jointly Learning to Align and Translateâ . In: The International Conference on Learning Representations (ICLR). 2015. [6] David Balduzzi and Muhammad Ghifary. â Strongly-typed Recurrent Neural Networksâ . In: International Con- ference on Machine Learning. PMLR. 2016, pp. 1292â 1300. [7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 94, tokens 134, triggered by: 0.28\n",
      "\u001b[32mPythia: A Suite for Analyzing Large Language Models across Training and Scalingâ . In: The International Conference on Machine Learning (ICML). PMLR. 2023, pp. 2397â 2430. [8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. â PIQA: Reasoning about Physical Commonsense in Natural Languageâ . In: Proceedings of the AAAI conference on Artificial Intelligence. Vol. 34. 05. 2020, pp. 7432â 7439. [9] Guy E Blelloch. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 95, tokens 159, triggered by: 0.23\n",
      "\u001b[34mPrefix Sums and Their Applicationsâ . In: (1990). [10] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. â Quasi-recurrent Neural Networksâ . In: arXiv preprint arXiv:1611.01576 (2016). 18 [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee- lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. â Language Models are Few-shot Learnersâ . In: Advances in Neural Information Processing Systems (NeurIPS) 33 (2020), pp. 1877â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 96, tokens 294, triggered by: token limit\n",
      "\u001b[35m1901. [12] Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. â Scaling Transformer to 1M tokens and Beyond with RMTâ . In: arXiv preprint arXiv:2304.11062 (2023). [13] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. â Generating Long Sequences with Sparse Trans- formersâ . In: arXiv preprint arXiv:1904.10509 (2019). [14] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Pe- ter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. â Rethinking Attention with Performersâ . In: The International Conference on Learning Representations (ICLR). 2021. [15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. â PaLM: Scaling Language Modeling with Pathwaysâ . In: Journal of Machine Learning Research 24.240 (2023), pp. 1â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 97, tokens 299, triggered by: 0.25\n",
      "\u001b[31m113. url: http://jmlr.org/ papers/v24/22-1144.html. Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. â Empirical Evaluation of Gated Re- current Neural Networks on Sequence Modelingâ . In: arXiv preprint arXiv:1412.3555 (2014). [17] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. â Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challengeâ . In: arXiv preprint arXiv:1803.05457 (2018). [18] Tri Dao. â FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioningâ . In: (2023). [19] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. â FlashAttention: Fast and Memory- Efficient Exact Attention with IO-Awarenessâ . In: Advances in Neural Information Processing Systems (NeurIPS). 2022. [20] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher RÃ©. â Hungry Hungry Hippos:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 98, tokens 162, triggered by: 0.27\n",
      "\u001b[32mTowards Language Modeling with State Space Modelsâ . In: The International Conference on Learning Representations (ICLR). 2023. [21] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. â Language Modeling with Gated Convolu- tional Networksâ . In: The International Conference on Machine Learning (ICML). PMLR. 2017, pp. 933â 941. # [22] DeepSound. SampleRNN. https://github.com/deepsound-project/samplernn-pytorch. 2017. [23] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. â LongNet:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 99, tokens 299, triggered by: token limit\n",
      "\u001b[34mScaling Transformers to 1,000,000,000 Tokensâ . In: arXiv preprint arXiv:2307.02486 (2023). [24] Chris Donahue, Julian McAuley, and Miller Puckette. â Adversarial Audio Synthesisâ . In: The International Conference on Learning Representations (ICLR). 2019. [25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. â An Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleâ . In: The International Conference on Learning Representations (ICLR). 2020. [26] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. â A Mathematical Framework for Transformer Circuitsâ . In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 100, tokens 251, triggered by: 0.22\n",
      "\u001b[35mTransformer Circuits Thread (2021). https://transformer-circuits.pub/2021/framework/index.html. [27] Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. â Block- State Transformerâ . In: arXiv preprint arXiv:2306.09539 (2023). [28] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. â Multi-Head State Space Model for Sequence Modelingâ . In: INTERSPEECH. 2023. [29] Karl J Friston, Lee Harrison, and Will Penny. â Dynamic Causal Modellingâ . In: Neuroimage 19.4 (2003), pp. 1273â 1302. [30] Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christo- pher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 101, tokens 270, triggered by: token limit\n",
      "\u001b[31mSimple Hardware-efficient Long Convolutions for Sequence Modelingâ . In: The International Confer- ence on Machine Learning (ICML) (2023). [31] Ken-ichi Funahashi and Yuichi Nakamura. â Approximation of Dynamical Systems by Continuous Time Recur- rent Neural Networksâ . In: Neural Networks 6.6 (1993), pp. 801â 806. 19 [32] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. â The Pile: An 800GB Dataset of Diverse Text for Language Modelingâ . In: arXiv preprint arXiv:2101.00027 (2020). [33] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A Framework for Few-shot Language Model Evaluation. Version v0.0.1.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 102, tokens 122, triggered by: 0.26\n",
      "\u001b[32mSept. 2021. doi: 10.5281/zenodo.5371628. url: https://doi.org/10.5281/zenodo.5371628. [34] Karan Goel, Albert Gu, Chris Donahue, and Christopher RÃ©. â Itâ s Raw! Audio Generation with State-Space Modelsâ . In: The International Conference on Machine Learning (ICML). 2022. [35] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher RÃ©. â HIPPO:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 103, tokens 180, triggered by: 0.28\n",
      "\u001b[34mRecurrent Memory with Optimal Polynomial Projectionsâ . In: Advances in Neural Information Processing Systems (NeurIPS). 2020. [36] Albert Gu, Karan Goel, and Christopher RÃ©. â Efficiently Modeling Long Sequences with Structured State Spacesâ . In: The International Conference on Learning Representations (ICLR). 2022. [37] Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hoffman, and Razvan Pascanu. â Improving the Gating Mech- anism of Recurrent Neural Networksâ . In: The International Conference on Machine Learning (ICML). 2020. [38] Albert Gu, Ankit Gupta, Karan Goel, and Christopher RÃ©. â On the Parameterization and Initialization of Diag- onal State Space Modelsâ . In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 104, tokens 120, triggered by: 0.25\n",
      "\u001b[35mAdvances in Neural Information Processing Systems (NeurIPS). 2022. [39] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher RÃ©. â Combining Recur- rent, Convolutional, and Continuous-time Models with the Linear State Space Layerâ . In: Advances in Neural Information Processing Systems (NeurIPS). 2021. [40] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 105, tokens 298, triggered by: token limit\n",
      "\u001b[31mHow to Train Your HIPPO: State Space Models with Generalized Basis Projectionsâ . In: The International Conference on Learning Representations (ICLR). 2023. [41] Ankit Gupta, Albert Gu, and Jonathan Berant. â Diagonal State Spaces are as Effective as Structured State Spacesâ . In: Advances in Neural Information Processing Systems 35 (2022), pp. 22982â 22994. [42] David Ha, Andrew Dai, and Quoc V. Le. â HyperNetworksâ . In: The International Conference on Learning Rep- resentations (ICLR). 2017. [43] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. â Dream to Control: Learning Behav- iors by Latent Imaginationâ . In: The International Conference on Learning Representations (ICLR). 2020. [44] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. â Liquid Structural State-Space Modelsâ . In: The International Conference on Learning Representations (ICLR). 2023. [45] Mikael Henaff, Arthur Szlam, and Yann LeCun. â Recurrent Orthogonal Networks and Long-Memory Tasksâ . In: The International Conference on Machine Learning (ICML). 2016.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 106, tokens 147, triggered by: 0.27\n",
      "\u001b[32m[46] Dan Hendrycks and Kevin Gimpel. â Gaussian Error Linear Units (GELUs)â . In: arXiv preprint arXiv:1606.08415 (2016). [47] Sepp Hochreiter and JÃ¼rgen Schmidhuber. â Long Short-Term Memoryâ . In: Neural Computation 9.8 (1997), pp. 1735â 1780. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 107, tokens 146, triggered by: 0.26\n",
      "\u001b[34mAn Empirical Analysis of Compute- Optimal Large Language Model Trainingâ . In: Advances in Neural Information Processing Systems (NeurIPS) 35 (2022), pp. 30016â 30030. 48 [49] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. â Transformer Quality in Linear Timeâ . In: The Interna- tional Conference on Machine Learning (ICML). PMLR. 2022, pp. 9099â 9117. [50] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 108, tokens 205, triggered by: 0.22\n",
      "\u001b[35mDeep Learning for Time Series Classification: A Reviewâ . In: Data Mining and Knowledge Discovery 33.4 (2019), pp. 917â 963. [51] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. â Data Movement is All You Need: A Case Study on Optimizing Transformersâ . In: Proceedings of Machine Learning and Systems 3 (2021), pp. 711â 732. [52] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. â Gated Orthogonal Recurrent Units: On Learning to Forgetâ . In: Neural Computation 31.4 (2019), pp. 765â 783. [53] Rudolph Emil Kalman. â A New Approach to Linear Filtering and Prediction Problemsâ . In: (1960).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 109, tokens 119, triggered by: 0.27\n",
      "\u001b[31m20 [54] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. â Transformers are RNNs: Fast Autoregressive Transformers with Linear Attentionâ . In: International Conference on Machine Learning. PMLR. 2020, pp. 5156â 5165. [55] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. â DiffWave: A Versatile Diffusion Model for Audio Synthesisâ . In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 110, tokens 266, triggered by: 0.22\n",
      "\u001b[32mInternational Conference on Learning Representations. 2021. [56] Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. â Time-Parameterized Convolutional Neu- ral Networks for Irregularly Sampled Time Seriesâ . In: arXiv preprint arXiv:2308.03210 (2023). [57] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. â ImageNet Classification with Deep Convolutional Neural Networksâ . In: Advances in Neural Information Processing Systems (NeurIPS) 25 (2012). [58] Tao Lei. â When Attention Meets Fast Recurrence: Training Language Models with Reduced Computeâ . In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, pp. 7633â 7648. [59] Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. â Simple Recurrent Units for Highly Parallelizable Recurrenceâ . In: arXiv preprint arXiv:1709.02755 (2017). [60] Mario Lezcano-Casado and David MartÃ nez-Rubio. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 111, tokens 123, triggered by: 0.27\n",
      "\u001b[34mCheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Groupâ . In: The International Conference on Machine Learning (ICML). 2019. [61] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. â What Makes Convolutional Models Great on Long Sequence Modeling?â In: The International Conference on Learning Representations (ICLR). 2023. [62] Vasileios Lioutas and Yuhong Guo. â Time-aware Large Kernel Convolutionsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 112, tokens 230, triggered by: 0.27\n",
      "\u001b[35m. In: The International Conference on Machine Learning (ICML). PMLR. 2020, pp. 6172â 6183. [63] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behba- hani. â Structured State Space Models for In-Context Reinforcement Learningâ . In: Advances in Neural Informa- tion Processing Systems (NeurIPS). 2023. [64] Shahar Lutati, Itamar Zimerman, and Lior Wolf. â Focus Your Attention (with Adaptive IIR Filters)â . In: arXiv preprint arXiv:2305.14952 (2023). [65] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. â Mega: Moving Average Equipped Gated Attentionâ . In: The International Conference on Learning Representations (ICLR). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 113, tokens 300, triggered by: token limit\n",
      "\u001b[31m[66] Eric Martin and Chris Cundy. â Parallelizing Linear Recurrent Neural Nets Over Sequence Lengthâ . In: The International Conference on Learning Representations (ICLR). 2018. [67] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and Yoshua Bengio. â SampleRNN: An Unconditional End-to-End Neural Audio Generation Modelâ . In: The International Conference on Learning Representations (ICLR). 2017. [68] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. â Long Range Language Modeling via Gated State Spacesâ . In: The International Conference on Learning Representations (ICLR). 2023. [69] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. â Efficient Orthogonal Parametri- sation of Recurrent Neural Networks using Householder Reflectionsâ . In: International Conference on Machine Learning. PMLR. 2017, pp. 2401â 2409. [70] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher RÃ©. â S4ND: Modeling Images and Videos as Multidimensional Signals with State Spacesâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 114, tokens 266, triggered by: token limit\n",
      "\u001b[32m. In: Advances in Neural Information Processing Systems (NeurIPS). 2022. [71] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Pa- tel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. â HyenaDNA: Long-range Genomic Sequence Modeling at Single Nucleotide Resolutionâ . In: Advances in Neural Information Processing Systems (NeurIPS). 2023. [72] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. â In-context Learning and Induction Headsâ . In: Transformer Circuits Thread (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction- heads/index.html.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 115, tokens 131, triggered by: 0.24\n",
      "\u001b[34m[73] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalch- brenner, Andrew Senior, and Koray Kavukcuoglu. â WaveNet: A Generative Model for Raw Audioâ . In: arXiv preprint arXiv:1609.03499 (2016). 21 [74] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and So- ham De. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 116, tokens 162, triggered by: 0.23\n",
      "\u001b[35mResurrecting Recurrent Neural Networks for Long Sequencesâ . In: The International Conference on Machine Learning (ICML). 2023. [75] Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez. â The LAMBADA Dataset: Word Prediction Requiring a Broad Discourse Contextâ . In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 2016, pp. 1525â 1534. [76] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 117, tokens 220, triggered by: 0.20\n",
      "\u001b[31mOn the Difficulty of Training Recurrent Neural Net- worksâ . In: International Conference on Machine Learning. 2013, pp. 1310â 1318. [77] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. â RWKV: Reinventing RNNs for the Transformer Eraâ . In: arXiv preprint arXiv:2305.13048 (2023). [78] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. â Random Feature Attentionâ . In: The International Conference on Learning Representations (ICLR). 2021. [79] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 118, tokens 137, triggered by: 0.27\n",
      "\u001b[32mHyena Hierarchy: Towards Larger Convolutional Language Modelsâ . In: The International Conference on Machine Learning (ICML). 2023. [80] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. â Toeplitz Neural Network for Sequence Modelingâ . In: The International Conference on Learning Representations (ICLR). 2023. [81] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 119, tokens 296, triggered by: 0.25\n",
      "\u001b[34mThe devil in linear transformerâ . In: arXiv preprint arXiv:2210.10340 (2022). [82] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. â CosFormer: Rethinking Softmax in Attentionâ . In: The International Conference on Learning Representations (ICLR). 2022. [83] Ali Rahimi and Benjamin Recht. â Random features for large-scale kernel machinesâ . In: Advances in neural information processing systems 20 (2007). [84] Prajit Ramachandran, Barret Zoph, and Quoc V Le. â Swish: A Self-gated Activation Functionâ . In: arXiv preprint arXiv:1710.05941 7.1 (2017), p. 5. [85] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. â CKConv: Con- tinuous Kernel Convolution For Sequential Dataâ . In: arXiv preprint arXiv:2102.02611 (2021). [86] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 120, tokens 135, triggered by: 0.22\n",
      "\u001b[35mWinogrande: An Adversarial Wino- grad Schema Challenge at Scaleâ . In: Communications of the ACM 64.9 (2021), pp. 99â 106. [87] George Saon, Ankit Gupta, and Xiaodong Cui. â Diagonal State Space Augmented Transformers for Speech Recognitionâ . In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2023, pp. 1â 5. Imanol Schlag, Kazuki Irie, and JÃ¼rgen Schmidhuber. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 121, tokens 169, triggered by: 0.26\n",
      "\u001b[31mLinear Transformers are Secretly Fast Weight Program- mersâ . In: The International Conference on Machine Learning (ICML). PMLR. 2021, pp. 9355â 9366. [89] Noam Shazeer. â GLU Variants Improve Transformerâ . In: arXiv preprint arXiv:2002.05202 (2020). [90] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael SchÃ¤rli, and Denny Zhou. â Large Language Models can be Easily Distracted by Irrelevant Contextâ . In: The International Conference on Machine Learning (ICML). PMLR. 2023, pp. 31210â 31227.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 122, tokens 209, triggered by: 0.27\n",
      "\u001b[32mJiaxin Shi, Ke Alexander Wang, and Emily Fox. â Sequence Modeling with Multiresolution Convolutional Mem- oryâ . In: The International Conference on Machine Learning (ICML). PMLR. 2023, pp. 31312â 31327. Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. â Simplified State Space Layers for Sequence Modelingâ . In: The International Conference on Learning Representations (ICLR). 2023. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. â Roformer: Enhanced Trans- former with Rotary Position Embeddingâ . In: arXiv preprint arXiv:2104.09864 (2021). [93] [94] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 123, tokens 103, triggered by: 0.22\n",
      "\u001b[34mRetentive network: A successor to transformer for large language modelsâ . In: arXiv preprint arXiv:2307.08621 (2023). Ilya Sutskever, Oriol Vinyals, and Quoc V Le. â Sequence to Sequence Learning with Neural Networksâ . In: Advances in Neural Information Processing Systems (NeurIPS) 27 (2014). 22 [96] Corentin Tallec and Yann Ollivier. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 124, tokens 270, triggered by: token limit\n",
      "\u001b[35mCan Recurrent Neural Networks Warp Time?â In: The International Con- ference on Learning Representations (ICLR). 2018. [97] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Se- bastian Ruder, and Donald Metzler. â Long Range Arena: A Benchmark for Efficient Transformersâ . In: Inter- national Conference on Learning Representations (ICLR). 2021. [98] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. â Efficient Transformers: A Surveyâ . In: ACM Com- puting Surveys 55.6 (2022), pp. 1â 28. [99] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Bap- tiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. â Llama: Open and Efficient Foundation Language Modelsâ . In: arXiv preprint arXiv:2302.13971 (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 125, tokens 244, triggered by: 0.29\n",
      "\u001b[31m[100] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. â Attention Is All You Needâ . In: Advances in Neural Information Processing Systems (NeurIPS). 2017. [101] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. â On Orthogonality and Learning Recur- rent Networks with Long Term Dependenciesâ . In: International Conference on Machine Learning. PMLR. 2017, pp. 3570â 3578. Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. â Selective Structured State-Spaces for Long-form Video Understandingâ . In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 6387â 6397. [102] [103] Pete Warden. â Speech Commands: A Dataset for Limited-Vocabulary Speech Recognitionâ . In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 126, tokens 296, triggered by: token limit\n",
      "\u001b[32mArXiv abs/1804.03209 (2018). [104] Samuel Williams, Andrew Waterman, and David Patterson. â Roofline: An Insightful Visual Performance Model for Multicore Architecturesâ . In: Communications of the ACM 52.4 (2009), pp. 65â 76. [105] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. â CondConv: Conditionally Parameterized Con- volutions for Efficient Inferenceâ . In: Advances in Neural Information Processing Systems (NeurIPS) 32 (2019). [106] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. â HellaSwag: Can a Machine Really Finish Your Sentence?â In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguis- tics. 2019. [107] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. â An Attention Free Transformerâ . In: arXiv preprint arXiv:2105.14103 (2021). [108] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 127, tokens 192, triggered by: 0.20\n",
      "\u001b[34mEffectively Modeling Time Series with Simple Discrete State Spacesâ . In: The International Conference on Learning Representations (ICLR). 2023. [109] Lin Zheng, Chong Wang, and Lingpeng Kong. â Linear complexity randomized self-attention mechanismâ . In: International Conference on Machine Learning. PMLR. 2022, pp. 27011â 27041. [110] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. â Efficient Long Sequence Modeling via State Space Augmented Transformerâ . In: arXiv preprint arXiv:2212.08136 (2022). 23 # A Discussion: Selection Mechanism Our selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence. It can also be viewed as related to â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 128, tokens 291, triggered by: token limit\n",
      "\u001b[35mfast weightsâ (J. Ba et al. 2016), which connects classical RNNs with the mechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct concept that is worth clarifying. Gating. Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and Schmidhuber 1997) and GRU (J. Chung et al. 2014), or the gated equation (5)n Theorem 1. This was interpreted as a particular mechanism for controlling whether to let an input into the hidden state of an RNN. In particular, this aï¬ ects the propagation of signal through time and causes inputs to interact along the sequence length dimension. However, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction (often with an activation function). For example, elementwise multiplicative components of neural network architectures (that do not interact along sequence length) are now commonly referred to as gated architectures (Hua et al. 2022; Mehta et al. 2023), despite a very diï¬ erent meaning than the original RNN sense. Thus we believe the original concept of RNN gating versus the popular usage of multiplicative gating actually have a very diï¬ erent semantic meaning. Hypernetworks.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 129, tokens 101, triggered by: 0.22\n",
      "\u001b[31mHypernetworks refer to neural networks whose parameters are themselves generated by smaller neural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to deï¬ ne a large RNN whose recurrent parameters are generated by a smaller RNN. Data-dependence. Similar to hypernetworks, data-dependence can refer to any notion where some parameters of the model depend on the data (Poli et al. 2023). Example:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 130, tokens 125, triggered by: 0.33\n",
      "\u001b[32mGLU Activation. To illustrate the issues with these concepts, consider a simple diagonal linear layer ð ¦ = Dð ¥, where D is a diagonal weight parameter. Now suppose that D is itself generated from a linear transformation of ð ¥, with an optional nonlinearity: D = ð (W ð ¥). Since it is diagonal, the multiplication becomes an elementwise product: ð ¦ = ð (W ð ¥)â ¦ð ¥. This is a rather trivial transformation, yet it technically satisï¬ es the common meanings of gating (since it has a multiplicative â branchâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 131, tokens 162, triggered by: 0.32\n",
      "\u001b[34m), hypernetworks (since the parameter D is generated by another layer), and data-dependent (since D depends on the data ð ¥). However, this in fact simply deï¬ nes a GLU function, which is so simple that it is often considered just an activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer. Selection. Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating, hypernetworks, or data-dependence, so can an enormous range of other constructionsâ essentially anything with a multiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) as wellâ and we ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 132, tokens 231, triggered by: 0.20\n",
      "\u001b[35mnd it uninformative to think of them as such. Instead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case (Theorem 1) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization of â (Funahashi and Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term â gatingâ in favor of selection to clarify the overloaded use of former. More narrowly, we use selection to refer to the mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence length (Section 3.1). Beyond selective SSMs and gated RNNs, other examples may include input-dependent convolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and Guo 2020; Lutati, Zimerman, and Wolf 2023; Yang et al. 2019) and even attention.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 133, tokens 154, triggered by: 0.34\n",
      "\u001b[31m24 # B Related Work We overview several prior works related to our methods. We mention that some of the most closely related models include recurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet, and RWKV. # B.1 S4 Variants and Derivatives We describe a brief overview of some structured SSMs from past work, particularly those that have a relation to our method. â ¢ S4 (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021) introduced the ï¬ rst structured SSM, describing diagonal structure and diagonal plus low-rank (DPLR).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 134, tokens 154, triggered by: 0.30\n",
      "\u001b[32mIt focused on eï¬ cient convolutional algorithms for DPLR SSMs due to a connection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020). â ¢ DSS (Gupta, Gu, and Berant 2022) ï¬ rst discovered the empirical eï¬ ectiveness of diagonal structured SSMs by approximating the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022). â ¢ S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is the ï¬ rst S4 model to be computed recurrently with the parallel scan.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 135, tokens 195, triggered by: 0.19\n",
      "\u001b[34mHowever, this required lowering the eï¬ ective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) to MIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but diï¬ ers by (i) keeping the SISO dimensions, which provides a larger eï¬ ective recurrent state, (ii) using a hardware-aware algorithm to overcome the computation issue, (iii) adding the selection mechanism. Lu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories. Their mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where A is manually set to 0, instead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically to this setting and probe if the model has learned to automatically reset its state on episode boundaries.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 136, tokens 289, triggered by: token limit\n",
      "\u001b[35mâ ¢ Mega (Ma et al. 2023) introduced a simpliï¬ cation of S4 to be real- instead of complex- valued, giving it an interpretation of being an exponential moving average (EMA). They additionally make an interesting connection of the discretization step of SSMs to an EMA damping term. Contrary to ï¬ ndings in the original S4 papers, this was the ï¬ rst model to show that real-valued SSMs are empirically eï¬ ective in certain settings or when combined with diï¬ erent architectural components. â ¢ Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this perspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally and close to LTI. â ¢ SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox 2023), and Toeplitz Neural Network (Qin, Han, W. Sun, He, et al. 2023) all focus on the convolutional representation of S4 and create global or long convolution kernels with diï¬ erent parameterizations.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 137, tokens 200, triggered by: 0.18\n",
      "\u001b[31mHowever, these methods cannot do fast autoregressive inference directly. Notably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually strictly LTI (linear time invariant). # B.2 SSM Architectures We use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating one of the previous SSMs as a black box layer. â ¢ GSS (Mehta et al. 2023) was the ï¬ rst gated neural network architecture incorporating SSMs. It is motivated by the gated attention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most importantly, its projection contracts the model dimension to reduce the state size of the SSM, while ours expands the model dimension in order to increase the state size, based on the motivation in Section 3.1.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 138, tokens 294, triggered by: token limit\n",
      "\u001b[32m25 â ¢ Mega (Ma et al. 2023) combined the EMA simpliï¬ cation of S4 described above into a hybrid architecture using an eï¬ cient attention approximation. â ¢ H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It is the ï¬ rst to generalize this formulation of linear attention to more general recurrences, which is also the basis of later architectures. â ¢ Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied on the input. While sharing the â selectionâ name, we consider this an architectural modiï¬ cation that is closer to architectural gating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective Copying task because simply masking out the irrelevant inputs does not aï¬ ect the spacing between the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0). â ¢ RetNet (Y. Sun et al. 2023) is also based on Linear Attention and very similar to H3, but reduces the inner S4 layer to a special case where the state dimension is ð = 1.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 139, tokens 131, triggered by: 0.20\n",
      "\u001b[34mAlthough not framed as such, its recurrence can be viewed as a special case of a linear SSM. Its primary source of improvement is using a linear attention with large head dimension, which can be viewed as another method to perform input-dependent state expansion. Using a larger head dimension in the context of linear attention variants was ï¬ rst done by H3, but not extensively used since this requires a proportional amount of extra computation. RetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention instead of convolutions, made feasible by their particular special case of SSMs which acts as a simple EMA.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 140, tokens 293, triggered by: 0.13\n",
      "\u001b[35mâ ¢ RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT (attention-free Transformer (S. Zhai et al. 2021)), another variant of linear attention. Its main â WKVâ mechanism involves LTI recurrences and can be seen as the ratio of two SSMs. We also highlight the gated attention unit (GAU) from Hua et al. (2022), which was motivated by combining the Transformerâ s MHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining the H3 and MLP blocks. # B.3 Relationship to RNNs RNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state. Several older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016), and simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without time-wise nonlinearities. Because of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs, and are thus more powerful in a sense than the family of LTI structured SSMs above.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 141, tokens 298, triggered by: token limit\n",
      "\u001b[31mThe main diï¬ erences are: â ¢ They do not use state expansion (ð = 1) or selective B, C parameters, both of which are important for performance (Section 4.6). â ¢ They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism + discretization (Theorem 1). The connections to principled SSM theory provides better parameterizations and initializations (Section 3.6). Additionally, older RNNs famously suï¬ ered from eï¬ ciency issues and the vanishing gradients problem (Pascanu, Mikolov, and Bengio 2013), both caused by their sequential nature. The latter could be solved for some of the above RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the former was diï¬ cult without theory later developed for SSMs. For example, modern structured SSMs diï¬ er in more careful parameterization of the recurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson, Goel, et al. 2021; Gu, Johnson, Timalsina, et al. 2023)), or direct analysis (Orvieto et al. 2023)). We also note that there is a long line of work on orthogonal RNNs (Arjovsky, Shah, and Bengio 2016; Henaï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 142, tokens 206, triggered by: 0.20\n",
      "\u001b[32m, Szlam, and LeCun 2016; Lezcano-Casado and MartÃ nez-Rubio 2019; Mhammedi et al. 2017; Vorontsov et al. 2017) 26 which are motivated by constraining the A transition matrix to be orthogonal or unitary, in order to control its eigenvalues and prevent the vanishing gradient problem. However, these had other limitations; we believe that these stem from the fact that orthogonal/unitary RNNs are also LTI. For example, they are almost always evaluated on the Copying task which they can solve perfectly, but observed to struggle on the Selective Copying task (Jing et al. 2019). # B.4 Linear Attention The Linear Attention (LA) (Katharopoulos et al. 2020) framework is an important result popularizing kernel attention and showing how it relates to recurrent autoregressive models. Many variants have proposed alternative kernels and other modiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 143, tokens 109, triggered by: 0.28\n",
      "\u001b[34mcations. Random Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature map to approximate softmax attention (i.e. the exp feature map) using the random Fourier feature approximation of Gaussian kernels (Rahimi and Recht 2007). Performer (Choromanski et al. 2021) ï¬ nds an approximation to the exponential kernel involving only positive features, which also allows the softmax normalization term. TransNormer (Qin, Han, W.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 144, tokens 217, triggered by: 0.24\n",
      "\u001b[35mSun, D. Li, et al. 2022) showed that the LA denominator term can be unstable and proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al. 2022) augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality. Linear Randomized Attention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importance sampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformed numerator). Aside from kernel attention, many other variants of eï¬ cient attention exist; the survey Tay, Dehghani, Bahri, et al. (2022) oï¬ ers an extensive categorization of many of these. # B.5 Long Context Models Long context has become a popular subject, and several recent models have claimed to scale to longer and longer sequences. However, these are often from a computational standpoint and have not been extensively validated. These include:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 145, tokens 102, triggered by: 0.36\n",
      "\u001b[31mâ ¢ Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023), a lightweight wrapper around a Transformer backbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization tasks; their main result is similar to our Induction Heads extrapolation experiment (Table 2). â ¢ LongNet (Ding et al. 2023), which claimed to scale to 1B length but only evaluated on length < 100ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 146, tokens 138, triggered by: 0.39\n",
      "\u001b[32m¾ for actual tasks. â ¢ Hyena and HyenaDNA (Nguyen, Poli, et al. 2023; Poli et al. 2023), which claimed to leverage up to 1M context. However, their experiments trained on proportionally more data at longer contexts, making it hard to conclude if quality improvements at 1M context are due to context length or due to more data and computation. â ¢ Sparse Transformer (Child et al. 2019) showed a proof-of-concept of using a strided sparse attention Transformer to model audio waveforms of length 220 = 1048576, although did not discuss performance tradeoï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 147, tokens 110, triggered by: 0.34\n",
      "\u001b[34ms when controlling for computation and model size. In contrast, we believe this work presents one of the ï¬ rst approaches to meaningfully demonstrate increasing performance with longer context. # C Mechanics of Selective SSMs Proof of Theorem 1. Consider a selective SSM (Algorithm 2) with ð = 1, A = â 1, B = 1, ð â = ð «ð ð ð ¾ð ºð (ð ¥), ð â = ð ð ð ¿ð ð ð ð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 148, tokens 155, triggered by: 0.41\n",
      "\u001b[35m. The corresponding continuous-time SSM (1) is â (ð ¡) = â â (ð ¡) + ð ¥(ð ¡) which is also called a leaky integrator. 27 The discretization step size is The discretization step size is # â ð ¡ = ð â (ð ¯ð ºð ð ºð ð ¾ð ð ¾ð + ð â (ð ¥ð ¡)) = ð ð ð ¿ð ð ð ð ð (ð ¯ð ºð ð ºð ð ¾ð ð ¾ð + ð «ð ð ð ¾ð ºð (ð ¥ð ¡)) = ð ð ð ¿ð ð ð ð ð (ð «ð ð ð ¾ð ºð (ð ¥ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 149, tokens 163, triggered by: 0.31\n",
      "\u001b[31m¡)) where we observe that the parameter can be viewed as a learnable bias and folded into the linear projection. Now applying the zero-order hold (ZOH) discretization formulas: Að ¡ = exp(â A) = 1 1 + exp(ð «ð ð ð ¾ð ºð (ð ¥ð ¡) = ð (â ð «ð ð ð ¾ð ºð (ð ¥ð ¡)) = 1 â ð (ð «ð ð ð ¾ð ºð (ð ¥ð ¡)) Bð ¡ = (â A)â 1(exp(â A) â I) â â B = â (exp(â A) â I) = 1 â A = ð (ð «ð ð ð ¾ð ºð (ð ¥ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 150, tokens 166, triggered by: 0.27\n",
      "\u001b[32m¡)). Thus the final discrete recurrence (2a) is ð ð ¡ = ð (ð «ð ð ð ¾ð ºð (ð ¥ð ¡)) â ð ¡ = (1 â ð ð ¡)â ð ¡â 1 + ð ð ¡ð ¥ð ¡ as desired. # D Hardware-aware Algorithm For Selective SSMs Without input-dependent selectivity, SSMs can be eï¬ ciently implemented as a convolution (Dao, Fu, Saab, et al. 2023; Gu, Goel, and RÃ© 2022), which leverages the fast Fourier transform (FFT) as primitive. With selectivity, SSMs are no-longer equivalent to convolution, but we leverage the parallel associative scan. While SSM scans are theoretically eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 151, tokens 297, triggered by: token limit\n",
      "\u001b[34mcient (ð (ð µð ¿ð ·ð ) FLOPs, scaling linear in ð ¿), training foundation models with selective SSMs requires them to be eï¬ cient on modern hardware (GPUs) as well. We describe how we use kernel fusion and recomputation to make SSM scan fast and memory-eï¬ cient. We evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5, showing that it is up to 7Ã times faster than attention at sequence length 32K, and is as memory-eï¬ cient as the best attention implementation (FlashAttention). Speed. On modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by memory-bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This the case with our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to signiï¬ cant speedup compared to a standard implementation. The standard way to implement the scan algorithm in Section 3.2 is to prepare the scan input A, B of size (ð µ, ð ¿, ð ·, ð ) in GPU HBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel associative scan implementation to write the scan output of size (ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 152, tokens 115, triggered by: 0.25\n",
      "\u001b[35mµ, ð ¿, ð ·, ð ) to GPU HBM, then multiply that scan output with C to produce an output of size (ð µ, ð ¿, ð ·). However, this requires the number of memory reads/writes on the order of ð (ð µð ¿ð ·ð ). We can instead fuse the discretization step, the scan, and the multiplication with C into one kernel: 1. We read in ð (ð µð ¿ð · + ð ·ð ) bytes of memory (â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 153, tokens 153, triggered by: 0.25\n",
      "\u001b[31m, A, B, C) from slow HBM to fast SRAM. 2. We discretize to produce A, B of size (ð µ, ð ¿, ð ·, ð ) in SRAM. 3. We perform a parallel associative scan, yielding intermediate states of size (ð µ, ð ¿, ð ·, ð ) in SRAM. 4. We multiply and sum with C, producing outputs of size (ð µ, ð ¿, ð ·) and write it to HBM. This way, we reduce IOs by a factor of ð (ð ) (the state dimension), which in practice speeds up the operation by 20-40 times (Section 4.5).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 154, tokens 322, triggered by: token limit\n",
      "\u001b[32m28 Table 11: (Induction heads.) Models are trained on sequence length 2Â° = 256, and tested on various sequence lengths of 2Â° = 64 up to 2Â° = 1048576. Y denotes perfect generalization accuracy, while X denotes out of memory. Model Params Test Accuracy (%) at Sequence Length 26 7 28 29 210 gl 212 913 214915216 917918919920 MHA-Abs 137K v 99.6 100.0 58.6 266 188 98 10.9 7.8 X x x x x x MHA-RoPE = 137K v v 100.0 83.6 31.3 184 8.6 9.0 5.5 xX x x x x x MHA-xPos 137K v v 100.0 99.6 67.6 254 7.0 9.0 78 =X x x x x x H3 153K v v 100.0 80.9 39.5 238 148 82 59 66 82 47 82 63 74 Hyena 69M* 977 Vo 100.0 Vv 441 125 66 5.1 70 #59 66 66 59 63 98 Mamba 74K v v 100.0 Vv v v v v v v v v v v v\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 155, tokens 253, triggered by: 0.23\n",
      "\u001b[34mâ Most of the parameters are in learnable positional encodings. For sequence length ð ¿ too long where we cannot ï¬ t the sequence in SRAM (which is much smaller than HBM), we split the sequences into chunks and perform the fused scan on each chunk. As long as we have the intermediate scan states, we can continue the scan with the next chunk. Memory. We describe how we use the classical technique of recomputation to reduce the total amount of memory required to train selective SSM layers. From the way we fuse the forward pass, we do not save the intermediate states of size (ð µ, ð ¿, ð ·, ð ) to avoid memory blowup. However, these intermediate states are necessary for the backward pass to compute gradients. We instead recompute those intermediate states in the backward pass. Since the inputs â , A, B, C and output gradient read from HBM to SRAM are of size ð (ð µð ¿ð + ð ·ð ), and the input gradients are also of size ð (ð µð ¿ð + ð ·ð ), recomputation avoids the cost of reading ð (ð µð ¿ð ð ·) elements from HBM.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 156, tokens 219, triggered by: 0.21\n",
      "\u001b[35mThis means that recomputation of the SSM states in the backward pass speeds up the computation compared to storing them and reading them from HBM. Beyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize the memory requirement of the entire selective SSM block (input projection, convolution, activation, scan, output projection). In particular, we do not save intermediate activations that take a lot of memory but are fast to recompute (e.g. output of activation function or short convolution). As a result, the selective SSM layer has the same memory requirement as an optimized Transformer implementation with FlashAttention. In particular, each attention layer (FlashAttention) stores around 12 bytes of activations per token, an each MLP layer stores around 20 bytes of activations per token, for a total of 32 bytes ((assuming mixed-precision training in FP16 or BF16)). Each selective SSM stores around 16 bytes of activations per token. Hence two layers of selective SSMs have around the same activation memory as an attention layer and an MLP layer.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 157, tokens 133, triggered by: 0.22\n",
      "\u001b[31m# E Experimental Details and Additional Results # E.1 Synthetic Tasks Selective Copying. Our setting is on sequences of length 4096, with a vocab size of 16 possible tokens (including the white â noiseâ token from Figure 2) and requiring models to memorize 16 â dataâ tokens. We use 2 layer models with a model dimension of ð · = 64. Models are trained for 400K steps at a constant learning rate of 0.0001 with a batch size of 64. Induction Heads. Training consists of randomly generating data every step, with a batch size of 8. We choose an â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 158, tokens 167, triggered by: 0.20\n",
      "\u001b[32mepochâ size of 8192 steps, and track the accuracy on ï¬ xed validation sets (also randomly generated) of each target sequence length. For the MHA-Abs and Mamba models, results are reported after the 25th epoch (8192 Ã 25 = 204800 steps). For the MHA-RoPE and MHA-xPos models, results are reported after the 50th epoch (8192 Ã 50 = 409600 steps). For the LTI H3 and Hyena models, results are reported after the 10th epoch (81920 steps) because they had converged by then and failed to improve further. 29 Table 12: (Scaling Law Model Sizes.) Our model sizes and hyperparameters for scaling experiments. (Model dimension and number of heads applies only to Transformer models.)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 159, tokens 165, triggered by: 0.20\n",
      "\u001b[34mParams ð _ð ð ð ¢ð ð ð ð _ð ð ð ð ð ð _ð ð ð ð ð / ð _ð ð ð ð Training steps Learning Rate Batch Size Tokens 125M 350M 760M 1.3B 12 24 24 24 768 1024 1536 2048 12 / 64 16 / 64 16 / 96 32 / 64 4800 13500 29000 50000 6e-4 3e-4 2.5e-4 2e-4 0.5M tokens 0.5M tokens 0.5M tokens 0.5M tokens 2.5B 7B 15B 26B\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 160, tokens 196, triggered by: 0.26\n",
      "\u001b[35mWe use the Adam optimizer with no weight decay. All models are trained at constant learning rates 2ð â 4 and 1ð â 3, and the better results are reported for each model (2ð â 4 for all models except Mamba). The attention and Hyena models did not learn at LR 1ð â 3. H3 learned at both LRs, but interestingly generalized better to shorter sequences at the smaller LR of 2ð â 4. Mamba learned at both LRs, but extrapolated better at the larger LR of 1ð â 3. # E.2 Language Modeling # E.2.1 Scaling Law Details All models were trained on the Pile. Model Sizes. Table 12 speciï¬ es the model sizes we use for scaling laws. This is taken directly from the GPT3 speciï¬ cations (Brown et al. 2020), with very minor modiï¬ cations.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 161, tokens 100, triggered by: 0.25\n",
      "\u001b[31mFirst, we changed the batch size of the 1.3B model from 1M tokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch size. Second, we changed the number of training steps and total tokens to roughly match Chinchilla scaling laws (Hoï¬ mann et al. 2022), which specify that training tokens should increase proportionally to model size. Training Recipes. All models used the AdamW optimizer with â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 162, tokens 180, triggered by: 0.23\n",
      "\u001b[32m¢ gradient clip value 1.0 â ¢ weight decay 0.1 no dropout linear learning rate warmup with cosine decay By default, the peak learning rate is the GPT3 speciï¬ cation. We give several models an â improved recipeâ , inspired by changes adopted by popular large language models such as PaLM (Chowdhery et al. 2023) and LLaMa (Touvron et al. 2023). These include: â ¢ linear learning rate warmup with cosine decay to 1ð â 5, with a peak value of 5Ã the GPT3 value no linear bias terms RMSNorm instead of LayerNorm â ¢ AdamW hyperparameter ð ½ = (.9, .95) (the GPT3 value) instead of the PyTorch default of ð ½ = (.9, .999)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 163, tokens 141, triggered by: 0.23\n",
      "\u001b[34mArchitecture and Training Details. Our models are: â ¢ Transformer: The standard Transformer based on GPT3 (Table 12). â ¢ Transformer++: A Transformer with an improved architecture, namely rotary positional encodings (Su et al. 2021) and SwiGLU MLP (Shazeer 2020), and the improved training recipe above. â ¢ Hyena: Interleaving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an MLP) with standard MLP blocks. The MLP blocks have expansion factor 2 instead of 4 and the number of layers is correspondingly increased by 1.5Ã to preserve parameter count.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 164, tokens 143, triggered by: 0.20\n",
      "\u001b[35m30 â ¢ H3++: The H3 architecture with a few modiï¬ cations, including (i) using the same â thinâ Hyena dimensions above (ii) the improved training recipe above (iii) a linear attention head dimension of 8. â ¢ RWKV: The default RWKV model from B. Peng et al. (2023), including its modiï¬ ed MLP block. We also used as much of its speciï¬ ed training recipe as possible, such as increasing the learning rates by 2Ã or 3Ã on certain parameters. â ¢ RetNet: The default RetNet model from Y. Sun et al. (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 165, tokens 132, triggered by: 0.24\n",
      "\u001b[31mWe also gave it the improved training recipe above. â ¢ Mamba: The standard Mamba architecture, with the improved training recipe. # E.2.2 Additional Scaling Law Ablations We perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws in Figure 4 (Left). Mamba Architecture: Interleaving Blocks. We test the eï¬ ect of diï¬ erent architectural blocks combined with the Mamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with an extra ð ¼ð ð ð â ð ²ð ²ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 166, tokens 262, triggered by: 0.16\n",
      "\u001b[32m¬ path added. This leads to two natural ablations: â ¢ What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This can also be interpreted as taking Mamba and removing half of the SSMs. â ¢ What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as taking a Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks. Figure 9 (Right) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly, neither change matters too much. The Mamba-MLP architecture is only slightly worse, and still better than all models except Transformer++. The Mamba-MHA architecture is only slightly better, which is somewhat surprising in light of the fact that many recent works have found that combining (LTI) SSMs with Attention can lead to substantial improvements (Dao, Fu, Saab, et al. 2023; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta, and Cui 2023; Zuo et al. 2022). H3 Architecture:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 167, tokens 141, triggered by: 0.25\n",
      "\u001b[34mTraining Recipes. Next we ablate diï¬ erences between the Hyena and H3++ models, our weakest and strongest models outside of Transformer++ and Mamba, particularly to isolate the eï¬ ect of training recipes. â ¢ Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4). â ¢ Hyena+: The same architecture but with the improved training recipe described above. â ¢ H3+: The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution kernel. â ¢ H3++: The same as H3+, but with a linear attention head dimension of 8.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 168, tokens 106, triggered by: 0.24\n",
      "\u001b[35mThis increases computation inside the SSM recurrence but does not increase parameters. Our general convention is that â Model+â represents the base model with the improved training recipe, and â Model++â also allows for architectural changes. Figure 9 (Right) shows that A large improvement is achieved by the improved training recipe, which was used for many of the models in the main Figure 4 (RetNet, H3++, Transformer++, Mamba). The choice of the inner LTI SSM does not matter (e.g.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 169, tokens 139, triggered by: 0.25\n",
      "\u001b[31mHyena vs. S4), consistent with ï¬ ndings throughout this paper. The head dimension expansion improves performance, consistent with one of our main themes that expanded state dimension improves performance for SSMs (Section 3). 31 Scaling Laws on The Pile (Sequence Length 2048) Scaling Laws on The Pile (Sequence Length 2048) â â Mamba Hyena Mamba-mLp | = â Hyenas â â Members |g â â He a â He 3 Sox! = 2104 ext? 5 2S 7x0 Ea 1 1 1 1 10 30 10Â° 10â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 170, tokens 111, triggered by: 0.26\n",
      "\u001b[32mFLOPS (log scale) FLOPs (log scale) s 5 2 3 2 = 3 8 Figure 9: (Scaling laws: extra ablations.) (Left) Instead of (Right) Instead of # E.2.3 Downstream Evaluation Details This pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens. For the 1.3B model, we use a batch size of 1M tokens to be consistent with the GPT3 speciï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 171, tokens 239, triggered by: 0.32\n",
      "\u001b[34mcations. We report the perplexity on the Pile validation set, and for this metric only compare to models trained on the same dataset and with the same tokenizer, in particular Pythia and RWKV. For downstream evaluation, we use the LM evaluation harness from EleutherAI (L. Gao, Tow, et al. 2021), as done by most work in this area. We evaluate on the following tasks/datasets that measure common sense reasoning: â ¢ LAMBADA (Paperno et al. 2016). â ¢ HellaSwag (Zellers et al. 2019). â ¢ PIQA (Bisk et al. 2020). â ¢ ARC-challenge (P. Clark et al. 2018). â ¢ ARC-easy: an easy subset of ARC-challenge. â ¢ WinoGrande (Sakaguchi et al. 2021). We report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence length for HellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these task).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 172, tokens 277, triggered by: 0.28\n",
      "\u001b[35m# E.3 DNA Modeling # E.3.1 Pretraining Details We describe the dataset and training procedure of the HG38 pretraining task in more detail. The dataset follows the splits from the prior Enformer work on genomics (Avsec et al. 2021); the training split contains a total of ð = 34021 segments of length 217 = 131072 that cover the genome, for a total of approximately 4.5 billion tokens (DNA base pairs). These segments are pairs of (chromosome number, starting index, ending index), and can be extended if necessary (e.g. to get longer segments). We deviate from HyenaDNA when the training sequence length is not 217. HyenaDNA always takes a ï¬ xed sub-segment (e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length each epoch is ï¬ xed to 34021 samples and doesnâ t necessarily go through the whole genome. On the other hand, we use the entire training data: â ¢ When the context length ð ¿ is less than (or equal to) 217, we divide up each segment into non-overlapping sub-segments of length ð ¿, so that there are ð Ã 217 ð ¿ total samples and ð Ã 217 â 4.5ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 173, tokens 102, triggered by: 0.23\n",
      "\u001b[31mµ tokens per epoch. â ¢ When the context length ð ¿ is greater than 217, we turn each segment into two samples, one that begins with the prescribed segment and one that ends with the prescribed segment. Thus each epoch has 2ð items and 2ð ð ¿ 32 tokens per epoch. For example, at sequence length 218 = 262144 there are 4Ã as many tokens as the default, and at sequence length 220 there are 16Ã as many tokens.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 174, tokens 155, triggered by: 0.27\n",
      "\u001b[32mOther training details generally follow the same protocol as our language modeling experiments (Appendix E.2). For example, we use the AdamW with (ð ½1, ð ½2) = (0.9, 0.95), no dropout, weight decay 0.1. We use a cosine learning rate scheduler with linear warmup for 10% of total steps. # E.3.2 Scaling: Model Size Details Models. The models we consider are: â ¢ Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su et al. 2021). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani et al. 2017).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 175, tokens 162, triggered by: 0.27\n",
      "\u001b[34mâ ¢ HyenaDNA: the Hyena model from Nguyen, Poli, et al. (2023) and Poli et al. (2023), which is roughly a Transformer with the MHA block replaced by an H3 block using a global convolution parameterized by an MLP. â ¢ Mamba: the standard Mamba architecture. Model Sizes. We use the following model sizes. Blocks Model Dimension Params (Approx.) 4 64 250K 700K 1.4M 3.5M 7.0M 19.3M 40.7M 5 96 6 128 7 192 8 256 10 384 12 512 Note that the number of blocks for Mamba is doubled, because one Transformer â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 176, tokens 208, triggered by: 0.16\n",
      "\u001b[35mlayerâ includes both the MHA and MLP blocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4). Training. For each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across {1ð â 3, 2ð â 3, 4ð â 3, 8ð â 3}. The optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal Mamba learning rate was 8e-3; note that Mamba performed better than baselines with matched learning rates (2e-3), but was more stable and improved even more at higher learning rates. (Furthermore, as this LR is on the upper range of the sweep, it is possible that our results are still suboptimal.) Note that, in contrast to standard LM scaling laws (Table 12), our LR held constant across model sizes for simplicity. The optimal LR should go down for larger models, but we didnâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 177, tokens 283, triggered by: token limit\n",
      "\u001b[31mt ï¬ nd a noticeable eï¬ ect at the small model sizes (at most a few million parameters) we considered. E.3.3 Scaling: Context Length Details We use a total batch size of 224 â 16ð tokens per training step, for every sequence length (e.g. at length 220 there are 16 segments per batch and at length 210 there are 16384 segments per batch). This is a large batch size relative to the model size by usual LM standards, but note that a batch size of 223 is the minimum possible on a machine with 8 GPUs and sequence length of 220, and that HyenaDNA used much larger batches of 228. The learning rate used was 0.008 for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same learning rate of 0.002 from the previous section for HyenaDNA, but found that it was unstable at the longest context length. Sequence Length Warmup. Following (Nguyen, Poli, et al. 2023), we use sequence length warmup (SLW) during pretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from 210 = 1024. (Note that because of how data is curated, at the longest sequence lengths more steps and tokens are spent proportionally.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 178, tokens 291, triggered by: token limit\n",
      "\u001b[32mIn particular, each stage up to length 217 processes the same number of tokens, but 4Ã as many tokens are processed at length 218, 8Ã as many at length 219, and 16Ã as many at length 220.) Unlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successively halved as the sequence lengths are doubled in each stage. 33 Table 13: (Great Apes DNA Classification.) Accuracy after fine-tuning on sequences of length 210 = 1024 up to 220 = 1048576 using pretrained models of the same context length. Random guessing is 20%. Params Accuracy (%) at Sequence Length 210 212 214 216 218 220 28.04 31.47 28.43 27.50 41.17 27.66 42.22 40.72 31.10 42.41 7M 30.00 29.01 31.48 43.73 56.60 Remark E.1. We also note that the schedule was not tuned, and we never experimented with turning off sequence length warmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at similar lengths (Section 4.4), and it is possible that it is not necessary for DNA pretraining either.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 179, tokens 106, triggered by: 0.19\n",
      "\u001b[34m# E.3.4 Species (Great Apes) Classification Models are causal and therefore only the last element (across the sequence length) of the modelâ s output is used for the classiï¬ cation head. Note that we control for the total number of elements in the loss function per gradient step. The pretraining objective includes all positions across the sequence length, so that ð ð ð ð ð _ð ð ð £ð Ã ð ð ð ð ð ð ð ð _ð ð ð ð ð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 180, tokens 249, triggered by: 0.14\n",
      "\u001b[35mis held constant; in other words, the batch size decreases as the sequence length increases. However, for a classiï¬ cation task, since only the last position enters the loss, the batch size itself is held constant. Note that this also means that ï¬ ne-tuning models with longer sequence lengths is more computationally expensive. Training consists of 10 epochs, each of which has 1024 gradient steps. Each gradient step uses batch size 64, which are all independently randomly drawn by uniformly picking a species, uniformly picking a chromosome, and then uniformly picking a contiguous segment of DNA. Following (Nguyen, Poli, et al. 2023), models with a maximum context length greater than 214 = 16384 use sequence length warmup with 1 epoch at length 214 = 16384, 1 epoch at length 215 = 32768, 1 epoch at length 216 = 65536, and so on up to the maximum sequence length. For example, the model with 220 = 1048576 context undergoes 6 epochs of sequence length warmup before 4 more epochs at its maximum sequence length. The learning rate for all Hyena models is ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 181, tokens 292, triggered by: token limit\n",
      "\u001b[31mºð â ð », while the learning rate for all Mamba models is ð ·ð â ð º. These were found by performing learning rate sweeps for each model among {1ð â 5, 2ð â 5, 4ð â 5, 1ð â 4, 2ð â 4} for the smaller sequence lengths (210, 212, 214, 216), and these values were consistently found to be the best for each model. An abridged learning rate sweep was done at length 218, which agreed with these values, and a single run at length 220 was performed (as described above, the computational cost of these experiments is proportional to the sequence length). The learning rate followed a cosine decay schedule with warmup with 5 epochs of linear warmup to the maximum learning rate, and 5 epochs of cosine decay down to 1ð â 6. The unusually long learning rate warmup schedule was chosen because the sequence length warmup was also long (e.g. comprising 6 out of 10 epochs for the model with context length 220); we did not experiment with this choice. Results for the Species classiï¬ cation task are in Table 13. # E.4 Audio Details # E.4.1 YouTubeMix Audio Pretraining Model. We use a model with 3 blocks per stage (3 Ã 5 = 15 total Mamba blocks), pooling factor ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 182, tokens 227, triggered by: 0.22\n",
      "\u001b[32m= 16, and outer dimension ð · = 64, for about 3.5M parameters. Dataset. The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of 256. The dataset consists of clips of up to 1 minute long, or length 960000, which is subsampled and divided into segments of any desired sequence length. Since the architecture involves two stages of pooling by a factor of 16, 34 Table 14: YouTubeMix length scaling sequence lengths and batch sizes. 468 Ã 2048 = 958464 234 Ã 2048 = 479232 117 Ã 2048 = 239616 59 Ã 2048 = 120832 30 Ã 2048 = 61440 15 Ã 2048 = 30720 8 Ã 2048 = 16384 4 Ã 2048 = 8192 1 2 4 8 16 32 64 128 958464 958464 958464 966656 983040 983040 1048576 1048576\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 183, tokens 293, triggered by: token limit\n",
      "\u001b[34mAudio Waveforms - SSM Parameterization aso â â samp â â Mamba (s6) = â sy = sSeaive B/C Â° 1.40 4 â â -selective A s ras | __Mamba-$4) B 1204 124 108 108 Sequence Length Audio Waveforms - SSM Parameterization â â Mamba ($6) 4 â â +complex = Solestive a | (Mamba-S4) 1.35 1.304 1.254 108 108 Sequence Length 1.48 21404 . Ã© ag Figure 10: (Audio Pretraining (YouTubeMix) Ablations.) As a uniformly-sampled â continuousâ signal modality, audio wave- forms actually benefit from LTI models which have matching inductive bias. (Left) Homogenous models (all blocks have the same parameterization) (Right) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as figure on left. and we want the resulting sequence length to be a a multiple of 8 for hardware eï¬ ciency, the longest possible sequence is 468 Ã 2048 = 958464. The rest of our sequence lengths are deï¬ ned by successively halving this and rounding up to the nearest multiple of 2048. Table 14 lists the speciï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 184, tokens 152, triggered by: 0.25\n",
      "\u001b[35mcations used in Figure 7. Beyond the varying batch sizes, the number of valid segments in the training set varied between diï¬ erent sequence lengths (e.g. the number of training steps per epoch was not constant for diï¬ erent points in the graph), which may have contributed to kinks in the scaling curves. Training. Models were trained for 200ð ¾ training steps with a maximum learning rate of 0.002, 20ð ¾ (10%) warmup steps, and weight decay 0.1 (similar to our general pretraining recipe across domains). Additional Ablations: SSM Parameterizations. We investigate SSM parameterizations on long-form audio waveform pretraining in the setting of Figure 7.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 185, tokens 112, triggered by: 0.25\n",
      "\u001b[31mThe setting is modiï¬ ed slightly to use larger models (8 layers and ð · = 64 for 6M params, the SaShiMi default), shorter sequences (211 = 2048 to 218 = 262144 instead of 213 to 220), lower LR (0.001 from 0.002), and shorter training cycles (100K instead of 200K steps). Figure 10 shows that the change from S4 â S6 (i.e. the selection mechanism) is not always beneï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 186, tokens 211, triggered by: 0.18\n",
      "\u001b[32mcial. On long-form audio waveforms, it in fact signiï¬ cantly hampers performance, which may be intuitive from the point of view that audio is uniformly sampled and very smooth, and therefore beneï¬ ts from continuous linear time-invariant (LTI) methods. After ablating away the selection mechanism, note that the resulting model is the S4 layer inside the Mamba block. To disambiguate, we call this Mamba-S4 as opposed the default Mamba architecture Mamba-S6. However, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers. The performance diï¬ erences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio signal should be LTI, but once they are â tokenizedâ and compressed by the outer layers, the inner layers no longer need to be LTI. In this setting however, the real-valued SSM still underperforms the complex-valued one.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 187, tokens 160, triggered by: 0.24\n",
      "\u001b[34m35 # E.4.2 SC09 Speech Generation Autoregressive training largely followed the autoregressive language modeling protocol, such as â ¢ Weight decay 0.1 â ¢ Learning rate warmup for 10% of total steps â ¢ AdamW optimizer with ð ½ = (0.9, 0.95) â ¢ Gradient clip value 0.1 We used a learning rate of 0.002 and 200000 training steps at a batch size of 16. The large Mamba model in Table 4 has 15 layers per stage with an outer dimension of ð · = 96 and pooling factor 4. We note that this dataset is small (training went through 100 epochs) and for this large model, there was signiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 188, tokens 148, triggered by: 0.25\n",
      "\u001b[35mcant overï¬ tting of the BPB or NLL. However, automated metrics of generated samples continually improving throughout training. The models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of ð ³ = 64 and pooling factor 4. The S4+MLP block has roughly 2ð ·2 + 4ð ·2 parameters (expansion factor 2 in the MLP). The Transformer block has 4ð ·2 + 2ð ·2 parameters (expansion factor 1 in the MLP). The Mamba block has the usual â 6ð ·2 parameters. All models have roughly 6M total parameters. # E.5 Efficiency Benchmark\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 189, tokens 210, triggered by: 0.14\n",
      "\u001b[31mScan Operation. We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3), against convolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost of other operations outside of this core operation, such as computing the convolutional kernel in global-convolution models, or computing the QKV projections in attention. As a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing the parameters A, B, C in HBM. Our scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all the large parameters in HBM. For convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs and the ï¬ lters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The theoretical complexity is ð (ð ¿ log(ð ¿)) for sequence length ð ¿.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 190, tokens 295, triggered by: token limit\n",
      "\u001b[32mFor attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2023)), with causal mask. Note that FlashAttention-2 with causal mask is about 1.7Ã faster than without causal mask, since approximately only half of the attention entries are computed. We use batch size of 1 and increase the sequence length from 29 = 512, 210 â 1ð ¾, 211 â 2ð ¾, up to 219 â 500ð ¾ (some of the baselines run out of memory before reaching 500K). We use a model dimension of ð · = 1024 and state dimension ð = 16. We measure with BF16 inputs, which is the data type most commonly used for large scale training. End-to-end Inference. We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba 6.9B model, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard Transformer implementation in the Huggingface transformers library. We set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16, 32, 64, to 128, and measure time time taken to generate 128 tokens.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 191, tokens 288, triggered by: 0.18\n",
      "\u001b[34mWe then calculate the throughput (tokens/s) as batch size Ã 128â time taken. We repeat the measurements 3 times and take the average. Measurements are done on an A100 80GB PCIe GPU. Memory Benchmark. The memory usage simply scales proportionally to the size of the activation tensors, as with most deep sequence models. We report measurements of the training memory requirements of 125M models 36 Table 15: (Memory benchmark.) Mambaâ s memory footprint is comparable to the most optimized Transformer. Results for 125M models. Batch size Transformer (w/ FlashAttention-2) Mamba 1 2 4 8 16 32 4.6GB 5.2GB 6.9GB 11.5GB 20.7GB 34.5GB 4.8GB 5.8GB 7.3GB 12.3GB 23.1GB 38.2GB on 1 A100 80GB GPU. Each batch consists of sequences of length 2048. We compare to the most memory-eï¬ cient Transformer implementation we are aware of (with kernel fusion from torch.compile and with FlashAttention-2). Table 15 shows that Mambaâ s memory requirement is comparable to a similar-sized Transformer with an extremely optimized implementation, and we expect further improvement in Mambaâ s memory footprint in the future.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 192, tokens 1, triggered by: final split\n",
      "\u001b[35m37\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunker.print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eba365",
   "metadata": {},
   "source": [
    "CONSECUTIVE CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84ae56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import ConsecutiveChunker\n",
    "chunker = ConsecutiveChunker(encoder=encoder, score_threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1818f1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:29<00:00,  1.83s/it]\n",
      "100%|██████████| 3086/3086 [00:00<00:00, 23013.58it/s]\n"
     ]
    }
   ],
   "source": [
    "chunks = chunker(docs=[content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e98b14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1, tokens None, triggered by: 0.09\n",
      "\u001b[31m# Mamba:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 2, tokens None, triggered by: 0.10\n",
      "\u001b[32mLinear-Time Sequence Modeling with Selective State Spaces\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 3, tokens None, triggered by: 0.25\n",
      "\u001b[34m# Albert Gu*1 and Tri Dao*2 1Machine Learning Department, Carnegie Mellon University 2Department of Computer Science, Princeton University agu@cs.cmu.edu, tri@tridao.me\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 4, tokens None, triggered by: 0.22\n",
      "\u001b[35m# Abstract\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 5, tokens None, triggered by: 0.30\n",
      "\u001b[31mFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformersâ computational ineï¬ ciency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 6, tokens None, triggered by: 0.22\n",
      "\u001b[32mSecond, even though this change prevents the use of eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 7, tokens None, triggered by: 0.28\n",
      "\u001b[34mcient convolutions, we design a hardware-aware parallel algorithm in recurrent mode.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 8, tokens None, triggered by: 0.25\n",
      "\u001b[35mWe integrate these selective SSMs into a simpliï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 9, tokens None, triggered by: 0.11\n",
      "\u001b[31med end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5Ã higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 10, tokens None, triggered by: 0.21\n",
      "\u001b[32m# 1 Introduction\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 11, tokens None, triggered by: 0.14\n",
      "\u001b[34mFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an eï¬ ective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The eï¬ cacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a ï¬ nite window, and quadratic scaling with respect to the window length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 12, tokens None, triggered by: 0.21\n",
      "\u001b[35mAn enormous body of research has appeared on more eï¬ cient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it eï¬ ective.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 13, tokens None, triggered by: 0.27\n",
      "\u001b[31mAs of yet, none of these variants have been shown to be empirically eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 14, tokens None, triggered by: 0.26\n",
      "\u001b[32mective at scale across domains.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 15, tokens None, triggered by: 0.09\n",
      "\u001b[34mRecently, structured state space sequence models (SSMs) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very eï¬ ciently as either a recurrence or convolution, with linear or near-linear scaling in sequence length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 16, tokens None, triggered by: 0.28\n",
      "\u001b[35mAdditionally, they have principled\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 17, tokens None, triggered by: 0.23\n",
      "\u001b[31mEqual contribution.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 18, tokens None, triggered by: 0.07\n",
      "\u001b[32m1\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 19, tokens None, triggered by: 0.15\n",
      "\u001b[34mmechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Range Arena (Tay, Dehghani, Abnar, et al. 2021).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 20, tokens None, triggered by: 0.23\n",
      "\u001b[35mMany ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 21, tokens None, triggered by: 0.21\n",
      "\u001b[31mavors of SSMs (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 22, tokens None, triggered by: 0.20\n",
      "\u001b[32mHowever, they have been less eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 23, tokens None, triggered by: 0.20\n",
      "\u001b[34mective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 24, tokens None, triggered by: 0.18\n",
      "\u001b[35mSelection Mechanism.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 25, tokens None, triggered by: 0.25\n",
      "\u001b[31mFirst, we identify a key limitation of prior models: the ability to eï¬ ciently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 26, tokens None, triggered by: 0.18\n",
      "\u001b[32mThis allows the model to ï¬ lter out irrelevant information and remember relevant information indeï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 27, tokens None, triggered by: 0.16\n",
      "\u001b[34mnitely.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 28, tokens None, triggered by: 0.27\n",
      "\u001b[35mHardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 29, tokens None, triggered by: 0.18\n",
      "\u001b[31mcient.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 30, tokens None, triggered by: 0.29\n",
      "\u001b[32mWe overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 31, tokens None, triggered by: 0.28\n",
      "\u001b[34merent levels of the GPU memory hierarchy.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 32, tokens None, triggered by: 0.19\n",
      "\u001b[35mThe resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3Ã faster on A100 GPUs).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 33, tokens None, triggered by: 0.28\n",
      "\u001b[31mArchitecture.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 34, tokens None, triggered by: 0.29\n",
      "\u001b[32mWe simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and eï¬ ciency together yield performance improvements on real data up to sequence length 1M.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 35, tokens None, triggered by: 0.24\n",
      "\u001b[34mWe empirically validate Mambaâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 36, tokens None, triggered by: 0.24\n",
      "\u001b[35ms potential as a general sequence FM backbone, in both pretraining quality and domain-speciï¬ c task performance, on several types of modalities and settings:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 37, tokens None, triggered by: 0.19\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 38, tokens None, triggered by: 0.26\n",
      "\u001b[32m¢ Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indeï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 39, tokens None, triggered by: 0.20\n",
      "\u001b[34mnitely long (>1M tokens).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 40, tokens None, triggered by: 0.24\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 41, tokens None, triggered by: 0.13\n",
      "\u001b[31m¢ Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform- ers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 42, tokens None, triggered by: 0.24\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 43, tokens None, triggered by: 0.15\n",
      "\u001b[34m¢ Language Modeling.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 44, tokens None, triggered by: 0.10\n",
      "\u001b[35mMamba is the ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 45, tokens None, triggered by: 0.20\n",
      "\u001b[31mrst linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5Ã generation throughput compared to Transformers of similar size, and Mamba-3Bâ s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 46, tokens None, triggered by: 0.08\n",
      "\u001b[32mModel code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 47, tokens None, triggered by: 0.14\n",
      "\u001b[34m2\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 48, tokens None, triggered by: 0.19\n",
      "\u001b[35m# Selective State Space Model # with Hardware-aware State Expansion # A\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 49, tokens None, triggered by: 0.29\n",
      "\u001b[31mvuvy GPU SRAM Selection Mechanism es\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 50, tokens None, triggered by: 0.25\n",
      "\u001b[32mSelection Mechanism\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 51, tokens None, triggered by: 0.25\n",
      "\u001b[34mFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. ð · = 5) of an input ð ¥ to output ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 52, tokens None, triggered by: 0.28\n",
      "\u001b[35m¦ through a higher dimensional latent state â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 53, tokens None, triggered by: 0.23\n",
      "\u001b[31m(e.g. ð = 4).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 54, tokens None, triggered by: 0.29\n",
      "\u001b[32mPrior SSMs avoid materializing this large effective state (ð ·ð , times batch size ð µ and sequence length ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 55, tokens None, triggered by: 0.26\n",
      "\u001b[34m¿) through clever alternate computation paths requiring time-invariance: the (â , A, B, C) parameters are constant across time.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 56, tokens None, triggered by: 0.26\n",
      "\u001b[35mOur selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 57, tokens None, triggered by: 0.24\n",
      "\u001b[31m# 2 State Space Models Structured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a 1-dimensional function or sequence ð ¥(ð ¡) â â â ¦ ð ¦(ð ¡) â â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 58, tokens None, triggered by: 0.28\n",
      "\u001b[32mthrough an implicit latent state â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 59, tokens None, triggered by: 0.23\n",
      "\u001b[34m(ð ¡) â â ð .\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 60, tokens None, triggered by: 0.22\n",
      "\u001b[35mConcretely, S4 models are deï¬ ned with four parameters (â , A, B, C), which deï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 61, tokens None, triggered by: 0.18\n",
      "\u001b[31mne a sequence-to-sequence trans- formation in two stages.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 62, tokens None, triggered by: 0.27\n",
      "\u001b[32mâ â ²(ð ¡) = Aâ (ð ¡) + Bð ¥(ð ¡) ð ¦(ð ¡) = Câ (ð ¡) (1a) (1b) â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 63, tokens None, triggered by: 0.27\n",
      "\u001b[34mð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 64, tokens None, triggered by: 0.27\n",
      "\u001b[35m¡ = Aâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 65, tokens None, triggered by: 0.24\n",
      "\u001b[31mð ¡â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 66, tokens None, triggered by: 0.28\n",
      "\u001b[32m1 + Bð ¥ð ¡ ð ¦ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 67, tokens None, triggered by: 0.30\n",
      "\u001b[34m¡ = Câ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 68, tokens None, triggered by: 0.26\n",
      "\u001b[35mð ¡ (2a) (2b) ð ð ² = (Cð ©, Cð ¨ð ©, â ¦ , Cð ¨ ð ¦ = ð ¥ â ð ² ð ©, â ¦ ) (3a) (3b)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 69, tokens None, triggered by: 0.22\n",
      "\u001b[31mDiscretization.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 70, tokens None, triggered by: 0.30\n",
      "\u001b[32mThe ï¬ rst stage transforms the â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 71, tokens None, triggered by: 0.23\n",
      "\u001b[34mcontinuous parametersâ (â , A, B) to â discrete parametersâ (A, B) through ï¬ xed formulas A = ð ð ´(â , A) and B = ð ð µ(â , A, B), where the pair (ð ð ´, ð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 72, tokens None, triggered by: 0.27\n",
      "\u001b[35mµ) is called a discretization rule. Various rules can be used such as the zero-order hold (ZOH) deï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 73, tokens None, triggered by: 0.28\n",
      "\u001b[31mned in equation (4).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 74, tokens None, triggered by: 0.13\n",
      "\u001b[32mA = exp(â A) B = (â A)â 1(exp(â A) â I) â â B (4)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 75, tokens None, triggered by: 0.26\n",
      "\u001b[34mDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly normalized (Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023). It also has connections to gating mechanisms of RNNs (Gu, Gulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 76, tokens None, triggered by: 0.29\n",
      "\u001b[35mHowever, from a mechanical point of view discretization can simply be viewed as the ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 77, tokens None, triggered by: 0.15\n",
      "\u001b[31mrst step of the computation graph in the forward pass of an SSM.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 78, tokens None, triggered by: 0.22\n",
      "\u001b[32mAlternate ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 79, tokens None, triggered by: 0.29\n",
      "\u001b[34mavors of SSMs can bypass the discretization step and parameterize (A, B) directly instead (Zhang et al. 2023), which may be easier to reason about. Computation.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 80, tokens None, triggered by: 0.25\n",
      "\u001b[35mAfter the parameters have been transformed from (â , A, B, C) â ¦ (A, B, C), the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 81, tokens None, triggered by: 0.24\n",
      "\u001b[31m3\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 82, tokens None, triggered by: 0.14\n",
      "\u001b[32mCommonly, the model uses the convolutional mode (3) for eï¬ cient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for eï¬ cient autoregressive inference (where the inputs are seen one timestep at a time). Linear Time Invariance (LTI). An important property of equations (1) to (3) is that the modelâ s dynamics are constant through time.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 83, tokens None, triggered by: 0.21\n",
      "\u001b[34mIn other words (â , A, B, C), and consequently (A, B) as well, are ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 84, tokens None, triggered by: 0.24\n",
      "\u001b[35mxed for all time-steps.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 85, tokens None, triggered by: 0.21\n",
      "\u001b[31mThis property is called linear time invariance (LTI), which is deeply connected to recurrence and convolutions. Informally, we think of LTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these classes of models. Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental eï¬ ciency constraints, discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint while overcoming the eï¬ ciency bottlenecks.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 86, tokens None, triggered by: 0.23\n",
      "\u001b[32mStructure and Dimensions. Finally, we note that structured SSMs are so named because computing them eï¬ ciently also requires imposing structure on the A matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 87, tokens None, triggered by: 0.28\n",
      "\u001b[34mIn this case, the A â â ð Ã ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 88, tokens None, triggered by: 0.27\n",
      "\u001b[35m, B â â ð Ã 1, C â â 1Ã ð matrices can all be represented by ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 89, tokens None, triggered by: 0.18\n",
      "\u001b[31mnumbers.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 90, tokens None, triggered by: 0.10\n",
      "\u001b[32mTo operate over an input sequence ð ¥ of batch size ð µ and length ð ¿ with ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 91, tokens None, triggered by: 0.28\n",
      "\u001b[34m· channels, the SSM is applied independently to each channel.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 92, tokens None, triggered by: 0.28\n",
      "\u001b[35mNote that in this case, the total hidden state has dimension ð ·ð per input, and computing it over the sequence length requires ð (ð µð ¿ð ·ð ) time and memory; this is the root of the fundamental eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 93, tokens None, triggered by: 0.20\n",
      "\u001b[31mciency bottleneck addressed in Section 3.3.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 94, tokens None, triggered by: 0.24\n",
      "\u001b[32mGeneral State Space Models. We note that the term state space model has a very broad meaning which simply represents the notion of any recurrent process with a latent state.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 95, tokens None, triggered by: 0.23\n",
      "\u001b[34mIt has been used to refer to many disparate concepts in diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 96, tokens None, triggered by: 0.19\n",
      "\u001b[35merent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman ï¬ lters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes convolutional) models at large (deep learning).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 97, tokens None, triggered by: 0.26\n",
      "\u001b[31mThroughout this entire paper we use the term â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 98, tokens None, triggered by: 0.16\n",
      "\u001b[32mSSMâ to refer exclusively to the class of structured SSMs or S4 models (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington, and Linderman 2023) and use these terms interchangeably. For convenience we may also include derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y. Li et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 99, tokens None, triggered by: 0.09\n",
      "\u001b[34mSSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 100, tokens None, triggered by: 0.12\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 101, tokens None, triggered by: 0.12\n",
      "\u001b[31m¢ Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 102, tokens None, triggered by: 0.28\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 103, tokens None, triggered by: 0.13\n",
      "\u001b[34m¢ H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 104, tokens None, triggered by: 0.12\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 105, tokens None, triggered by: 0.12\n",
      "\u001b[31m¢ Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 106, tokens None, triggered by: 0.23\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 107, tokens None, triggered by: 0.17\n",
      "\u001b[34m¢ RetNet (Y.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 108, tokens None, triggered by: 0.07\n",
      "\u001b[35mSun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 109, tokens None, triggered by: 0.24\n",
      "\u001b[31m4 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 110, tokens None, triggered by: 0.19\n",
      "\u001b[32m¢ RWKV (B.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 111, tokens None, triggered by: 0.16\n",
      "\u001b[34mPeng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation (attention-free Transformer (S. Zhai et al. 2021)).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 112, tokens None, triggered by: 0.10\n",
      "\u001b[35mIts main â WKVâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 113, tokens None, triggered by: 0.29\n",
      "\u001b[31mmechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. # 3 Selective State Space Models We motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 114, tokens None, triggered by: 0.22\n",
      "\u001b[32mThe resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 115, tokens None, triggered by: 0.28\n",
      "\u001b[34mciently.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 116, tokens None, triggered by: 0.21\n",
      "\u001b[35mWe overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). # 3.1 Motivation:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 117, tokens None, triggered by: 0.10\n",
      "\u001b[31mSelection as a Means of Compression We argue that a fundamental problem of sequence modeling is compressing context into a smaller state.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 118, tokens None, triggered by: 0.20\n",
      "\u001b[32mIn fact, we can view the tradeoï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 119, tokens None, triggered by: 0.21\n",
      "\u001b[34ms of popular sequence models from this point of view.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 120, tokens None, triggered by: 0.30\n",
      "\u001b[35mFor example, attention is both eï¬ ective and ineï¬ cient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 121, tokens None, triggered by: 0.24\n",
      "\u001b[31mcient because they have a ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 122, tokens None, triggered by: 0.16\n",
      "\u001b[32mnite state, implying constant-time inference and linear-time training.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 123, tokens None, triggered by: 0.27\n",
      "\u001b[34mHowever, their eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 124, tokens None, triggered by: 0.26\n",
      "\u001b[35mectiveness is limited by how well this state has compressed the context.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 125, tokens None, triggered by: 0.12\n",
      "\u001b[31mTo understand this principle, we focus on two running examples of synthetic tasks (Figure 2).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 126, tokens None, triggered by: 0.25\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 127, tokens None, triggered by: 0.20\n",
      "\u001b[34m¢ The Selective Copying task modiï¬ es the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and ï¬ lter out the irrelevant ones (white).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 128, tokens None, triggered by: 0.12\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 129, tokens None, triggered by: 0.21\n",
      "\u001b[31m¢ The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 130, tokens None, triggered by: 0.26\n",
      "\u001b[32mThese tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the (A, B) transitions in (2)) cannot let them select the correct information from their context, or aï¬ ect the hidden state passed along the sequence an in input-dependent way.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 131, tokens None, triggered by: 0.20\n",
      "\u001b[34mFrom the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have diï¬ culty with the Selective Copying task because of lack of content-awareness (Figure 2).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 132, tokens None, triggered by: 0.13\n",
      "\u001b[35mMore concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 133, tokens None, triggered by: 0.20\n",
      "\u001b[31mIn summary, the eï¬ ciency vs. eï¬ ectiveness tradeoï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 134, tokens None, triggered by: 0.05\n",
      "\u001b[32mof sequence models is characterized by how well they compress their state: eï¬ cient models must have a small state, while eï¬ ective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or ï¬ lter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). # Improving SSMs with Selection One method of incorporating a selection mechanism into models is by letting their parameters that aï¬ ect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN) be input-dependent.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 135, tokens None, triggered by: 0.17\n",
      "\u001b[34m5\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 136, tokens None, triggered by: 0.18\n",
      "\u001b[35mCopying Output noo am > mt HE nee Tt Solution\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 137, tokens None, triggered by: 0.25\n",
      "\u001b[31m# Tetons\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 138, tokens None, triggered by: 0.28\n",
      "\u001b[32m| # oO S lective Copying\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 139, tokens None, triggered by: 0.29\n",
      "\u001b[34m# aoe # i) # [coe # Induction Heads\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 140, tokens None, triggered by: 0.10\n",
      "\u001b[35m# EES > # fo\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 141, tokens None, triggered by: 0.15\n",
      "\u001b[31mPerfectly solved by LTI (e.g. convolutional) models that do not need to look at the actual inputs\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 142, tokens None, triggered by: 0.17\n",
      "\u001b[32mHi i Hl ] Bw H a H > BH\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 143, tokens None, triggered by: 0.23\n",
      "\u001b[34mFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. Algorithm 2 SSM + Selection (S6) Input: ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 144, tokens None, triggered by: 0.28\n",
      "\u001b[35m¥ â ¶ (ð ±, ð », ð ³) Output: ð ¦ â ¶ (ð ±, ð », ð ³) 1: A â ¶ (ð ³, ð ½) â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 145, tokens None, triggered by: 0.25\n",
      "\u001b[31mð ¯ð ºð ð ºð ð ¾ð ð ¾ð â ³ Represents structured ð Ã ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 146, tokens None, triggered by: 0.24\n",
      "\u001b[32mmatrix â ³ Represents structured ð Ã ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 147, tokens None, triggered by: 0.28\n",
      "\u001b[34mmatrix 2: B â ¶ (ð ³, ð ½) â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 148, tokens None, triggered by: 0.28\n",
      "\u001b[35mð ¯ð ºð ð ºð ð ¾ð ð ¾ð 3: C â ¶ (ð ³, ð ½) â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 149, tokens None, triggered by: 0.28\n",
      "\u001b[31mð ¯ð ºð ð ºð ð ¾ð ð ¾ð 4: â â ¶ (ð ³) â ð â (ð ¯ð ºð ð ºð ð ¾ð ð ¾ð ) 5: A, B â ¶ (ð ³, ð ½) â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 150, tokens None, triggered by: 0.28\n",
      "\u001b[32mð ½ð ð ð ¼ð ð ¾ð ð ð ð ¾(â , A, B) 6: ð ¦ â ð ²ð ²ð ¬(A, B, C)(ð ¥) 2: B â ¶ (ð ±, ð », ð ½) â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 151, tokens None, triggered by: 0.26\n",
      "\u001b[34mð ð µ(ð ¥) 3:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 152, tokens None, triggered by: 0.28\n",
      "\u001b[35mC â ¶ (ð ±, ð », ð ½) â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 153, tokens None, triggered by: 0.28\n",
      "\u001b[31mð ð ¶(ð ¥) 4: â â ¶ (ð ±, ð », ð ³) â ð â (ð ¯ð ºð ð ºð ð ¾ð ð ¾ð +ð â (ð ¥)) 5: A, B â ¶ (ð ±, ð », ð ³, ð ½) â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 154, tokens None, triggered by: 0.15\n",
      "\u001b[32mð ½ð ð ð ¼ð ð ¾ð ð ð ð ¾(â , A, B) 6: ð ¦ â ð ²ð ²ð ¬(A, B, C)(ð ¥) â ³ Time-invariant: recurrence or convolution â ³ Time-varying: recurrence (scan) only 7: return ð ¦ 7: return ð ¦\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 155, tokens None, triggered by: 0.20\n",
      "\u001b[34mAlgorithms 1 and 2 illustrates the main selection mechanism that we use.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 156, tokens None, triggered by: 0.24\n",
      "\u001b[35mThe main diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 157, tokens None, triggered by: 0.19\n",
      "\u001b[31merence is simply making several parameters â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 158, tokens None, triggered by: 0.22\n",
      "\u001b[32m, B, C functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension ð ¿, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2). This loses the equivalence to convolutions (3) with implications for its eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 159, tokens None, triggered by: 0.25\n",
      "\u001b[34mciency, discussed next.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 160, tokens None, triggered by: 0.27\n",
      "\u001b[35mWe speciï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 161, tokens None, triggered by: 0.15\n",
      "\u001b[31mcally choose ð ð µ(ð ¥) = ð «ð ð ð ¾ð ºð ð (ð ¥), ð ð ¶(ð ¥) = ð «ð ð ð ¾ð ºð ð (ð ¥), ð â (ð ¥) = ð ¡ð ð ð ºð ½ð ¼ð ºð ð ð ·(ð «ð ð ð ¾ð ºð 1(ð ¥)), and ð â = ð ð ð ¿ð ð ð ð ð , where ð «ð ð ð ¾ð ºð ð is a parameterized projection to dimension ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 162, tokens None, triggered by: 0.18\n",
      "\u001b[32m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 163, tokens None, triggered by: 0.12\n",
      "\u001b[34mThe choice of ð â and ð â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 164, tokens None, triggered by: 0.25\n",
      "\u001b[35mis due to a connection to RNN gating mechanisms explained in Section 3.5.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 165, tokens None, triggered by: 0.25\n",
      "\u001b[31m# 3.3 Efficient Implementation of Selective SSMs\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 166, tokens None, triggered by: 0.16\n",
      "\u001b[32mHardware-friendly architectures such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and Transform- ers (Vaswani et al. 2017) enjoy widespread application.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 167, tokens None, triggered by: 0.16\n",
      "\u001b[34mHere we aim to make selective SSMs eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 168, tokens None, triggered by: 0.16\n",
      "\u001b[35mcient on modern hardware (GPU) as well.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 169, tokens None, triggered by: 0.28\n",
      "\u001b[31mThe selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 170, tokens None, triggered by: 0.30\n",
      "\u001b[32mvary over time in recurrent SSMs (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational eï¬ ciency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 171, tokens None, triggered by: 0.08\n",
      "\u001b[34m# 3.3.1 Motivation of Prior Models\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 172, tokens None, triggered by: 0.10\n",
      "\u001b[35mWe ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 173, tokens None, triggered by: 0.10\n",
      "\u001b[31mrst revisit this motivation and overview our approach to overcome limitations of prior methods.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 174, tokens None, triggered by: 0.21\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 175, tokens None, triggered by: 0.22\n",
      "\u001b[34m¢ At a high level, recurrent models such as SSMs always balance a tradeoï¬ between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more eï¬ ective but slower.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 176, tokens None, triggered by: 0.21\n",
      "\u001b[35mThus\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 177, tokens None, triggered by: 0.13\n",
      "\u001b[31m6\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 178, tokens None, triggered by: 0.11\n",
      "\u001b[32mwe want to maximize hidden state dimension without paying speed and memory costs.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 179, tokens None, triggered by: 0.29\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 180, tokens None, triggered by: 0.28\n",
      "\u001b[35m¢ Note that the recurrent mode is more ï¬ exible than the convolution mode, since the latter (3) is derived from expanding the former (2) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 181, tokens None, triggered by: 0.20\n",
      "\u001b[31mHowever, this would require computing and materializing the latent state â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 182, tokens None, triggered by: 0.30\n",
      "\u001b[32mwith shape (ð ±, ð », ð ³, ð ½), much larger (by a factor of ð , the SSM state dimension) than the input ð ¥ and output ð ¦ of shape (ð ±, ð », ð ³).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 183, tokens None, triggered by: 0.29\n",
      "\u001b[34mThus the more eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 184, tokens None, triggered by: 0.18\n",
      "\u001b[35mcient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of only (ð ±, ð », ð ³). â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 185, tokens None, triggered by: 0.26\n",
      "\u001b[31m¢ Prior LTI SSMs leverage the dual recurrent-convolutional forms to increase the eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 186, tokens None, triggered by: 0.23\n",
      "\u001b[32mective state dimension by a factor of ð (â 10 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 187, tokens None, triggered by: 0.18\n",
      "\u001b[34m100), much larger than traditional RNNs, without eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 188, tokens None, triggered by: 0.15\n",
      "\u001b[35mciency penalties.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 189, tokens None, triggered by: 0.21\n",
      "\u001b[31m# 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 190, tokens None, triggered by: 0.29\n",
      "\u001b[32mThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 191, tokens None, triggered by: 0.27\n",
      "\u001b[34mWe address this with three classical techniques: kernel fusion, parallel scan, and recomputation.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 192, tokens None, triggered by: 0.16\n",
      "\u001b[35mWe make two main observations:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 193, tokens None, triggered by: 0.25\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 194, tokens None, triggered by: 0.28\n",
      "\u001b[32m¢ The naive recurrent computation uses ð (ð µð ¿ð ·ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 195, tokens None, triggered by: 0.27\n",
      "\u001b[34m) FLOPs while the convolutional computation uses ð (ð µð ¿ð · log(ð ¿)) FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 196, tokens None, triggered by: 0.11\n",
      "\u001b[35m, the recurrent mode can actually use fewer FLOPs.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 197, tokens None, triggered by: 0.17\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 198, tokens None, triggered by: 0.19\n",
      "\u001b[32m¢ The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 199, tokens None, triggered by: 0.18\n",
      "\u001b[34m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 200, tokens None, triggered by: 0.17\n",
      "\u001b[35mThe main idea is to leverage properties of modern accelerators (GPUs) to materialize the state â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 201, tokens None, triggered by: 0.21\n",
      "\u001b[31monly in more eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 202, tokens None, triggered by: 0.28\n",
      "\u001b[32mcient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a signiï¬ cant speedup compared to a standard implementation.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 203, tokens None, triggered by: 0.17\n",
      "\u001b[34mConcretely, instead of preparing the scan input (A, B) of size (ð ±, ð », ð ³, ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 204, tokens None, triggered by: 0.30\n",
      "\u001b[35m½) in GPU HBM (high-bandwidth memory), we load the SSM parameters (â , A, B, C) directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the ï¬ nal outputs of size (ð ±, ð », ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 205, tokens None, triggered by: 0.20\n",
      "\u001b[31m³) back to HBM.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 206, tokens None, triggered by: 0.22\n",
      "\u001b[32mTo avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-eï¬ cient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 207, tokens None, triggered by: 0.28\n",
      "\u001b[34mFinally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. # 3.4 A Simplified SSM Architecture As with structured SSMs, selective SSMs are standalone sequence transformations that can be ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 208, tokens None, triggered by: 0.29\n",
      "\u001b[35mexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 209, tokens None, triggered by: 0.24\n",
      "\u001b[31mThis is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 210, tokens None, triggered by: 0.29\n",
      "\u001b[32mThis architecture involves expanding the model dimension ð · by a controllable expansion factor ð ¸. For each block, most of the parameters (3ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 211, tokens None, triggered by: 0.14\n",
      "\u001b[34m¸ð ·2) are in the linear projections (2ð ¸ð ·2 for input projections, ð ¸ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 212, tokens None, triggered by: 0.22\n",
      "\u001b[35m·2 for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 213, tokens None, triggered by: 0.23\n",
      "\u001b[31m, B, C, and\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 214, tokens None, triggered by: 0.14\n",
      "\u001b[32m7\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 215, tokens None, triggered by: 0.14\n",
      "\u001b[34mLinear projection Sequence transformation Nonlinearity (activation multiplication) H3 Â®@ Gated MLP â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 216, tokens None, triggered by: 0.21\n",
      "\u001b[35mMamba\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 217, tokens None, triggered by: 0.16\n",
      "\u001b[31m# or\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 218, tokens None, triggered by: 0.01\n",
      "\u001b[32mFigure 3: (Architecture.) Our simplified block design combines the H3 block, which is the basis of most SSM architectures, with the ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba block homogenously. Compared to the H3 block, Mamba replaces the first multiplicative gate with an activation function. Compared to the MLP block, Mamba adds an SSM to the main branch.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 219, tokens None, triggered by: 0.10\n",
      "\u001b[34mFor ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 220, tokens None, triggered by: 0.11\n",
      "\u001b[35mwe use the SiLU / Swish activation (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 221, tokens None, triggered by: 0.21\n",
      "\u001b[31mthe matrix A) are much smaller in comparison.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 222, tokens None, triggered by: 0.14\n",
      "\u001b[32mWe repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 223, tokens None, triggered by: 0.27\n",
      "\u001b[34mWe always ï¬ x to ð ¸ = 2 in our experiments and use two stacks of the block to match the 12ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 224, tokens None, triggered by: 0.24\n",
      "\u001b[35m·2 parameters of a Transformerâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 225, tokens None, triggered by: 0.12\n",
      "\u001b[31ms interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular â SwiGLUâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 226, tokens None, triggered by: 0.27\n",
      "\u001b[32mvariant (Chowdhery et al. 2023; Shazeer 2020; Touvron et al. 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 227, tokens None, triggered by: 0.29\n",
      "\u001b[34mFinally, we additionally use an optional normalization layer (we choose LayerNorm (J.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 228, tokens None, triggered by: 0.19\n",
      "\u001b[35mL.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 229, tokens None, triggered by: 0.30\n",
      "\u001b[31mBa, Kiros, and Hinton 2016)), motivated by RetNetâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 230, tokens None, triggered by: 0.19\n",
      "\u001b[32ms usage of a normalization layer in a similar location (Y.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 231, tokens None, triggered by: 0.18\n",
      "\u001b[34mSun et al. 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 232, tokens None, triggered by: 0.27\n",
      "\u001b[35m# 3.5 Properties of Selection Mechanisms The selection mechanism is a broader concept that can be applied in diï¬ erent ways, such as to more traditional RNNs or CNNs, to diï¬ erent parameters (e.g.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 233, tokens None, triggered by: 0.14\n",
      "\u001b[31mA in Algorithm 2), or using diï¬ erent transformations ð (ð ¥).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 234, tokens None, triggered by: 0.28\n",
      "\u001b[32m# 3.5.1 Connection to Gating Mechanisms We highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selection mechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time systems is well established (Funahashi and Nakamura 1993; Tallec and Ollivier 2018). In fact, Theorem 1 is an improvement of Gu, Johnson, Goel, et al. (2021, Lemma 3.1) generalizing to the ZOH discretization and input-dependent gates (proof in Appendix C).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 235, tokens None, triggered by: 0.23\n",
      "\u001b[34mMore broadly, â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 236, tokens None, triggered by: 0.28\n",
      "\u001b[35min SSMs can be seen to play a generalized role of the RNN gating mechanism. In line with prior work, we adopt the view that discretization of SSMs is the principled foundation of heuristic gating mechanisms.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 237, tokens None, triggered by: 0.25\n",
      "\u001b[31mTheorem 1.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 238, tokens None, triggered by: 0.14\n",
      "\u001b[32mWhen ð = 1, A = â 1, B = 1, ð â = ð «ð ð ð ¾ð ºð (ð ¥), and ð â = ð ð ð ¿ð ð ð ð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 239, tokens None, triggered by: 0.14\n",
      "\u001b[34m, then the selective SSM recurrence (Algorithm 2) takes the form\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 240, tokens None, triggered by: 0.24\n",
      "\u001b[35mð ð ¡ = ð (ð «ð ð ð ¾ð ºð (ð ¥ð ¡)) â ð ¡ = (1 â ð ð ¡)â ð ¡â 1 + ð ð ¡ð ¥ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 241, tokens None, triggered by: 0.24\n",
      "\u001b[31m¡. (5)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 242, tokens None, triggered by: 0.22\n",
      "\u001b[32mAs mentioned in Section 3.2, our speciï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 243, tokens None, triggered by: 0.19\n",
      "\u001b[34mc choices of ð â , ð â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 244, tokens None, triggered by: 0.20\n",
      "\u001b[35mis from this connection.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 245, tokens None, triggered by: 0.18\n",
      "\u001b[31mIn particular, note that if a given input ð ¥ð ¡ should be completely ignored (as necessary in the synthetic tasks), all ð · channels should ignore it, and so we project the input down to 1 dimension before repeating/broadcasting with â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 246, tokens None, triggered by: 0.11\n",
      "\u001b[32m. 8\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 247, tokens None, triggered by: 0.18\n",
      "\u001b[34m# Interpretation of Selection Mechanisms We elaborate on two particular mechanistic eï¬ ects of selection.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 248, tokens None, triggered by: 0.27\n",
      "\u001b[35mVariable Spacing.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 249, tokens None, triggered by: 0.16\n",
      "\u001b[31mSelectivity allows ï¬ ltering out irrelevant noise tokens that may occur between inputs of interest.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 250, tokens None, triggered by: 0.26\n",
      "\u001b[32mThis is exempliï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 251, tokens None, triggered by: 0.26\n",
      "\u001b[34med by the Selective Copying task, but occurs ubiquitously in common data modalities, particularly for discrete data â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 252, tokens None, triggered by: 0.27\n",
      "\u001b[35mfor example the presence of language ï¬ llers such as â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 253, tokens None, triggered by: 0.25\n",
      "\u001b[31mumâ .\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 254, tokens None, triggered by: 0.25\n",
      "\u001b[32mThis property arises because the model can mechanistically ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 255, tokens None, triggered by: 0.28\n",
      "\u001b[34mlter out any particular input ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 256, tokens None, triggered by: 0.30\n",
      "\u001b[35m¥ð ¡, for example in the gated RNN case (Theorem 1) when ð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 257, tokens None, triggered by: 0.24\n",
      "\u001b[31m¡ â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 258, tokens None, triggered by: 0.17\n",
      "\u001b[32m0.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 259, tokens None, triggered by: 0.19\n",
      "\u001b[34mIt has been empirically observed that many sequence models do not improve with longer Filtering Context. context (F. Shi et al. 2023), despite the principle that more context should lead to strictly better performance. An explanation is that many sequence models cannot eï¬ ectively ignore irrelevant context when necessary; an intuitive example are global convolutions (and general LTI models). On the other hand, selective models can simply reset their state at any time to remove extraneous history, and thus their performance in principle improves monotonicly with context length (e.g.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 260, tokens None, triggered by: 0.23\n",
      "\u001b[35mSection 4.3.2).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 261, tokens None, triggered by: 0.14\n",
      "\u001b[31mIn settings where multiple independent sequences are stitched together, Transformers Boundary Resetting. can keep them separate by instantiating a particular attention mask, while LTI models will bleed information between the sequences. Selective SSMs can also reset their state at boundaries (e.g. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 262, tokens None, triggered by: 0.30\n",
      "\u001b[32mð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 263, tokens None, triggered by: 0.30\n",
      "\u001b[34m¡ â â or Theorem 1 when ð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 264, tokens None, triggered by: 0.25\n",
      "\u001b[35m¡ â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 265, tokens None, triggered by: 0.25\n",
      "\u001b[31m1).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 266, tokens None, triggered by: 0.27\n",
      "\u001b[32mThese settings may occur artiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 267, tokens None, triggered by: 0.22\n",
      "\u001b[34mcially (e.g. packing documents together to improve hardware utilization) or naturally (e.g. episode boundaries in reinforcement learning (Lu et al. 2023)). Additionally, we elaborate on eï¬ ects of each selective parameter.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 268, tokens None, triggered by: 0.19\n",
      "\u001b[35mIn general, â controls the balance between how much to focus or ignore the current input Interpretation of â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 269, tokens None, triggered by: 0.27\n",
      "\u001b[31m. ð ¥ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 270, tokens None, triggered by: 0.17\n",
      "\u001b[32m¡.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 271, tokens None, triggered by: 0.24\n",
      "\u001b[34mIt generalizes RNN gates (e.g. ð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 272, tokens None, triggered by: 0.26\n",
      "\u001b[35m¡ in Theorem 1), mechanically, a large â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 273, tokens None, triggered by: 0.18\n",
      "\u001b[31mresets the state â and focuses on the current input ð ¥, while a small â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 274, tokens None, triggered by: 0.26\n",
      "\u001b[32mpersists the state and ignores the current input.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 275, tokens None, triggered by: 0.28\n",
      "\u001b[34mSSMs (1)-(2) can be interpreted as a continuous system discretized by a timestep â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 276, tokens None, triggered by: 0.24\n",
      "\u001b[35m, and in this context the intuition is that large â â â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 277, tokens None, triggered by: 0.29\n",
      "\u001b[31mrepresents the system focusing on the current input for longer (thus â selectingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 278, tokens None, triggered by: 0.27\n",
      "\u001b[32mit and forgetting its current state) while a small â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 279, tokens None, triggered by: 0.17\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 280, tokens None, triggered by: 0.25\n",
      "\u001b[35m0 represents a transient input that is ignored.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 281, tokens None, triggered by: 0.28\n",
      "\u001b[31mInterpretation of A. We remark that while the A parameter could also be selective, it ultimately aï¬ ects the model only through its interaction with â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 282, tokens None, triggered by: 0.28\n",
      "\u001b[32mvia A = exp(â A) (the discretization (4)).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 283, tokens None, triggered by: 0.20\n",
      "\u001b[34mThus selectivity in â is enough to ensure selectivity in (A, B), and is the main source of improvement. We hypothesize that making A selective in addition to (or instead of) â would have similar performance, and leave it out for simplicity.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 284, tokens None, triggered by: 0.23\n",
      "\u001b[35mInterpretation of B and C.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 285, tokens None, triggered by: 0.27\n",
      "\u001b[31mAs discussed in Section 3.1, the most important property of selectivity is ï¬ ltering out irrelevant information so that a sequence modelâ s context can be compressed into an eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 286, tokens None, triggered by: 0.21\n",
      "\u001b[32mcient state. In an SSM, modifying B and C to be selective allows ï¬ ner-grained control over whether to let an input ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 287, tokens None, triggered by: 0.21\n",
      "\u001b[34m¥ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 288, tokens None, triggered by: 0.23\n",
      "\u001b[35m¡ into the state â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 289, tokens None, triggered by: 0.12\n",
      "\u001b[31mð ¡ or the state into the output ð ¦ð ¡.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 290, tokens None, triggered by: 0.29\n",
      "\u001b[32mThese can be interpreted as allowing the model to modulate the recurrent dynamics based on content (input) and context (hidden states) respectively. 3.6 Additional Model Details Real vs.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 291, tokens None, triggered by: 0.24\n",
      "\u001b[34mComplex. Most prior SSMs use complex numbers in their state â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 292, tokens None, triggered by: 0.30\n",
      "\u001b[35m, which is necessary for strong performance on many tasks (Gu, Goel, and RÃ© 2022).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 293, tokens None, triggered by: 0.16\n",
      "\u001b[31mHowever, it has been empirically observed that completely real-valued SSMs seem to work ï¬ ne, and possibly even better, in some settings (Ma et al. 2023). We use real values as the default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeoï¬ is related to the continuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous modalities (e.g. audio, video) but not discrete (e.g. text, DNA).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 294, tokens None, triggered by: 0.20\n",
      "\u001b[32m9\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 295, tokens None, triggered by: 0.16\n",
      "\u001b[34mInitialization. Most prior SSMs also suggest special initializations, particularly in the complex-valued case, which can help in several settings such as low-data regimes. Our default initialization for the complex case is S4D-Lin and for the real case is S4D-Real (Gu, Gupta, et al. 2022), which is based on the HIPPO theory (Gu, Dao, et al. 2020).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 296, tokens None, triggered by: 0.26\n",
      "\u001b[35mThese deï¬ ne the ð -th element of A as â 1â 2 + ð ð and â (ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 297, tokens None, triggered by: 0.24\n",
      "\u001b[31m+ 1) respectively.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 298, tokens None, triggered by: 0.26\n",
      "\u001b[32mHowever, we expect many initializations to work ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 299, tokens None, triggered by: 0.25\n",
      "\u001b[34mne, particularly in the large-data and real-valued SSM regimes; some ablations are considered in Section 4.6. Parameterization of â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 300, tokens None, triggered by: 0.23\n",
      "\u001b[35m. We deï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 301, tokens None, triggered by: 0.29\n",
      "\u001b[31mned the selective adjustment to â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 302, tokens None, triggered by: 0.24\n",
      "\u001b[32mas ð â (ð ¥) = ð ¡ð ð ð ºð ½ð ¼ð ºð ð ð ·(ð «ð ð ð ¾ð ºð 1(ð ¥)), which was motivated by the mechanics of â (Section 3.5).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 303, tokens None, triggered by: 0.11\n",
      "\u001b[34mWe observe that it can be generalized from dimension 1 to a larger dimension ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 304, tokens None, triggered by: 0.18\n",
      "\u001b[35m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 305, tokens None, triggered by: 0.24\n",
      "\u001b[31mWe set this to be a small fraction of ð ³, which uses a negligible number of parameters compared to the main Linear projections in the block. We additionally note that the broadcasting operation can instead be viewed as another Linear projection, initialized to a speciï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 306, tokens None, triggered by: 0.25\n",
      "\u001b[32mc pattern of 1â s and 0â s; if this projection is trainable, this leads to the alternative ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 307, tokens None, triggered by: 0.23\n",
      "\u001b[34mâ (ð ¥) = ð «ð ð ð ¾ð ºð ð ·(ð «ð ð ð ¾ð ºð ð (ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 308, tokens None, triggered by: 0.24\n",
      "\u001b[35m¥)), which can be viewed as a low-rank projection.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 309, tokens None, triggered by: 0.21\n",
      "\u001b[31mIn our experiments, the â parameter (which can be viewed as a bias term) is initialized to ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 310, tokens None, triggered by: 0.13\n",
      "\u001b[32mâ 1 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 311, tokens None, triggered by: 0.30\n",
      "\u001b[34mfollowing prior work on SSMs (Gu, Johnson, Timalsina, et al. 2023). Remark 3.1. For brevity in our experimental results, we sometimes abbreviate selective SSMs as S6 models, because they are S4 models with a selection mechanism and computed with a scan. # 4 Empirical Evaluation In Section 4.1 we test Mambaâ s ability to solve the two synthetic tasks motivated in Section 3.1. We then evaluate on three domains, each evaluated on autoregressive pretraining as well as downstream tasks. Section 4.2: language model pretraining (scaling laws), and zero-shot downstream evaluation. Section 4.3: DNA sequence pretraining, and ï¬ ne-tuning on a long-sequence classiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 312, tokens None, triggered by: 0.19\n",
      "\u001b[35mcation task.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 313, tokens None, triggered by: 0.19\n",
      "\u001b[31mSection 4.4: audio waveform pretraining, and the quality of autoregressively generated speech clips.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 314, tokens None, triggered by: 0.20\n",
      "\u001b[32mFinally, Section 4.5 shows Mambaâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 315, tokens None, triggered by: 0.29\n",
      "\u001b[34ms computational eï¬ ciency at both training and inference time, and Section 4.6 ablates various components of the architecture and selective SSMs.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 316, tokens None, triggered by: 0.20\n",
      "\u001b[35m# 4.1 Synthetic Tasks Full experiment details for these tasks including task details and training protocol are in Appendix E.1.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 317, tokens None, triggered by: 0.30\n",
      "\u001b[31m# 4.1.1 Selective Copying The Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test the memorization abilities of recurrent models.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 318, tokens None, triggered by: 0.17\n",
      "\u001b[32mAs discussed in Section 3.1, LTI SSMs (linear recurrences and global convolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for example, by constructing a convolution kernel of exactly the right length (Figure 2). This was explicitly validated in earlier work on global convolutions (Romero et al. 2021).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 319, tokens None, triggered by: 0.20\n",
      "\u001b[34mThe Selective Copying task prevents this shortcut by randomizing the spacing between tokens. Note that this task has been introduced before as the Denoising task (Jing et al. 2019). Note that many previous works argue that adding architecture gating (multiplicative interactions) can endow models with â data-dependenceâ and solve related tasks (Dao, Fu, Saab, et al. 2023; Poli et al. 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 320, tokens None, triggered by: 0.19\n",
      "\u001b[35mHowever, we ï¬ nd this explanation insuï¬ cient intuitively because such gating does not interact along the sequence axis, and cannot aï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 321, tokens None, triggered by: 0.15\n",
      "\u001b[31mect the spacing between tokens.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 322, tokens None, triggered by: 0.18\n",
      "\u001b[32mIn particular architecture gating is not an instance of a selection mechanism (Appendix A).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 323, tokens None, triggered by: 0.09\n",
      "\u001b[34mTable 1 conï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 324, tokens None, triggered by: 0.10\n",
      "\u001b[35mrms that gated architectures such as H3 and Mamba only partially improve performance, while the selection mechanism (modifying S4 to S6) easily solves this task, particularly when combined with these more powerful architectures.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 325, tokens None, triggered by: 0.19\n",
      "\u001b[31m10\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 326, tokens None, triggered by: 0.19\n",
      "\u001b[32mModel Arch. Layer Acc.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 327, tokens None, triggered by: 0.24\n",
      "\u001b[34mS4 - No gate No gate S4 S6 18.3 97.0 H3 Hyena - H3 H3 H3 S4 Hyena S6 57.0 30.1 99.7 - - Mamba Mamba Mamba Mamba Hyena S4 S6 56.4 28.4 99.8\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 328, tokens None, triggered by: 0.28\n",
      "\u001b[35mInduction Heads Extrapolation Extrapolation 1.05 ' â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 329, tokens None, triggered by: 0.23\n",
      "\u001b[31mâ Mua-Absotute 08] ; â â MHA-RoPE i =~ MHA-xPos 6) i â HB oa = byena ' Random 1 ran benath 0.0 , ; ; : , 10Â° 10Â° 108 10Â° 10Â° Test Sequence Length\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 330, tokens None, triggered by: 0.14\n",
      "\u001b[32m> g 8\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 331, tokens None, triggered by: 0.12\n",
      "\u001b[34mTable 1: (Selective Copying.) Accuracy for combinations of architectures and inner sequence layers. Table 2: (Induction Heads.) Models are trained on sequence length 28 = 256, and tested on increasing sequence lengths of 26 = 64 up to 220 = 1048576. Full numbers in Table 11. # 4.1.2 Induction Heads Induction heads (Olsson et al. 2022) is a simple task from the mechanistic interpretability lens (Elhage et al. 2021) that is surprisingly predictive of the in-context learning ability of LLMs. It requires models to perform associative recall and copy: for example, if the model has seen a bigram such as â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 332, tokens None, triggered by: 0.23\n",
      "\u001b[35mHarry Potterâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 333, tokens None, triggered by: 0.27\n",
      "\u001b[31min the sequence, then the next time â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 334, tokens None, triggered by: 0.14\n",
      "\u001b[32mHarryâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 335, tokens None, triggered by: 0.12\n",
      "\u001b[34mappears in the same sequence, the model should be able to predict â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 336, tokens None, triggered by: 0.04\n",
      "\u001b[35mPotterâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 337, tokens None, triggered by: 0.20\n",
      "\u001b[31mby copying from history.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 338, tokens None, triggered by: 0.29\n",
      "\u001b[32mDataset.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 339, tokens None, triggered by: 0.16\n",
      "\u001b[34mWe train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of 16, which is comparable to prior work on this task (Dao, Fu, Saab, et al. 2023) but with longer sequences. We additionally investigate generalization and extrapolation abilities by evaluating on a range of sequence lengths from 26 = 64 up to 220 = 1048576 at test time.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 340, tokens None, triggered by: 0.21\n",
      "\u001b[35mModels. Following established work on induction heads, we use 2 layer models, which allows attention to mechanistically solve the induction heads task (Olsson et al. 2022). We test both multi-head attention (8 heads, with various positional encodings) and SSM variants.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 341, tokens None, triggered by: 0.25\n",
      "\u001b[31mWe use a model dimension ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 342, tokens None, triggered by: 0.16\n",
      "\u001b[32m· of 64 for Mamba and 128 for the other models.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 343, tokens None, triggered by: 0.29\n",
      "\u001b[34mResults.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 344, tokens None, triggered by: 0.25\n",
      "\u001b[35mTable 2 shows that Mambaâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 345, tokens None, triggered by: 0.24\n",
      "\u001b[31mor more precisely, its selective SSM layerâ has the ability to solve the task perfectly because of its ability to selectively remember the relevant token while ignoring everything else in between.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 346, tokens None, triggered by: 0.18\n",
      "\u001b[32mIt generalizes perfectly to million-length sequences, or 4000Ã longer than it saw during training, while no other method goes beyond 2Ã\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 347, tokens None, triggered by: 0.09\n",
      "\u001b[34m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 348, tokens None, triggered by: 0.22\n",
      "\u001b[35mOut of positional encoding variants for attention models, xPos (which was designed for length extrapolation) is slightly better than the others; also note that all attention models were only tested up to sequence length 214 = 16384 due to memory limitations.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 349, tokens None, triggered by: 0.25\n",
      "\u001b[31mOut of other SSMs, H3 and Hyena are similar, contrary to the ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 350, tokens None, triggered by: 0.18\n",
      "\u001b[32mndings in Poli et al. (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 351, tokens None, triggered by: 0.28\n",
      "\u001b[34m# 4.2 Language Modeling We evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on both pretraining metrics (perplexity) and zero-shot evaluations.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 352, tokens None, triggered by: 0.09\n",
      "\u001b[35mWe set the model sizes (depth and width) to mirror GPT3 speciï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 353, tokens None, triggered by: 0.15\n",
      "\u001b[31mcations.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 354, tokens None, triggered by: 0.28\n",
      "\u001b[32mWe use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training recipe described in Brown et al. (2020). All training details are in Appendix E.2.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 355, tokens None, triggered by: 0.20\n",
      "\u001b[34m# 4.2.1 Scaling Laws\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 356, tokens None, triggered by: 0.10\n",
      "\u001b[35mFor baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the strongest Transformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 357, tokens None, triggered by: 0.17\n",
      "\u001b[31m11\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 358, tokens None, triggered by: 0.18\n",
      "\u001b[32mScaling Laws on The Pile (Sequence Length 2048) Scaling Laws on The Pile (Sequence Length 8192) 2x10\" 2x10 Hyena Hyena RWKV s RWKV â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 359, tokens None, triggered by: 0.27\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 360, tokens None, triggered by: 0.27\n",
      "\u001b[35mTransformer Fy â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 361, tokens None, triggered by: 0.16\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 362, tokens None, triggered by: 0.16\n",
      "\u001b[32mTransformer fd RetNet 2 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 363, tokens None, triggered by: 0.25\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 364, tokens None, triggered by: 0.27\n",
      "\u001b[35mRetNet 3+ 2 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 365, tokens None, triggered by: 0.28\n",
      "\u001b[31mHH wd â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 366, tokens None, triggered by: 0.21\n",
      "\u001b[32m= Transformers |, | â â Transformert+ â â Mamba zg â â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 367, tokens None, triggered by: 0.28\n",
      "\u001b[34mMamba 2 2 S a 6x 10Â° 1 7 6x 10Â° 1 7 10\"?\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 368, tokens None, triggered by: 0.22\n",
      "\u001b[35m102 10 107Â° FLOPs (log scale) FLOPs (log scale) s 8 fd 2 2 > 3 2 2 S a\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 369, tokens None, triggered by: 0.27\n",
      "\u001b[31mFigure 4: (Scaling Laws.) Models of size â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 370, tokens None, triggered by: 0.24\n",
      "\u001b[32m125ð to â 1.3ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 371, tokens None, triggered by: 0.23\n",
      "\u001b[34mµ parameters, trained on the Pile. Mamba scales better than all other attention-free models and is the first to match the performance of a very strong â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 372, tokens None, triggered by: 0.20\n",
      "\u001b[35mTransformer++â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 373, tokens None, triggered by: 0.20\n",
      "\u001b[31mrecipe that has now become standard, particularly as the sequence length grows.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 374, tokens None, triggered by: 0.22\n",
      "\u001b[32marchitectures (e.g. rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher learning rates). We also compare against other recent subquadratic architectures (Figure 4).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 375, tokens None, triggered by: 0.24\n",
      "\u001b[34mAll model details are in Appendix E.2. Figure 4 shows scaling laws under the standard Chinchilla (Hoï¬ mann et al. 2022) protocol, on models from â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 376, tokens None, triggered by: 0.22\n",
      "\u001b[35m125ð to â 1.3ð µ parameters.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 377, tokens None, triggered by: 0.14\n",
      "\u001b[31mMamba is the ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 378, tokens None, triggered by: 0.25\n",
      "\u001b[32mrst attention-free model to match the performance of a very strong Transformer recipe (Transformer++) that has now become standard, particularly as the sequence length grows. We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior strong recurrent models that can also be interpreted as SSMs, due to a lack of eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 379, tokens None, triggered by: 0.21\n",
      "\u001b[34mcient implementation leading to out-of-memory or unrealistic computation requirements.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 380, tokens None, triggered by: 0.17\n",
      "\u001b[35m# 4.2.2 Downstream Evaluations Table 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We compare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al. 2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our models. (Note that Mamba and Pythia are trained with context length 2048, while RWKV was trained with context length 1024.)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 381, tokens None, triggered by: 0.29\n",
      "\u001b[31m# 4.3 DNA Modeling Motivated by the success of large language models, there has been recent exploration into using the foundation model paradigm for genomics. DNA has been likened to language in that it consists of sequences of discrete tokens with a ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 382, tokens None, triggered by: 0.17\n",
      "\u001b[32mnite vocab.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 383, tokens None, triggered by: 0.29\n",
      "\u001b[34mIt is also known for requiring long-range dependencies to model (Avsec et al. 2021).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 384, tokens None, triggered by: 0.29\n",
      "\u001b[35mWe investigate Mamba as a FM backbone for pretraining and ï¬ ne-tuning in the same setting as recent works on long-sequence models for DNA (Nguyen, Poli, et al. 2023). In particular, we focus on two explorations of scaling laws across model size and sequence length (Figure 5), and a diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 385, tokens None, triggered by: 0.14\n",
      "\u001b[31mcult downstream synthetic classiï¬ cation task requiring long context (Figure 6). For pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training and model details (see also Appendix E.2). For the dataset, we largely follow the setup of HyenaDNA (Nguyen, Poli, et al. 2023), which uses the HG38 dataset for pretraining consisting of a single human genome with about 4.5 billion tokens (DNA base pairs) in the training split.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 386, tokens None, triggered by: 0.15\n",
      "\u001b[32m# 4.3.1 Scaling: Model Size In this experiment, we investigate the scaling properties of genomics foundation models with various model backbones (Figure 5 Left).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 387, tokens None, triggered by: 0.28\n",
      "\u001b[34mTraining.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 388, tokens None, triggered by: 0.04\n",
      "\u001b[35mTo advantage the baselines, we train on a short sequence length of 1024; as shown in Section 4.3.2, we expect results to favor Mamba even more at longer sequence lengths.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 389, tokens None, triggered by: 0.16\n",
      "\u001b[31mWe ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 390, tokens None, triggered by: 0.19\n",
      "\u001b[32mx a global batch size of 1024, for a\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 391, tokens None, triggered by: 0.14\n",
      "\u001b[34m12\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 392, tokens None, triggered by: 0.24\n",
      "\u001b[35mTable 3: (Zero-shot Evaluations.) Best results for each size in bold. We compare against open source LMs with various tokenizers, trained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and tokenizer (GPT-NeoX-20B). For each model size, Mamba is best-in-class on every single evaluation result, and generally matches baselines at twice the model size.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 393, tokens None, triggered by: 0.13\n",
      "\u001b[31mModel Token.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 394, tokens None, triggered by: 0.20\n",
      "\u001b[32mPile ppl â LAMBADA LAMBADA HellaSwag ppl â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 395, tokens None, triggered by: 0.21\n",
      "\u001b[34macc â acc â acc â acc â acc â acc â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 396, tokens None, triggered by: 0.30\n",
      "\u001b[35mHybrid H3-130M GPT2 â Pythia-160M Mamba-130M NeoX NeoX 29.64 10.56 89.48 38.10 16.07 25.77 33.0 44.3 31.7 30.2 35.3 64.2 61.4 64.5 44.4 43.2 48.0 24.2 24.1 24.3 50.6 51.9 51.9 40.1 40.6 44.7 Hybrid H3-360M GPT2 â Pythia-410M Mamba-370M NeoX NeoX 9.95 8.28 12.58 10.84 8.14 48.0 51.4 55.6 41.5 40.6 46.5 68.1 66.9 69.5 51.4 52.1 55.1 24.7 24.6 28.0 54.1 53.8 55.3 48.0 48.2 50.0 Pythia-1B Mamba-790M NeoX NeoX 7.82 7.33 7.92 6.02 56.1 62.7 47.2 55.1 70.7 72.1 57.0 61.2 27.1 29.5 53.5 56.1 51.9 57.1 GPT-Neo 1.3B Hybrid H3-1.3B OPT-1.3B Pythia-1.4B RWKV-1.5B Mamba-1.4B GPT2 â GPT2 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 397, tokens None, triggered by: 0.12\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 398, tokens None, triggered by: 0.30\n",
      "\u001b[32mOPT 7.51 NeoX 7.70 NeoX NeoX 6.80 7.50 11.25 6.64 6.08 7.04 5.04 57.2 49.6 58.0 61.7 56.4 64.9 48.9 52.6 53.7 52.1 52.5 59.1 71.1 71.3 72.4 71.0 72.4 74.2 56.2 59.2 56.7 60.5 60.5 65.5 25.9 28.1 29.6 28.5 29.4 32.8 54.9 56.9 59.5 57.2 54.6 61.5 52.4 53.0 55.0 55.2 54.3 59.7 GPT-Neo 2.7B Hybrid H3-2.7B OPT-2.7B Pythia-2.8B RWKV-3B Mamba-2.8B GPT2 â GPT2 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 399, tokens None, triggered by: 0.13\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 400, tokens None, triggered by: 0.13\n",
      "\u001b[35mOPT 6.73 NeoX 7.00 NeoX NeoX 6.22 5.63 7.92 5.12 5.04 5.24 4.23 62.2 55.7 63.6 64.7 63.9 69.2 55.8 59.7 60.6 59.3 59.6 66.1 72.1 73.3 74.8 74.0 73.7 75.2 61.1 65.6 60.8 64.1 67.8 69.7 30.2 32.3 31.3 32.9 33.1 36.3 57.6 61.4 61.0 59.7 59.6 63.5 56.5 58.0 58.7 59.1 59.6 63.3 GPT-J-6B OPT-6.7B Pythia-6.9B RWKV-7.4B GPT2 OPT NeoX NeoX â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 401, tokens None, triggered by: 0.17\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 402, tokens None, triggered by: 0.21\n",
      "\u001b[32m6.51 6.31 4.10 4.25 4.45 4.38 68.3 67.7 67.1 67.2 66.3 67.2 64.0 65.5 75.4 76.3 75.2 76.1 67.0 65.6 67.3 67.8 36.6 34.9 35.5 37.5 64.1 65.5 61.3 61.0 63.0 62.9 61.7 62.5\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 403, tokens None, triggered by: 0.28\n",
      "\u001b[34mtotal of 220 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 404, tokens None, triggered by: 0.21\n",
      "\u001b[35m1ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 405, tokens None, triggered by: 0.24\n",
      "\u001b[31mtokens per batch.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 406, tokens None, triggered by: 0.19\n",
      "\u001b[32mModels were trained for 10ð ¾ gradient steps for a total of 10ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 407, tokens None, triggered by: 0.24\n",
      "\u001b[34mµ tokens.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 408, tokens None, triggered by: 0.25\n",
      "\u001b[35mResults.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 409, tokens None, triggered by: 0.26\n",
      "\u001b[31mFigure 5 (Left) shows that Mambaâ s pretraining perplexity improves smoothly with model size, and that Mamba scales better than both HyenaDNA and Transformer++. For example, at the largest model size of â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 410, tokens None, triggered by: 0.19\n",
      "\u001b[32m40ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 411, tokens None, triggered by: 0.28\n",
      "\u001b[34mparameters, the curve shows that Mamba can match the Transformer++ and HyenaDNA models with roughly 3Ã\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 412, tokens None, triggered by: 0.24\n",
      "\u001b[35mto 4Ã\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 413, tokens None, triggered by: 0.25\n",
      "\u001b[31mfewer parameters.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 414, tokens None, triggered by: 0.25\n",
      "\u001b[32m# 4.3.2 Scaling:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 415, tokens None, triggered by: 0.07\n",
      "\u001b[34mContext Length In the next DNA experiment, we investigate the scaling properties of models with respect to sequence length. We only compare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at longer sequence lengths. We pretrain models on sequence lengths 210 = 1024, 212 = 4096, 214 = 16384, 216 = 65536, 218 = 262144, 220 = 1048576.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 416, tokens None, triggered by: 0.12\n",
      "\u001b[35mWe ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 417, tokens None, triggered by: 0.27\n",
      "\u001b[31mx a model size of 6 layers by width 128 (about 1.3M-1.4M parameters). Models were trained for 20ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 418, tokens None, triggered by: 0.27\n",
      "\u001b[32m¾ gradient steps for a total of â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 419, tokens None, triggered by: 0.19\n",
      "\u001b[34m330ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 420, tokens None, triggered by: 0.20\n",
      "\u001b[35mµ tokens.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 421, tokens None, triggered by: 0.19\n",
      "\u001b[31mThe longer sequence lengths used sequence length warmup similar to (Nguyen, Poli, et al. 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 422, tokens None, triggered by: 0.16\n",
      "\u001b[32mResults.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 423, tokens None, triggered by: 0.20\n",
      "\u001b[34mFigure 5 (Right) shows that Mamba is able to make use of longer context even up to extremely long sequences of length 1M, and its pretraining perplexity improves as the context increases. On the other hand, the HyenaDNA model gets worse with sequence length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 424, tokens None, triggered by: 0.06\n",
      "\u001b[35mThis is intuitive from the discussion in Section 3.5 on properties of the selection mechanism. In particular, LTI models cannot selectively ignore information; from a convolutional perspective, a very long convolution kernel is aggregating all information across a long sequence\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 425, tokens None, triggered by: 0.12\n",
      "\u001b[31m13\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 426, tokens None, triggered by: 0.17\n",
      "\u001b[32mScaling Laws on the Human Genome (HG38) Scaling Laws - Sequence Length (HG38) â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 427, tokens None, triggered by: 0.26\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 428, tokens None, triggered by: 0.27\n",
      "\u001b[35mHyenaDNa 1.4m â = Mamba 1.4M â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 429, tokens None, triggered by: 0.24\n",
      "\u001b[31mâ Mamba 7M ae â â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 430, tokens None, triggered by: 0.28\n",
      "\u001b[32mHyenaDNA 3.00 4 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 431, tokens None, triggered by: 0.16\n",
      "\u001b[34mMamba â â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 432, tokens None, triggered by: 0.28\n",
      "\u001b[35mTransformert+ 2.98 | Perplexity Perplexity 2.80 4 284 2.754 274 r T r r r ; 10Â° 107 103 10 105 10Â° Parameters (log scale) Sequence Length Figure 5: (DNA Scaling Laws.) Pretraining on the HG38 (human genome) dataset. (Left) Fixing short context length 210 = 1024 and increasing size from â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 433, tokens None, triggered by: 0.13\n",
      "\u001b[31m200ð ¾ to â 40ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 434, tokens None, triggered by: 0.29\n",
      "\u001b[32mparameters, Mamba scales better than baselines. (Right) Fixing model size and increasing sequence lengths while keeping tokens/batch and total training tokens fixed. Unlike baselines, the selection mechanism of Mamba facilitates better performance with increasing context length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 435, tokens None, triggered by: 0.16\n",
      "\u001b[34mFinetuning Accuracy (Species DNA Classification) 0.8] â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 436, tokens None, triggered by: 0.23\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 437, tokens None, triggered by: 0.23\n",
      "\u001b[31mHyenaDNA1.4M 0.7-| â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 438, tokens None, triggered by: 0.24\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 439, tokens None, triggered by: 0.24\n",
      "\u001b[34mMamba 1.4m â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 440, tokens None, triggered by: 0.24\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 441, tokens None, triggered by: 0.24\n",
      "\u001b[31mMamba 7M mag] â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 442, tokens None, triggered by: 0.29\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 443, tokens None, triggered by: 0.29\n",
      "\u001b[34mRandom g 5 os 3 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 444, tokens None, triggered by: 0.18\n",
      "\u001b[35m8 oA 034 024 --------------------------------- T T T T 103 10Â¢ 108 10 Sequence Length Scaling Laws - Sequence Length (YouTubeMix) 1.475 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 445, tokens None, triggered by: 0.23\n",
      "\u001b[31mâ SA+FEN 1.450 4 â â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 446, tokens None, triggered by: 0.30\n",
      "\u001b[32mMamba @ 1.4254 2 1.400 4 5 o 1.375 4 Â© 1.3504 1.325 4 1.300 T T T 10* 10Â° 10 Sequence Length\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 447, tokens None, triggered by: 0.20\n",
      "\u001b[34mFigure 6: (Great Apes DNA Classification.) Accuracy after fine-tuning on sequences of length 210 = 1024 up to 220 = 1048576 using pretrained models of the same context length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 448, tokens None, triggered by: 0.21\n",
      "\u001b[35mNu- merical results in Table 13.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 449, tokens None, triggered by: 0.24\n",
      "\u001b[31mFigure 7: (Audio Pretraining.) Mamba improves performance over prior state-of-the-art (Sashimi) in autoregressive audio mod- eling, while improving up to minute-long context or million- length sequences (controlling for computation).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 450, tokens None, triggered by: 0.19\n",
      "\u001b[32mwhich may be very noisy.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 451, tokens None, triggered by: 0.24\n",
      "\u001b[34mNote that while HyenaDNA claims to improve with longer context, their results do not control for computation time.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 452, tokens None, triggered by: 0.20\n",
      "\u001b[35m# 4.3.3 Synthetic Species Classification We evaluate models on a downstream task of classifying between 5 diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 453, tokens None, triggered by: 0.25\n",
      "\u001b[31merent species by randomly sampling a contigu- ous segment of their DNA. This task is adapted from HyenaDNA, which used the species {human, lemur, mouse, pig, hippo}.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 454, tokens None, triggered by: 0.12\n",
      "\u001b[32mWe modify the task to be signiï¬ cantly more challenging by classifying between the ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 455, tokens None, triggered by: 0.08\n",
      "\u001b[34mve great apes species {human, chimpanzee, gorilla, orangutan, bonobo}, which are known to share 99% of their DNA.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 456, tokens None, triggered by: 0.25\n",
      "\u001b[35m# 4.4 Audio Modeling and Generation For the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols (Goel et al. 2022).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 457, tokens None, triggered by: 0.24\n",
      "\u001b[31mThis model comprises\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 458, tokens None, triggered by: 0.21\n",
      "\u001b[32m1. a U-Net backbone with two stages of pooling by a factor ð that doubles the model dimension ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 459, tokens None, triggered by: 0.18\n",
      "\u001b[34m· per stage, 2. alternating S4 and MLP blocks in each stage. We consider replacing the S4+MLP blocks with Mamba blocks.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 460, tokens None, triggered by: 0.23\n",
      "\u001b[35mExperiment details are in Appendix E.4.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 461, tokens None, triggered by: 0.09\n",
      "\u001b[31m# 4.4.1 Long-Context Autoregressive Pretraining We evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound 2017), a standard piano music dataset used by prior work consisting of 4 hours of solo piano music, sampled at a rate of\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 462, tokens None, triggered by: 0.09\n",
      "\u001b[32m14\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 463, tokens None, triggered by: 0.18\n",
      "\u001b[34m16000 Hz Pretraining details largely follow the standard language modeling setup (Section 4.2).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 464, tokens None, triggered by: 0.22\n",
      "\u001b[35mFigure 7 evaluates the eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 465, tokens None, triggered by: 0.29\n",
      "\u001b[31mect of increasing training sequence lengths from 213 = 8192 to 220 â 106, while keeping computation ï¬ xed. (There are some slight edge cases to the way the data is curated, which may lead to kinks in the scaling curves.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 466, tokens None, triggered by: 0.19\n",
      "\u001b[32mFor example, only minute-long clips were available so the maximum sequence length is actually bounded by 60ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 467, tokens None, triggered by: 0.19\n",
      "\u001b[34mâ 16000ð »ð § = 960000.)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 468, tokens None, triggered by: 0.30\n",
      "\u001b[35mBoth Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba is better throughout, and the gap widens at longer lengths. The main metric is bits per byte (BPB), which is a constant factor log(2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 469, tokens None, triggered by: 0.19\n",
      "\u001b[31mWe note one important detail: this is the only experiment in this paper in which we switched from the real parameterization to complex (Section 3.6). We show additional ablations in Appendix E.4.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 470, tokens None, triggered by: 0.20\n",
      "\u001b[32m# 4.4.2 Autoregressive Speech Generation SC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting of 1-second clips sampled at 16000 Hz of the digits â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 471, tokens None, triggered by: 0.12\n",
      "\u001b[34mzeroâ through â nineâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 472, tokens None, triggered by: 0.22\n",
      "\u001b[35mwith highly variable characteristics.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 473, tokens None, triggered by: 0.21\n",
      "\u001b[31mWe largely follow the autoregressive training setup and generation protocol of Goel et al. (2022). Table 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al. (2022): WaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette 2019), Diï¬ Wave (Z.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 474, tokens None, triggered by: 0.25\n",
      "\u001b[32mKong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art (and much larger) GAN- and diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 475, tokens None, triggered by: 0.29\n",
      "\u001b[34musion- based models. A larger model parameter-matched to the baselines further improves on ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 476, tokens None, triggered by: 0.22\n",
      "\u001b[35mdelity metrics dramatically.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 477, tokens None, triggered by: 0.23\n",
      "\u001b[31mTable 5 takes the small Mamba model and investigates combinations of diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 478, tokens None, triggered by: 0.28\n",
      "\u001b[32merent architectures for the outer stages and center stage.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 479, tokens None, triggered by: 0.16\n",
      "\u001b[34mIt shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba > S4+MLP > MHA+MLP in the center blocks. Table 4: (SC09) Automated metrics for unconditional generation on a challenging dataset of fixed-length speech clips. (Top to Bottom) Autoregressive baselines, non-autoregressive baselines, Mamba, and dataset metrics. Table 5: (SC09 Model Ablations) Models with 6M parameters.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 480, tokens None, triggered by: 0.09\n",
      "\u001b[35mIn SaShiMiâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 481, tokens None, triggered by: 0.24\n",
      "\u001b[31ms U-Net backbone, there are 8 center blocks operat- ing on sequence length 1000, sandwiched on each side by 8 outer blocks on sequence length 4000, sandwiched by 8 outer blocks on sequence length 16000 (40 blocks total). The architecture of the 8 center blocks are ablated independently of the rest. Note that Transformers (MHA+MLP) were not tested in the more im- portant outer blocks because of efficiency constraints.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 482, tokens None, triggered by: 0.25\n",
      "\u001b[32mModel Params NLL â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 483, tokens None, triggered by: 0.16\n",
      "\u001b[34mFID â IS â mIS â AM â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 484, tokens None, triggered by: 0.23\n",
      "\u001b[35mSampleRNN WaveNet SaShiMi 35.0M 4.2M 5.8M 2.042 1.925 1.873 8.96 5.08 1.99 1.71 2.27 5.13 3.02 5.80 42.57 1.76 1.47 0.74 WaveGAN DiffWave + SaShiMi Mamba Mamba Train Test 19.1M 24.1M 23.0M 6.1M 24.3M - - - - - 1.852 1.860 - - 2.03 1.92 1.42 0.94 0.67 0.00 0.02 4.90 5.26 5.94 6.26 7.33 8.56 8.33 36.10 51.21 69.17 88.54 144.9 292.5 257.6 0.80 0.68 0.59 0.52 0.36 0.16 0.19 Outer Center S4+MLP MHA+MLP S4+MLP S4+MLP Mamba Mamba Mamba Mamba S4+MLP MHA+MLP S4+MLP Mamba NLL â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 485, tokens None, triggered by: 0.28\n",
      "\u001b[31m1.859 1.867 1.859 1.850 1.853 1.852 FID â 1.45 1.43 1.42 1.37 1.07 0.94 IS â 5.06 5.42 5.71 5.63 6.05 6.26 mIS â 47.03 53.54 56.51 58.23 73.34 88.54 AM â 0.70 0.65 0.64 0.62 0.55 0.52\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 486, tokens None, triggered by: 0.15\n",
      "\u001b[32m4.5 Speed and Memory Benchmarks We benchmark the speed of the SSM scan operation (state expansion ð = 16), as well as the end-to-end inference throughput of Mamba, in Figure 8.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 487, tokens None, triggered by: 0.18\n",
      "\u001b[34mOur eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 488, tokens None, triggered by: 0.19\n",
      "\u001b[35mcient SSM scan is faster than the best attention implementation that we know of (FlashAttention-2 (Dao 2023)) beyond sequence length 2K, and up to 20-40Ã faster than a standard scan implementation in PyTorch.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 489, tokens None, triggered by: 0.22\n",
      "\u001b[31mMamba achieves 4-5Ã\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 490, tokens None, triggered by: 0.24\n",
      "\u001b[32mhigher inference throughput than a Transformer of similar size, since without the KV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained) would have higher inference throughput than a 5Ã smaller Transformer-1.3B.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 491, tokens None, triggered by: 0.13\n",
      "\u001b[34mDetails in Appendix E.5, which additionally includes a benchmark of memory consumption.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 492, tokens None, triggered by: 0.15\n",
      "\u001b[35m15\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 493, tokens None, triggered by: 0.23\n",
      "\u001b[31mScan vs Convolution vs Attention time (A100 80GB PCle) Inference throughput on A100 80GB (prompt length 2048) â Flashattention-2 ame ee ES 1000-1 â convolution @ 1500] mm Mamba 6.98 wwe â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 494, tokens None, triggered by: 0.17\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 495, tokens None, triggered by: 0.17\n",
      "\u001b[34mScan (PyTorch) Py mmm Transformer 6.78 100 4 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 496, tokens None, triggered by: 0.26\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 497, tokens None, triggered by: 0.30\n",
      "\u001b[31mScan (ours) Ei % 00M 2 a tod S 1000 B us Ff = 2 500 â = pad oid r S12 1k 2k Â«= 4k BKK 32K GK 128k 256K 512k 1 2 Hi A 16 32 oa 128 Sequence length Batch size\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 498, tokens None, triggered by: 0.23\n",
      "\u001b[32m@ = ~ Â£\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 499, tokens None, triggered by: 0.21\n",
      "\u001b[34mFigure 8: (Efficiency Benchmarks.) (Left) Training: our efficient scan is 40Ã faster than a standard implementation. (Right) Inference: as a recurrent model, Mamba can achieve 5Ã higher throughput than Transformers.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 500, tokens None, triggered by: 0.28\n",
      "\u001b[35m# 4.6 Model Ablations We perform a series of detailed ablations on components of our model, focusing on the setting of language modeling with size â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 501, tokens None, triggered by: 0.24\n",
      "\u001b[31m350M models at Chinchilla token counts (same setting as Figure 4).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 502, tokens None, triggered by: 0.20\n",
      "\u001b[32m# 4.6.1 Architecture\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 503, tokens None, triggered by: 0.28\n",
      "\u001b[34mTable 6 investigates the eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 504, tokens None, triggered by: 0.18\n",
      "\u001b[35mects of the architecture (block) and its inner SSM layer (Figure 3).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 505, tokens None, triggered by: 0.26\n",
      "\u001b[31mWe ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 506, tokens None, triggered by: 0.30\n",
      "\u001b[32mnd that\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 507, tokens None, triggered by: 0.16\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 508, tokens None, triggered by: 0.16\n",
      "\u001b[35m¢ Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very similar.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 509, tokens None, triggered by: 0.19\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 510, tokens None, triggered by: 0.26\n",
      "\u001b[32m¢ Replacing the complex-valued S4 variant from previous work with a real-valued one does not aï¬ ect performance much, suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 511, tokens None, triggered by: 0.27\n",
      "\u001b[34mciency.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 512, tokens None, triggered by: 0.26\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 513, tokens None, triggered by: 0.27\n",
      "\u001b[31m¢ Replacing any of these with a selective SSM (S6) signiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 514, tokens None, triggered by: 0.15\n",
      "\u001b[32mcantly improves performance, validating the motivation of Section 3.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 515, tokens None, triggered by: 0.18\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 516, tokens None, triggered by: 0.25\n",
      "\u001b[35m¢ The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a selective layer). We also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA (a hybrid attention architecture) in Appendix E.2.2.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 517, tokens None, triggered by: 0.21\n",
      "\u001b[31m# 4.6.2 Selective SSM Table 7 ablates the selective SSM layer by considering diï¬ erent combinations of selective â , B, and C param- eters (Algorithm 2), showing that â is the most important parameter due to its connection to RNN gating (Theorem 1).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 518, tokens None, triggered by: 0.22\n",
      "\u001b[32mTable 8 considers diï¬ erent initializations of the SSM, which have been shown to make a large diï¬ erence in some data modalities and settings (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022). On language modeling, we ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 519, tokens None, triggered by: 0.30\n",
      "\u001b[34mnd that simpler real-valued diagonal initializations (S4D-Real, row 3) instead of more standard complex-valued parameterizations (S4D-Lin, row 1) perform better. Random initializations also work well, consistent with ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 520, tokens None, triggered by: 0.26\n",
      "\u001b[35mndings from prior work (Mehta et al. 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 521, tokens None, triggered by: 0.21\n",
      "\u001b[31mTable 9 and Table 10 consider varying the dimension of the â and (B, C) projections respectively.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 522, tokens None, triggered by: 0.25\n",
      "\u001b[32mChanging them from static to selective provides the most beneï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 523, tokens None, triggered by: 0.24\n",
      "\u001b[34mt, while increasing the dimensions further generally improves performance modestly with a small increase in parameter count. Of particular note is the dramatic improvement of the selective SSM when the state size ð is increased, with over a 1.0 perplexity improvement for a cost of only 1% additional parameters.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 524, tokens None, triggered by: 0.08\n",
      "\u001b[35mThis validates our core motivation in Sections 3.1 and 3.3.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 525, tokens None, triggered by: 0.10\n",
      "\u001b[31m16\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 526, tokens None, triggered by: 0.22\n",
      "\u001b[32mTable 6: (Ablations:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 527, tokens None, triggered by: 0.21\n",
      "\u001b[34mArchitecture and SSM layer.) The Mamba block performs similarly to H3 while being simpler. In the inner layer, there is little difference among different parameterizations of LTI models, while selective SSMs (S6) provide a large improvement. More specifically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 528, tokens None, triggered by: 0.25\n",
      "\u001b[35mModel Arch. SSM Layer Perplexity Model Arch. SSM Layer Perplexity Hyena H3 H3 H3 H3 - H3 - Hyena S4 (complex) S4 (real) S6 10.24 10.30 10.34 8.95 Mamba Hyena - Mamba - - Mamba Mamba Mamba S4 (complex) S4 (real) S6 10.75 10.54 10.56 8.69 Table 7: (Ablations: Selective parameters.) â is the most im- portant parameter (Theorem 1), but using multiple selective pa- rameters together synergizes.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 529, tokens None, triggered by: 0.27\n",
      "\u001b[31mTable 8: (Ablations: Parameterization of A.) The more standard initializations based on S4D-Lin (Gu, Gupta, et al. 2022) perform worse than S4D-Real or a random initializa- tion, when the SSM is selective.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 530, tokens None, triggered by: 0.15\n",
      "\u001b[32mSelective A Selective B SelectiveC Perplexity \\Qx& xX Qk *Â®QX Qk Q&X 1093 10.15 9.98 9.81 8.71\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 531, tokens None, triggered by: 0.25\n",
      "\u001b[34mAð Initialization Að\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 532, tokens None, triggered by: 0.26\n",
      "\u001b[35m= â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 533, tokens None, triggered by: 0.26\n",
      "\u001b[31m1 Complex Real Að\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 534, tokens None, triggered by: 0.29\n",
      "\u001b[32m= â 1â 2 Að\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 535, tokens None, triggered by: 0.27\n",
      "\u001b[34m= â (ð + 1) Real Að\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 536, tokens None, triggered by: 0.23\n",
      "\u001b[35mâ ¼ exp(ð ©(0, 1)) Real Field + ð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 537, tokens None, triggered by: 0.27\n",
      "\u001b[31m2 9.16 8.85 8.71 8.71\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 538, tokens None, triggered by: 0.29\n",
      "\u001b[32mTable 9: (Ablations:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 539, tokens None, triggered by: 0.22\n",
      "\u001b[34mExpressivity of â .) The selection mechanism of â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 540, tokens None, triggered by: 0.25\n",
      "\u001b[35mconstructs it with a projection of the input. Project- ing it even to dim. 1 provides a large in- crease in performance; increasing it fur- ther provides further improvements at the cost of a modest increase in parameters.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 541, tokens None, triggered by: 0.28\n",
      "\u001b[31mState size fixed to ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 542, tokens None, triggered by: 0.26\n",
      "\u001b[32m= 16. Size of â proj. - 1 2 4 8 16 32 64 Params (M) 358.9 359.1 359.3 359.7 360.5 362.1 365.2 371.5 9.12 8.97 8.97 8.91 8.83 8.84 8.80 8.71 # Perplexity\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 543, tokens None, triggered by: 0.20\n",
      "\u001b[34mTable 10: (Ablations: SSM state dimension.) (Top) Constant B and C (Bottom) Selective B and C. Increasing the SSM state dimension ð , which can be viewed as an expansion factor on the dimension of the recurrent state, can significantly improve performance for a negligible cost in parameters/FLOPs, but only when B and C are also selective.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 544, tokens None, triggered by: 0.27\n",
      "\u001b[35mSize of â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 545, tokens None, triggered by: 0.19\n",
      "\u001b[31mprojection fixed to 64.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 546, tokens None, triggered by: 0.24\n",
      "\u001b[32mState dimension ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 547, tokens None, triggered by: 0.17\n",
      "\u001b[34mParams (M) Perplexity 1 2 4 8 16 1 2 4 8 16 367.1 367.4 368.0 369.1 371.5 367.1 367.4 368.0 369.1 371.5 9.88 9.86 9.82 9.82 9.81 9.73 9.40 9.09 8.84 8.71\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 548, tokens None, triggered by: 0.10\n",
      "\u001b[35m# 5 Discussion We discuss related work, limitations, and some future directions. Related Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an extended related work of SSMs and other related models.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 549, tokens None, triggered by: 0.19\n",
      "\u001b[31mNo Free Lunch:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 550, tokens None, triggered by: 0.26\n",
      "\u001b[32mContinuous-Discrete Spectrum.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 551, tokens None, triggered by: 0.07\n",
      "\u001b[34mStructured SSMs were originally deï¬ ned as discretizations of continuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as perceptual signals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes their weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 552, tokens None, triggered by: 0.12\n",
      "\u001b[35m17\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 553, tokens None, triggered by: 0.24\n",
      "\u001b[31mon data that LTI SSMs excel on.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 554, tokens None, triggered by: 0.27\n",
      "\u001b[32mOur ablations on audio waveforms examine this tradeoï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 555, tokens None, triggered by: 0.25\n",
      "\u001b[34min more detail.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 556, tokens None, triggered by: 0.26\n",
      "\u001b[35mDownstream Affordances.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 557, tokens None, triggered by: 0.26\n",
      "\u001b[31mTransformer-based foundation models (particularly LLMs) have a rich ecosystem of properties and modes of interaction with pretrained models, such as ï¬ ne-tuning, adaptation, prompting, in-context learning, instruction tuning, RLHF, quantization, and so on.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 558, tokens None, triggered by: 0.15\n",
      "\u001b[32mWe are particularly interested in whether Transformer alternatives such as SSMs have similar properties and aï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 559, tokens None, triggered by: 0.22\n",
      "\u001b[34mordances.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 560, tokens None, triggered by: 0.23\n",
      "\u001b[35mScaling.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 561, tokens None, triggered by: 0.28\n",
      "\u001b[31mOur empirical evaluation is limited to small model sizes, below the threshold of most strong open source LLMs (e.g. Llama (Touvron et al. 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023) and RetNet (Y. Sun et al. 2023), which have been evaluated at the 7B parameter scale and beyond.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 562, tokens None, triggered by: 0.24\n",
      "\u001b[32mIt remains to assess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve further engineering challenges and adjustments to the model that are not discussed in this paper.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 563, tokens None, triggered by: 0.20\n",
      "\u001b[34m# 6 Conclusion\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 564, tokens None, triggered by: 0.25\n",
      "\u001b[35mWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent reasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture, Mamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance of strong Transformer models. We are excited about the broad applications of selective state space models to build foundation models for diï¬ erent domains, especially in emerging modalities requiring long context such as genomics, audio, and video.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 565, tokens None, triggered by: 0.20\n",
      "\u001b[31mOur results suggest that Mamba is a strong candidate to be a general sequence model backbone.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 566, tokens None, triggered by: 0.18\n",
      "\u001b[32m# Acknowledgments We thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 567, tokens None, triggered by: 0.23\n",
      "\u001b[34m# References\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 568, tokens None, triggered by: 0.13\n",
      "\u001b[35m[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. â Unitary Evolution Recurrent Neural Networksâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 569, tokens None, triggered by: 0.10\n",
      "\u001b[31m. In: The\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 570, tokens None, triggered by: 0.21\n",
      "\u001b[32mInternational Conference on Machine Learning (ICML). 2016, pp. 1120â 1128. iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 571, tokens None, triggered by: 0.09\n",
      "\u001b[34mEffective Gene Expression Prediction from Sequence by Integrating Long-range Interactionsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 572, tokens None, triggered by: 0.28\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 573, tokens None, triggered by: 0.23\n",
      "\u001b[31mNature Methods 18.10 (2021), pp. 1196â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 574, tokens None, triggered by: 0.13\n",
      "\u001b[32m1203.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 575, tokens None, triggered by: 0.21\n",
      "\u001b[34mJimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 576, tokens None, triggered by: 0.15\n",
      "\u001b[35mUsing Fast Weights to Attend to the Recent Pastâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 577, tokens None, triggered by: 0.22\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 578, tokens None, triggered by: 0.12\n",
      "\u001b[32mAdvances in Neural Information Processing Systems (NeurIPS) 29 (2016).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 579, tokens None, triggered by: 0.09\n",
      "\u001b[34mJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 580, tokens None, triggered by: 0.23\n",
      "\u001b[35mLayer Normalizationâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 581, tokens None, triggered by: 0.19\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 582, tokens None, triggered by: 0.13\n",
      "\u001b[32mIn: arXiv preprint arXiv:1607.06450 (2016). [2] [3] [4] [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. â Neural Machine Translation by Jointly Learning to Align and Translateâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 583, tokens None, triggered by: 0.23\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 584, tokens None, triggered by: 0.18\n",
      "\u001b[35mThe International Conference on Learning Representations (ICLR). 2015.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 585, tokens None, triggered by: 0.14\n",
      "\u001b[31m[6] David Balduzzi and Muhammad Ghifary. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 586, tokens None, triggered by: 0.11\n",
      "\u001b[32mStrongly-typed Recurrent Neural Networksâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 587, tokens None, triggered by: 0.22\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 588, tokens None, triggered by: 0.28\n",
      "\u001b[35mInternational Con- ference on Machine Learning. PMLR. 2016, pp. 1292â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 589, tokens None, triggered by: 0.04\n",
      "\u001b[31m1300.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 590, tokens None, triggered by: 0.08\n",
      "\u001b[32m[7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 591, tokens None, triggered by: 0.07\n",
      "\u001b[34mPythia:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 592, tokens None, triggered by: 0.13\n",
      "\u001b[35mA Suite for Analyzing Large Language Models across Training and Scalingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 593, tokens None, triggered by: 0.18\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 594, tokens None, triggered by: 0.21\n",
      "\u001b[32mThe International Conference on Machine Learning (ICML). PMLR. 2023, pp. 2397â 2430.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 595, tokens None, triggered by: 0.15\n",
      "\u001b[34m[8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 596, tokens None, triggered by: 0.18\n",
      "\u001b[35mPIQA:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 597, tokens None, triggered by: 0.14\n",
      "\u001b[31mReasoning about Physical Commonsense in Natural Languageâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 598, tokens None, triggered by: 0.23\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 599, tokens None, triggered by: 0.25\n",
      "\u001b[34mProceedings of the AAAI conference on Artificial Intelligence.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 600, tokens None, triggered by: 0.27\n",
      "\u001b[35mVol. 34. 05. 2020, pp. 7432â 7439.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 601, tokens None, triggered by: 0.16\n",
      "\u001b[31m[9] Guy E Blelloch. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 602, tokens None, triggered by: 0.11\n",
      "\u001b[32mPrefix Sums and Their Applicationsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 603, tokens None, triggered by: 0.26\n",
      "\u001b[34m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 604, tokens None, triggered by: 0.19\n",
      "\u001b[35mIn: (1990). [10]\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 605, tokens None, triggered by: 0.13\n",
      "\u001b[31mJames Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 606, tokens None, triggered by: 0.16\n",
      "\u001b[32mQuasi-recurrent Neural Networksâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 607, tokens None, triggered by: 0.20\n",
      "\u001b[34m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 608, tokens None, triggered by: 0.16\n",
      "\u001b[35mIn: arXiv preprint arXiv:1611.01576 (2016).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 609, tokens None, triggered by: 0.18\n",
      "\u001b[31m18\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 610, tokens None, triggered by: 0.20\n",
      "\u001b[32m[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee- lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 611, tokens None, triggered by: 0.13\n",
      "\u001b[34mLanguage Models are Few-shot Learnersâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 612, tokens None, triggered by: 0.23\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 613, tokens None, triggered by: 0.15\n",
      "\u001b[31mAdvances in Neural Information Processing Systems (NeurIPS) 33 (2020), pp. 1877â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 614, tokens None, triggered by: 0.08\n",
      "\u001b[32m1901.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 615, tokens None, triggered by: 0.09\n",
      "\u001b[34m[12] Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 616, tokens None, triggered by: 0.15\n",
      "\u001b[35mScaling Transformer to 1M tokens and Beyond with RMTâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 617, tokens None, triggered by: 0.20\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 618, tokens None, triggered by: 0.20\n",
      "\u001b[32mIn: arXiv preprint arXiv:2304.11062 (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 619, tokens None, triggered by: 0.20\n",
      "\u001b[34m[13] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 620, tokens None, triggered by: 0.12\n",
      "\u001b[35mGenerating Long Sequences with Sparse Trans- formersâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 621, tokens None, triggered by: 0.18\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 622, tokens None, triggered by: 0.25\n",
      "\u001b[32mIn: arXiv preprint arXiv:1904.10509 (2019).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 623, tokens None, triggered by: 0.30\n",
      "\u001b[34m[14] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Pe- ter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 624, tokens None, triggered by: 0.13\n",
      "\u001b[35mRethinking Attention with Performersâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 625, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 626, tokens None, triggered by: 0.21\n",
      "\u001b[32mThe International Conference on Learning Representations (ICLR). 2021.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 627, tokens None, triggered by: 0.20\n",
      "\u001b[34m[15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 628, tokens None, triggered by: 0.28\n",
      "\u001b[35mPaLM:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 629, tokens None, triggered by: 0.17\n",
      "\u001b[31mScaling Language Modeling with Pathwaysâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 630, tokens None, triggered by: 0.23\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 631, tokens None, triggered by: 0.26\n",
      "\u001b[34mJournal of Machine Learning Research 24.240 (2023), pp. 1â 113. url: http://jmlr.org/ papers/v24/22-1144.html.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 632, tokens None, triggered by: 0.26\n",
      "\u001b[35mJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 633, tokens None, triggered by: 0.10\n",
      "\u001b[31mEmpirical Evaluation of Gated Re- current Neural Networks on Sequence Modelingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 634, tokens None, triggered by: 0.21\n",
      "\u001b[32m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 635, tokens None, triggered by: 0.24\n",
      "\u001b[34mIn: arXiv preprint arXiv:1412.3555 (2014).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 636, tokens None, triggered by: 0.20\n",
      "\u001b[35m[17] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 637, tokens None, triggered by: 0.19\n",
      "\u001b[31mThink you have Solved Question Answering? Try ARC, the AI2 Reasoning Challengeâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 638, tokens None, triggered by: 0.21\n",
      "\u001b[32m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 639, tokens None, triggered by: 0.21\n",
      "\u001b[34mIn: arXiv preprint arXiv:1803.05457 (2018).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 640, tokens None, triggered by: 0.18\n",
      "\u001b[35m[18] Tri Dao. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 641, tokens None, triggered by: 0.14\n",
      "\u001b[31mFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioningâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 642, tokens None, triggered by: 0.23\n",
      "\u001b[32m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 643, tokens None, triggered by: 0.22\n",
      "\u001b[34mIn: (2023). [19] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 644, tokens None, triggered by: 0.15\n",
      "\u001b[35mFlashAttention: Fast and Memory- Efficient Exact Attention with IO-Awarenessâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 645, tokens None, triggered by: 0.24\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 646, tokens None, triggered by: 0.18\n",
      "\u001b[32mAdvances in Neural Information Processing Systems (NeurIPS). 2022.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 647, tokens None, triggered by: 0.14\n",
      "\u001b[34m[20] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 648, tokens None, triggered by: 0.10\n",
      "\u001b[35mHungry Hungry Hippos:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 649, tokens None, triggered by: 0.13\n",
      "\u001b[31mTowards Language Modeling with State Space Modelsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 650, tokens None, triggered by: 0.23\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 651, tokens None, triggered by: 0.18\n",
      "\u001b[34mThe International Conference on Learning Representations (ICLR). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 652, tokens None, triggered by: 0.18\n",
      "\u001b[35m[21] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 653, tokens None, triggered by: 0.15\n",
      "\u001b[31mLanguage Modeling with Gated Convolu- tional Networksâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 654, tokens None, triggered by: 0.18\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 655, tokens None, triggered by: 0.25\n",
      "\u001b[34mThe International Conference on Machine Learning (ICML). PMLR. 2017, pp. 933â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 656, tokens None, triggered by: 0.20\n",
      "\u001b[35m941.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 657, tokens None, triggered by: 0.19\n",
      "\u001b[31m# [22] DeepSound. SampleRNN. https://github.com/deepsound-project/samplernn-pytorch. 2017. [23]\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 658, tokens None, triggered by: 0.15\n",
      "\u001b[32mJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 659, tokens None, triggered by: 0.14\n",
      "\u001b[34mLongNet:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 660, tokens None, triggered by: 0.17\n",
      "\u001b[35mScaling Transformers to 1,000,000,000 Tokensâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 661, tokens None, triggered by: 0.19\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 662, tokens None, triggered by: 0.26\n",
      "\u001b[32mIn: arXiv preprint arXiv:2307.02486 (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 663, tokens None, triggered by: 0.22\n",
      "\u001b[34m[24] Chris Donahue, Julian McAuley, and Miller Puckette. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 664, tokens None, triggered by: 0.16\n",
      "\u001b[35mAdversarial Audio Synthesisâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 665, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 666, tokens None, triggered by: 0.23\n",
      "\u001b[32mThe International Conference on Learning Representations (ICLR). 2019.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 667, tokens None, triggered by: 0.17\n",
      "\u001b[34m[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 668, tokens None, triggered by: 0.14\n",
      "\u001b[35mAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scaleâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 669, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 670, tokens None, triggered by: 0.17\n",
      "\u001b[32mThe International Conference on Learning Representations (ICLR). 2020.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 671, tokens None, triggered by: 0.10\n",
      "\u001b[34m[26] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 672, tokens None, triggered by: 0.13\n",
      "\u001b[35mA Mathematical Framework for Transformer Circuitsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 673, tokens None, triggered by: 0.21\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 674, tokens None, triggered by: 0.16\n",
      "\u001b[32mTransformer Circuits Thread (2021). https://transformer-circuits.pub/2021/framework/index.html. [27] Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 675, tokens None, triggered by: 0.22\n",
      "\u001b[34mBlock-\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 676, tokens None, triggered by: 0.25\n",
      "\u001b[35mState Transformerâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 677, tokens None, triggered by: 0.18\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 678, tokens None, triggered by: 0.26\n",
      "\u001b[32mIn: arXiv preprint arXiv:2306.09539 (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 679, tokens None, triggered by: 0.19\n",
      "\u001b[34m[28] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 680, tokens None, triggered by: 0.15\n",
      "\u001b[35mMulti-Head State Space Model for Sequence Modelingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 681, tokens None, triggered by: 0.29\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 682, tokens None, triggered by: 0.28\n",
      "\u001b[32mINTERSPEECH. 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 683, tokens None, triggered by: 0.28\n",
      "\u001b[34m[29] Karl J Friston, Lee Harrison, and Will Penny. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 684, tokens None, triggered by: 0.21\n",
      "\u001b[35mDynamic Causal Modellingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 685, tokens None, triggered by: 0.28\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 686, tokens None, triggered by: 0.17\n",
      "\u001b[32mNeuroimage 19.4 (2003), pp. 1273â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 687, tokens None, triggered by: 0.16\n",
      "\u001b[34m1302.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 688, tokens None, triggered by: 0.14\n",
      "\u001b[35m[30] Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christo- pher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 689, tokens None, triggered by: 0.14\n",
      "\u001b[31mSimple Hardware-efficient Long Convolutions for Sequence Modelingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 690, tokens None, triggered by: 0.20\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 691, tokens None, triggered by: 0.16\n",
      "\u001b[34mThe International Confer- ence on Machine Learning (ICML) (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 692, tokens None, triggered by: 0.17\n",
      "\u001b[35m[31] Ken-ichi Funahashi and Yuichi Nakamura. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 693, tokens None, triggered by: 0.15\n",
      "\u001b[31mApproximation of Dynamical Systems by Continuous Time Recur- rent Neural Networksâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 694, tokens None, triggered by: 0.25\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 695, tokens None, triggered by: 0.21\n",
      "\u001b[34mNeural Networks 6.6 (1993), pp. 801â 806.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 696, tokens None, triggered by: 0.17\n",
      "\u001b[35m19\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 697, tokens None, triggered by: 0.18\n",
      "\u001b[31m[32] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 698, tokens None, triggered by: 0.18\n",
      "\u001b[32mThe Pile:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 699, tokens None, triggered by: 0.11\n",
      "\u001b[34mAn 800GB Dataset of Diverse Text for Language Modelingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 700, tokens None, triggered by: 0.19\n",
      "\u001b[35m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 701, tokens None, triggered by: 0.19\n",
      "\u001b[31mIn: arXiv preprint arXiv:2101.00027 (2020).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 702, tokens None, triggered by: 0.17\n",
      "\u001b[32m[33] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 703, tokens None, triggered by: 0.15\n",
      "\u001b[34mA Framework for Few-shot Language Model Evaluation.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 704, tokens None, triggered by: 0.27\n",
      "\u001b[35mVersion v0.0.1.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 705, tokens None, triggered by: 0.25\n",
      "\u001b[31mSept. 2021. doi: 10.5281/zenodo.5371628. url: https://doi.org/10.5281/zenodo.5371628.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 706, tokens None, triggered by: 0.17\n",
      "\u001b[32m[34] Karan Goel, Albert Gu, Chris Donahue, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 707, tokens None, triggered by: 0.23\n",
      "\u001b[34mItâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 708, tokens None, triggered by: 0.09\n",
      "\u001b[35ms Raw!\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 709, tokens None, triggered by: 0.16\n",
      "\u001b[31mAudio Generation with State-Space Modelsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 710, tokens None, triggered by: 0.21\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 711, tokens None, triggered by: 0.10\n",
      "\u001b[34mThe International Conference on Machine Learning (ICML). 2022.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 712, tokens None, triggered by: 0.12\n",
      "\u001b[35m[35] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 713, tokens None, triggered by: 0.13\n",
      "\u001b[31mHIPPO:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 714, tokens None, triggered by: 0.12\n",
      "\u001b[32mRecurrent Memory with Optimal Polynomial Projectionsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 715, tokens None, triggered by: 0.23\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 716, tokens None, triggered by: 0.20\n",
      "\u001b[35mAdvances in Neural Information Processing Systems (NeurIPS). 2020.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 717, tokens None, triggered by: 0.15\n",
      "\u001b[31m[36] Albert Gu, Karan Goel, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 718, tokens None, triggered by: 0.11\n",
      "\u001b[32mEfficiently Modeling Long Sequences with Structured State Spacesâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 719, tokens None, triggered by: 0.24\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 720, tokens None, triggered by: 0.18\n",
      "\u001b[35mThe International Conference on Learning Representations (ICLR). 2022.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 721, tokens None, triggered by: 0.15\n",
      "\u001b[31m[37] Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hoffman, and Razvan Pascanu. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 722, tokens None, triggered by: 0.12\n",
      "\u001b[32mImproving the Gating Mech- anism of Recurrent Neural Networksâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 723, tokens None, triggered by: 0.20\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 724, tokens None, triggered by: 0.23\n",
      "\u001b[35mThe International Conference on Machine Learning (ICML). 2020.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 725, tokens None, triggered by: 0.21\n",
      "\u001b[31m[38] Albert Gu, Ankit Gupta, Karan Goel, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 726, tokens None, triggered by: 0.27\n",
      "\u001b[32mOn the Parameterization and Initialization of Diag-\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 727, tokens None, triggered by: 0.19\n",
      "\u001b[34monal State Space Modelsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 728, tokens None, triggered by: 0.23\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 729, tokens None, triggered by: 0.16\n",
      "\u001b[31mAdvances in Neural Information Processing Systems (NeurIPS). 2022.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 730, tokens None, triggered by: 0.14\n",
      "\u001b[32m[39] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 731, tokens None, triggered by: 0.16\n",
      "\u001b[34mCombining Recur- rent, Convolutional, and Continuous-time Models with the Linear State Space Layerâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 732, tokens None, triggered by: 0.23\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 733, tokens None, triggered by: 0.14\n",
      "\u001b[31mAdvances in Neural Information Processing Systems (NeurIPS). 2021.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 734, tokens None, triggered by: 0.12\n",
      "\u001b[32m[40] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 735, tokens None, triggered by: 0.12\n",
      "\u001b[34mHow to Train Your HIPPO:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 736, tokens None, triggered by: 0.14\n",
      "\u001b[35mState Space Models with Generalized Basis Projectionsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 737, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 738, tokens None, triggered by: 0.20\n",
      "\u001b[32mThe International Conference on Learning Representations (ICLR). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 739, tokens None, triggered by: 0.21\n",
      "\u001b[34m[41] Ankit Gupta, Albert Gu, and Jonathan Berant. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 740, tokens None, triggered by: 0.13\n",
      "\u001b[35mDiagonal State Spaces are as Effective as Structured State Spacesâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 741, tokens None, triggered by: 0.24\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 742, tokens None, triggered by: 0.23\n",
      "\u001b[32mAdvances in Neural Information Processing Systems 35 (2022), pp. 22982â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 743, tokens None, triggered by: 0.25\n",
      "\u001b[34m22994.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 744, tokens None, triggered by: 0.18\n",
      "\u001b[35m[42] David Ha, Andrew Dai, and Quoc V.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 745, tokens None, triggered by: 0.16\n",
      "\u001b[31mLe. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 746, tokens None, triggered by: 0.21\n",
      "\u001b[32mHyperNetworksâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 747, tokens None, triggered by: 0.27\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 748, tokens None, triggered by: 0.21\n",
      "\u001b[35mThe International Conference on Learning Rep- resentations (ICLR). 2017.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 749, tokens None, triggered by: 0.08\n",
      "\u001b[31m[43] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 750, tokens None, triggered by: 0.12\n",
      "\u001b[32mDream to Control: Learning Behav- iors by Latent Imaginationâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 751, tokens None, triggered by: 0.24\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 752, tokens None, triggered by: 0.24\n",
      "\u001b[35mThe International Conference on Learning Representations (ICLR). 2020. [44] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 753, tokens None, triggered by: 0.17\n",
      "\u001b[31mLiquid Structural State-Space Modelsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 754, tokens None, triggered by: 0.23\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 755, tokens None, triggered by: 0.28\n",
      "\u001b[34mThe International Conference on Learning Representations (ICLR). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 756, tokens None, triggered by: 0.25\n",
      "\u001b[35m[45] Mikael Henaff, Arthur Szlam, and Yann LeCun. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 757, tokens None, triggered by: 0.10\n",
      "\u001b[31mRecurrent Orthogonal Networks and Long-Memory Tasksâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 758, tokens None, triggered by: 0.21\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 759, tokens None, triggered by: 0.15\n",
      "\u001b[34mThe International Conference on Machine Learning (ICML). 2016.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 760, tokens None, triggered by: 0.24\n",
      "\u001b[35m[46] Dan Hendrycks and Kevin Gimpel. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 761, tokens None, triggered by: 0.14\n",
      "\u001b[31mGaussian Error Linear Units (GELUs)â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 762, tokens None, triggered by: 0.19\n",
      "\u001b[32m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 763, tokens None, triggered by: 0.29\n",
      "\u001b[34mIn: arXiv preprint arXiv:1606.08415 (2016). [47] Sepp Hochreiter and JÃ¼rgen Schmidhuber. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 764, tokens None, triggered by: 0.18\n",
      "\u001b[35mLong Short-Term Memoryâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 765, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 766, tokens None, triggered by: 0.23\n",
      "\u001b[32mNeural Computation 9.8 (1997),\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 767, tokens None, triggered by: 0.14\n",
      "\u001b[34mpp. 1735â 1780.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 768, tokens None, triggered by: 0.13\n",
      "\u001b[35mJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 769, tokens None, triggered by: 0.10\n",
      "\u001b[31mAn Empirical Analysis of Compute- Optimal Large Language Model Trainingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 770, tokens None, triggered by: 0.25\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 771, tokens None, triggered by: 0.22\n",
      "\u001b[34mAdvances in Neural Information Processing Systems (NeurIPS) 35 (2022), pp. 30016â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 772, tokens None, triggered by: 0.24\n",
      "\u001b[35m30030. 48\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 773, tokens None, triggered by: 0.15\n",
      "\u001b[31m[49] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 774, tokens None, triggered by: 0.15\n",
      "\u001b[32mTransformer Quality in Linear Timeâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 775, tokens None, triggered by: 0.22\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 776, tokens None, triggered by: 0.20\n",
      "\u001b[35mThe Interna- tional Conference on Machine Learning (ICML). PMLR. 2022, pp. 9099â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 777, tokens None, triggered by: 0.15\n",
      "\u001b[31m9117.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 778, tokens None, triggered by: 0.04\n",
      "\u001b[32m[50] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 779, tokens None, triggered by: 0.05\n",
      "\u001b[34mDeep Learning for Time Series Classification:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 780, tokens None, triggered by: 0.27\n",
      "\u001b[35mA Reviewâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 781, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 782, tokens None, triggered by: 0.26\n",
      "\u001b[32mData Mining and Knowledge Discovery 33.4 (2019), pp. 917â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 783, tokens None, triggered by: 0.14\n",
      "\u001b[34m963.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 784, tokens None, triggered by: 0.14\n",
      "\u001b[35m[51] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 785, tokens None, triggered by: 0.23\n",
      "\u001b[31mData Movement is All You Need:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 786, tokens None, triggered by: 0.14\n",
      "\u001b[32mA Case Study on Optimizing Transformersâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 787, tokens None, triggered by: 0.27\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 788, tokens None, triggered by: 0.23\n",
      "\u001b[35mProceedings of Machine Learning and Systems 3 (2021), pp. 711â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 789, tokens None, triggered by: 0.19\n",
      "\u001b[31m732.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 790, tokens None, triggered by: 0.21\n",
      "\u001b[32m[52] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 791, tokens None, triggered by: 0.23\n",
      "\u001b[34mGated Orthogonal Recurrent Units:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 792, tokens None, triggered by: 0.16\n",
      "\u001b[35mOn Learning to Forgetâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 793, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 794, tokens None, triggered by: 0.20\n",
      "\u001b[32mNeural Computation 31.4 (2019), pp. 765â 783. [53] Rudolph Emil Kalman. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 795, tokens None, triggered by: 0.09\n",
      "\u001b[34mA New Approach to Linear Filtering and Prediction Problemsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 796, tokens None, triggered by: 0.22\n",
      "\u001b[35m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 797, tokens None, triggered by: 0.17\n",
      "\u001b[31mIn: (1960).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 798, tokens None, triggered by: 0.15\n",
      "\u001b[32m20\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 799, tokens None, triggered by: 0.13\n",
      "\u001b[34m[54] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 800, tokens None, triggered by: 0.14\n",
      "\u001b[35mTransformers are RNNs: Fast Autoregressive Transformers with Linear Attentionâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 801, tokens None, triggered by: 0.21\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 802, tokens None, triggered by: 0.27\n",
      "\u001b[32mInternational Conference on Machine Learning. PMLR. 2020, pp. 5156â 5165.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 803, tokens None, triggered by: 0.19\n",
      "\u001b[34m[55] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 804, tokens None, triggered by: 0.14\n",
      "\u001b[35mDiffWave: A Versatile Diffusion Model for Audio Synthesisâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 805, tokens None, triggered by: 0.22\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 806, tokens None, triggered by: 0.15\n",
      "\u001b[32mInternational Conference on Learning Representations. 2021.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 807, tokens None, triggered by: 0.13\n",
      "\u001b[34m[56] Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 808, tokens None, triggered by: 0.13\n",
      "\u001b[35mTime-Parameterized Convolutional Neu- ral Networks for Irregularly Sampled Time Seriesâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 809, tokens None, triggered by: 0.19\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 810, tokens None, triggered by: 0.26\n",
      "\u001b[32mIn: arXiv preprint arXiv:2308.03210 (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 811, tokens None, triggered by: 0.09\n",
      "\u001b[34m[57] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. â ImageNet Classification with Deep Convolutional Neural Networksâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 812, tokens None, triggered by: 0.22\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 813, tokens None, triggered by: 0.11\n",
      "\u001b[31mAdvances in Neural Information Processing Systems (NeurIPS) 25 (2012).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 814, tokens None, triggered by: 0.16\n",
      "\u001b[32m[58] Tao Lei. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 815, tokens None, triggered by: 0.28\n",
      "\u001b[34mWhen Attention Meets Fast Recurrence:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 816, tokens None, triggered by: 0.12\n",
      "\u001b[35mTraining Language Models with Reduced Computeâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 817, tokens None, triggered by: 0.26\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 818, tokens None, triggered by: 0.28\n",
      "\u001b[32mProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, pp. 7633â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 819, tokens None, triggered by: 0.15\n",
      "\u001b[34m7648. [59] Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 820, tokens None, triggered by: 0.27\n",
      "\u001b[35mSimple Recurrent Units for Highly Parallelizable Recurrenceâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 821, tokens None, triggered by: 0.20\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 822, tokens None, triggered by: 0.25\n",
      "\u001b[32mIn: arXiv preprint arXiv:1709.02755 (2017).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 823, tokens None, triggered by: 0.08\n",
      "\u001b[34m[60] Mario Lezcano-Casado and David MartÃ nez-Rubio. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 824, tokens None, triggered by: 0.11\n",
      "\u001b[35mCheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Groupâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 825, tokens None, triggered by: 0.21\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 826, tokens None, triggered by: 0.29\n",
      "\u001b[32mThe International Conference on Machine Learning (ICML). 2019.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 827, tokens None, triggered by: 0.21\n",
      "\u001b[34m[61] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 828, tokens None, triggered by: 0.08\n",
      "\u001b[35mWhat Makes Convolutional Models Great on Long Sequence Modeling?â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 829, tokens None, triggered by: 0.23\n",
      "\u001b[31mIn:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 830, tokens None, triggered by: 0.11\n",
      "\u001b[32mThe International Conference on Learning Representations (ICLR). 2023. [62] Vasileios Lioutas and Yuhong Guo. â Time-aware Large Kernel Convolutionsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 831, tokens None, triggered by: 0.26\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 832, tokens None, triggered by: 0.26\n",
      "\u001b[35mThe International Conference\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 833, tokens None, triggered by: 0.25\n",
      "\u001b[31mon Machine Learning (ICML). PMLR. 2020, pp. 6172â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 834, tokens None, triggered by: 0.24\n",
      "\u001b[32m6183.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 835, tokens None, triggered by: 0.16\n",
      "\u001b[34m[63] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behba- hani. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 836, tokens None, triggered by: 0.11\n",
      "\u001b[35mStructured State Space Models for In-Context Reinforcement Learningâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 837, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 838, tokens None, triggered by: 0.11\n",
      "\u001b[32mAdvances in Neural Informa- tion Processing Systems (NeurIPS). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 839, tokens None, triggered by: 0.18\n",
      "\u001b[34m[64] Shahar Lutati, Itamar Zimerman, and Lior Wolf. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 840, tokens None, triggered by: 0.17\n",
      "\u001b[35mFocus Your Attention (with Adaptive IIR Filters)â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 841, tokens None, triggered by: 0.19\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 842, tokens None, triggered by: 0.17\n",
      "\u001b[32mIn: arXiv preprint arXiv:2305.14952 (2023). [65] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 843, tokens None, triggered by: 0.24\n",
      "\u001b[34mMega:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 844, tokens None, triggered by: 0.18\n",
      "\u001b[35mMoving Average Equipped Gated Attentionâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 845, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 846, tokens None, triggered by: 0.10\n",
      "\u001b[32mThe International Conference on Learning Representations (ICLR). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 847, tokens None, triggered by: 0.05\n",
      "\u001b[34m[66] Eric Martin and Chris Cundy. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 848, tokens None, triggered by: 0.08\n",
      "\u001b[35mParallelizing Linear Recurrent Neural Nets Over Sequence Lengthâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 849, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 850, tokens None, triggered by: 0.27\n",
      "\u001b[32mThe International Conference on Learning Representations (ICLR). 2018.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 851, tokens None, triggered by: 0.24\n",
      "\u001b[34m[67] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and Yoshua Bengio. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 852, tokens None, triggered by: 0.13\n",
      "\u001b[35mSampleRNN: An Unconditional End-to-End Neural Audio Generation Modelâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 853, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 854, tokens None, triggered by: 0.22\n",
      "\u001b[32mThe International Conference on Learning Representations (ICLR). 2017.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 855, tokens None, triggered by: 0.17\n",
      "\u001b[34m[68] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 856, tokens None, triggered by: 0.12\n",
      "\u001b[35mLong Range Language Modeling via Gated State Spacesâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 857, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 858, tokens None, triggered by: 0.16\n",
      "\u001b[32mThe International Conference on Learning Representations (ICLR). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 859, tokens None, triggered by: 0.15\n",
      "\u001b[34m[69] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 860, tokens None, triggered by: 0.10\n",
      "\u001b[35mEfficient Orthogonal Parametri- sation of Recurrent Neural Networks using Householder Reflectionsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 861, tokens None, triggered by: 0.21\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 862, tokens None, triggered by: 0.28\n",
      "\u001b[32mInternational Conference on Machine Learning. PMLR. 2017, pp. 2401â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 863, tokens None, triggered by: 0.23\n",
      "\u001b[34m2409.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 864, tokens None, triggered by: 0.22\n",
      "\u001b[35m[70] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 865, tokens None, triggered by: 0.16\n",
      "\u001b[31mS4ND:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 866, tokens None, triggered by: 0.14\n",
      "\u001b[32mModeling Images and Videos as Multidimensional Signals with State Spacesâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 867, tokens None, triggered by: 0.23\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 868, tokens None, triggered by: 0.22\n",
      "\u001b[35mAdvances in Neural Information Processing Systems (NeurIPS). 2022. [71] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Pa- tel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 869, tokens None, triggered by: 0.13\n",
      "\u001b[31mHyenaDNA: Long-range Genomic Sequence Modeling at Single Nucleotide Resolutionâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 870, tokens None, triggered by: 0.23\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 871, tokens None, triggered by: 0.25\n",
      "\u001b[34mAdvances in Neural Information Processing Systems (NeurIPS). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 872, tokens None, triggered by: 0.20\n",
      "\u001b[35m[72] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 873, tokens None, triggered by: 0.12\n",
      "\u001b[31mIn-context Learning and Induction Headsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 874, tokens None, triggered by: 0.20\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 875, tokens None, triggered by: 0.19\n",
      "\u001b[34mTransformer Circuits Thread (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction- heads/index.html.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 876, tokens None, triggered by: 0.24\n",
      "\u001b[35m[73] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalch- brenner, Andrew Senior, and Koray Kavukcuoglu. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 877, tokens None, triggered by: 0.30\n",
      "\u001b[31mWaveNet:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 878, tokens None, triggered by: 0.13\n",
      "\u001b[32mA Generative Model for Raw Audioâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 879, tokens None, triggered by: 0.19\n",
      "\u001b[34m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 880, tokens None, triggered by: 0.12\n",
      "\u001b[35mIn: arXiv preprint arXiv:1609.03499 (2016).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 881, tokens None, triggered by: 0.14\n",
      "\u001b[31m21\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 882, tokens None, triggered by: 0.07\n",
      "\u001b[32m[74] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and So- ham De. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 883, tokens None, triggered by: 0.08\n",
      "\u001b[34mResurrecting Recurrent Neural Networks for Long Sequencesâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 884, tokens None, triggered by: 0.20\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 885, tokens None, triggered by: 0.16\n",
      "\u001b[31mThe International Conference on Machine Learning (ICML). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 886, tokens None, triggered by: 0.20\n",
      "\u001b[32m[75] Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 887, tokens None, triggered by: 0.18\n",
      "\u001b[34mThe LAMBADA Dataset:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 888, tokens None, triggered by: 0.14\n",
      "\u001b[35mWord Prediction Requiring a Broad Discourse Contextâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 889, tokens None, triggered by: 0.26\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 890, tokens None, triggered by: 0.21\n",
      "\u001b[32mProceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 2016, pp. 1525â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 891, tokens None, triggered by: 0.15\n",
      "\u001b[34m1534.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 892, tokens None, triggered by: 0.24\n",
      "\u001b[35m[76] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 893, tokens None, triggered by: 0.12\n",
      "\u001b[31mOn the Difficulty of Training Recurrent Neural Net- worksâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 894, tokens None, triggered by: 0.26\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 895, tokens None, triggered by: 0.24\n",
      "\u001b[34mInternational Conference on Machine Learning. 2013, pp. 1310â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 896, tokens None, triggered by: 0.16\n",
      "\u001b[35m1318.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 897, tokens None, triggered by: 0.14\n",
      "\u001b[31m[77] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 898, tokens None, triggered by: 0.17\n",
      "\u001b[32mRWKV:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 899, tokens None, triggered by: 0.09\n",
      "\u001b[34mReinventing RNNs for the Transformer Eraâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 900, tokens None, triggered by: 0.19\n",
      "\u001b[35m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 901, tokens None, triggered by: 0.24\n",
      "\u001b[31mIn: arXiv preprint arXiv:2305.13048 (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 902, tokens None, triggered by: 0.19\n",
      "\u001b[32m[78] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 903, tokens None, triggered by: 0.15\n",
      "\u001b[34mRandom Feature Attentionâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 904, tokens None, triggered by: 0.23\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 905, tokens None, triggered by: 0.26\n",
      "\u001b[31mThe International Conference on Learning Representations (ICLR). 2021.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 906, tokens None, triggered by: 0.13\n",
      "\u001b[32m[79] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 907, tokens None, triggered by: 0.13\n",
      "\u001b[34mHyena Hierarchy:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 908, tokens None, triggered by: 0.08\n",
      "\u001b[35mTowards Larger Convolutional Language Modelsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 909, tokens None, triggered by: 0.20\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 910, tokens None, triggered by: 0.14\n",
      "\u001b[32mThe International Conference on Machine Learning (ICML). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 911, tokens None, triggered by: 0.15\n",
      "\u001b[34m[80] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 912, tokens None, triggered by: 0.12\n",
      "\u001b[35mToeplitz Neural Network for Sequence Modelingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 913, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 914, tokens None, triggered by: 0.19\n",
      "\u001b[32mThe International Conference on Learning Representations (ICLR). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 915, tokens None, triggered by: 0.10\n",
      "\u001b[34m[81] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 916, tokens None, triggered by: 0.17\n",
      "\u001b[35mThe devil in linear transformerâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 917, tokens None, triggered by: 0.18\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 918, tokens None, triggered by: 0.29\n",
      "\u001b[32mIn: arXiv preprint arXiv:2210.10340 (2022).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 919, tokens None, triggered by: 0.22\n",
      "\u001b[34m[82] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 920, tokens None, triggered by: 0.22\n",
      "\u001b[35mCosFormer:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 921, tokens None, triggered by: 0.11\n",
      "\u001b[31mRethinking Softmax in Attentionâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 922, tokens None, triggered by: 0.24\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 923, tokens None, triggered by: 0.20\n",
      "\u001b[34mThe International Conference on Learning Representations (ICLR). 2022.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 924, tokens None, triggered by: 0.12\n",
      "\u001b[35m[83] Ali Rahimi and Benjamin Recht. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 925, tokens None, triggered by: 0.12\n",
      "\u001b[31mRandom features for large-scale kernel machinesâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 926, tokens None, triggered by: 0.25\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 927, tokens None, triggered by: 0.15\n",
      "\u001b[34mAdvances in neural information processing systems 20 (2007). [84] Prajit Ramachandran, Barret Zoph, and Quoc V Le. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 928, tokens None, triggered by: 0.17\n",
      "\u001b[35mSwish:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 929, tokens None, triggered by: 0.15\n",
      "\u001b[31mA Self-gated Activation Functionâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 930, tokens None, triggered by: 0.21\n",
      "\u001b[32m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 931, tokens None, triggered by: 0.21\n",
      "\u001b[34mIn: arXiv preprint arXiv:1710.05941 7.1 (2017), p. 5. [85] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 932, tokens None, triggered by: 0.16\n",
      "\u001b[35mCKConv: Con- tinuous Kernel Convolution For Sequential Dataâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 933, tokens None, triggered by: 0.18\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 934, tokens None, triggered by: 0.26\n",
      "\u001b[32mIn: arXiv preprint arXiv:2102.02611 (2021).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 935, tokens None, triggered by: 0.08\n",
      "\u001b[34m[86] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 936, tokens None, triggered by: 0.27\n",
      "\u001b[35mWinogrande:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 937, tokens None, triggered by: 0.17\n",
      "\u001b[31mAn Adversarial Wino- grad Schema Challenge at Scaleâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 938, tokens None, triggered by: 0.26\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 939, tokens None, triggered by: 0.25\n",
      "\u001b[34mCommunications of the ACM 64.9 (2021), pp. 99â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 940, tokens None, triggered by: 0.27\n",
      "\u001b[35m106. [87] George Saon, Ankit Gupta, and Xiaodong Cui. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 941, tokens None, triggered by: 0.13\n",
      "\u001b[31mDiagonal State Space Augmented Transformers for Speech Recognitionâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 942, tokens None, triggered by: 0.22\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 943, tokens None, triggered by: 0.23\n",
      "\u001b[34mICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2023, pp. 1â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 944, tokens None, triggered by: 0.14\n",
      "\u001b[35m5.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 945, tokens None, triggered by: 0.14\n",
      "\u001b[31mImanol Schlag, Kazuki Irie, and JÃ¼rgen Schmidhuber. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 946, tokens None, triggered by: 0.20\n",
      "\u001b[32mLinear Transformers are Secretly Fast Weight Program- mersâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 947, tokens None, triggered by: 0.18\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 948, tokens None, triggered by: 0.14\n",
      "\u001b[35mThe International Conference on Machine Learning (ICML). PMLR. 2021, pp. 9355â 9366. [89] Noam Shazeer. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 949, tokens None, triggered by: 0.12\n",
      "\u001b[31mGLU Variants Improve Transformerâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 950, tokens None, triggered by: 0.18\n",
      "\u001b[32m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 951, tokens None, triggered by: 0.26\n",
      "\u001b[34mIn: arXiv preprint arXiv:2002.05202 (2020). [90] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael SchÃ¤rli, and Denny Zhou. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 952, tokens None, triggered by: 0.12\n",
      "\u001b[35mLarge Language Models can be Easily Distracted by Irrelevant Contextâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 953, tokens None, triggered by: 0.18\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 954, tokens None, triggered by: 0.13\n",
      "\u001b[32mThe International Conference on Machine Learning (ICML). PMLR. 2023, pp. 31210â 31227.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 955, tokens None, triggered by: 0.14\n",
      "\u001b[34mJiaxin Shi, Ke Alexander Wang, and Emily Fox. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 956, tokens None, triggered by: 0.17\n",
      "\u001b[35mSequence Modeling with Multiresolution Convolutional Mem- oryâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 957, tokens None, triggered by: 0.18\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 958, tokens None, triggered by: 0.15\n",
      "\u001b[32mThe International Conference on Machine Learning (ICML). PMLR. 2023, pp. 31312â 31327.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 959, tokens None, triggered by: 0.14\n",
      "\u001b[34mJimmy TH Smith, Andrew Warrington, and Scott W Linderman. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 960, tokens None, triggered by: 0.16\n",
      "\u001b[35mSimplified State Space Layers for Sequence Modelingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 961, tokens None, triggered by: 0.23\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 962, tokens None, triggered by: 0.23\n",
      "\u001b[32mThe International Conference on Learning Representations (ICLR). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 963, tokens None, triggered by: 0.14\n",
      "\u001b[34mJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 964, tokens None, triggered by: 0.17\n",
      "\u001b[35mRoformer: Enhanced Trans- former with Rotary Position Embeddingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 965, tokens None, triggered by: 0.18\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 966, tokens None, triggered by: 0.25\n",
      "\u001b[32mIn: arXiv preprint arXiv:2104.09864 (2021).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 967, tokens None, triggered by: 0.09\n",
      "\u001b[34m[93] [94] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 968, tokens None, triggered by: 0.19\n",
      "\u001b[35mRetentive network:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 969, tokens None, triggered by: 0.11\n",
      "\u001b[31mA successor to transformer for large language modelsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 970, tokens None, triggered by: 0.18\n",
      "\u001b[32m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 971, tokens None, triggered by: 0.24\n",
      "\u001b[34mIn: arXiv preprint arXiv:2307.08621 (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 972, tokens None, triggered by: 0.30\n",
      "\u001b[35mIlya Sutskever, Oriol Vinyals, and Quoc V Le. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 973, tokens None, triggered by: 0.09\n",
      "\u001b[31mSequence to Sequence Learning with Neural Networksâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 974, tokens None, triggered by: 0.23\n",
      "\u001b[32m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 975, tokens None, triggered by: 0.15\n",
      "\u001b[34mAdvances in Neural Information Processing Systems (NeurIPS) 27 (2014).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 976, tokens None, triggered by: 0.19\n",
      "\u001b[35m22\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 977, tokens None, triggered by: 0.12\n",
      "\u001b[31m[96] Corentin Tallec and Yann Ollivier. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 978, tokens None, triggered by: 0.09\n",
      "\u001b[32mCan Recurrent Neural Networks Warp Time?â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 979, tokens None, triggered by: 0.24\n",
      "\u001b[34mIn:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 980, tokens None, triggered by: 0.25\n",
      "\u001b[35mThe International Con- ference on Learning Representations (ICLR). 2018.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 981, tokens None, triggered by: 0.12\n",
      "\u001b[31m[97] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Se- bastian Ruder, and Donald Metzler. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 982, tokens None, triggered by: 0.20\n",
      "\u001b[32mLong Range Arena:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 983, tokens None, triggered by: 0.14\n",
      "\u001b[34mA Benchmark for Efficient Transformersâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 984, tokens None, triggered by: 0.24\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 985, tokens None, triggered by: 0.19\n",
      "\u001b[31mInter- national Conference on Learning Representations (ICLR). 2021.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 986, tokens None, triggered by: 0.14\n",
      "\u001b[32m[98] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 987, tokens None, triggered by: 0.15\n",
      "\u001b[34mEfficient Transformers:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 988, tokens None, triggered by: 0.27\n",
      "\u001b[35mA Surveyâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 989, tokens None, triggered by: 0.24\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 990, tokens None, triggered by: 0.28\n",
      "\u001b[32mACM Com- puting Surveys 55.6 (2022), pp. 1â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 991, tokens None, triggered by: 0.25\n",
      "\u001b[34m28.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 992, tokens None, triggered by: 0.11\n",
      "\u001b[35m[99] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Bap- tiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 993, tokens None, triggered by: 0.18\n",
      "\u001b[31mLlama:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 994, tokens None, triggered by: 0.14\n",
      "\u001b[32mOpen and Efficient Foundation Language Modelsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 995, tokens None, triggered by: 0.19\n",
      "\u001b[34m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 996, tokens None, triggered by: 0.27\n",
      "\u001b[35mIn: arXiv preprint arXiv:2302.13971 (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 997, tokens None, triggered by: 0.15\n",
      "\u001b[31m[100] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 998, tokens None, triggered by: 0.24\n",
      "\u001b[32mAttention Is All You Needâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 999, tokens None, triggered by: 0.24\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1000, tokens None, triggered by: 0.23\n",
      "\u001b[35mAdvances in Neural Information Processing Systems (NeurIPS). 2017.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1001, tokens None, triggered by: 0.20\n",
      "\u001b[31m[101] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1002, tokens None, triggered by: 0.15\n",
      "\u001b[32mOn Orthogonality and Learning Recur- rent Networks with Long Term Dependenciesâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1003, tokens None, triggered by: 0.21\n",
      "\u001b[34m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1004, tokens None, triggered by: 0.27\n",
      "\u001b[35mInternational Conference on Machine Learning. PMLR. 2017, pp. 3570â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1005, tokens None, triggered by: 0.21\n",
      "\u001b[31m3578.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1006, tokens None, triggered by: 0.14\n",
      "\u001b[32mJue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1007, tokens None, triggered by: 0.12\n",
      "\u001b[34mSelective Structured State-Spaces for Long-form Video Understandingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1008, tokens None, triggered by: 0.22\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1009, tokens None, triggered by: 0.21\n",
      "\u001b[31mProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 6387â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1010, tokens None, triggered by: 0.19\n",
      "\u001b[32m6397. [102] [103] Pete Warden. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1011, tokens None, triggered by: 0.13\n",
      "\u001b[34mSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognitionâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1012, tokens None, triggered by: 0.27\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1013, tokens None, triggered by: 0.18\n",
      "\u001b[31mArXiv abs/1804.03209 (2018).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1014, tokens None, triggered by: 0.11\n",
      "\u001b[32m[104] Samuel Williams, Andrew Waterman, and David Patterson. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1015, tokens None, triggered by: 0.12\n",
      "\u001b[34mRoofline:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1016, tokens None, triggered by: 0.17\n",
      "\u001b[35mAn Insightful Visual Performance Model for Multicore Architecturesâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1017, tokens None, triggered by: 0.27\n",
      "\u001b[31m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1018, tokens None, triggered by: 0.20\n",
      "\u001b[32mCommunications of the ACM 52.4 (2009), pp. 65â 76. [105] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1019, tokens None, triggered by: 0.14\n",
      "\u001b[34mCondConv: Conditionally Parameterized Con- volutions for Efficient Inferenceâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1020, tokens None, triggered by: 0.24\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1021, tokens None, triggered by: 0.10\n",
      "\u001b[31mAdvances in Neural Information Processing Systems (NeurIPS) 32 (2019). [106] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1022, tokens None, triggered by: 0.14\n",
      "\u001b[32mHellaSwag:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1023, tokens None, triggered by: 0.17\n",
      "\u001b[34mCan a Machine Really Finish Your Sentence?â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1024, tokens None, triggered by: 0.26\n",
      "\u001b[35mIn:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1025, tokens None, triggered by: 0.30\n",
      "\u001b[31mProceedings of the 57th Annual Meeting of the Association for Computational Linguis- tics. 2019.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1026, tokens None, triggered by: 0.19\n",
      "\u001b[32m[107] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1027, tokens None, triggered by: 0.21\n",
      "\u001b[34mAn Attention Free Transformerâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1028, tokens None, triggered by: 0.18\n",
      "\u001b[35m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1029, tokens None, triggered by: 0.25\n",
      "\u001b[31mIn: arXiv preprint arXiv:2105.14103 (2021).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1030, tokens None, triggered by: 0.17\n",
      "\u001b[32m[108] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher RÃ©. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1031, tokens None, triggered by: 0.15\n",
      "\u001b[34mEffectively Modeling Time Series with Simple Discrete State Spacesâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1032, tokens None, triggered by: 0.23\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1033, tokens None, triggered by: 0.20\n",
      "\u001b[31mThe International Conference on Learning Representations (ICLR). 2023.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1034, tokens None, triggered by: 0.21\n",
      "\u001b[32m[109] Lin Zheng, Chong Wang, and Lingpeng Kong. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1035, tokens None, triggered by: 0.16\n",
      "\u001b[34mLinear complexity randomized self-attention mechanismâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1036, tokens None, triggered by: 0.21\n",
      "\u001b[35m. In:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1037, tokens None, triggered by: 0.28\n",
      "\u001b[31mInternational Conference on Machine Learning. PMLR. 2022, pp. 27011â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1038, tokens None, triggered by: 0.28\n",
      "\u001b[32m27041.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1039, tokens None, triggered by: 0.21\n",
      "\u001b[34m[110] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1040, tokens None, triggered by: 0.15\n",
      "\u001b[35mEfficient Long Sequence Modeling via State Space Augmented Transformerâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1041, tokens None, triggered by: 0.16\n",
      "\u001b[31m.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1042, tokens None, triggered by: 0.14\n",
      "\u001b[32mIn: arXiv preprint arXiv:2212.08136 (2022).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1043, tokens None, triggered by: 0.15\n",
      "\u001b[34m23\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1044, tokens None, triggered by: 0.23\n",
      "\u001b[35m# A Discussion:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1045, tokens None, triggered by: 0.26\n",
      "\u001b[31mSelection Mechanism Our selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1046, tokens None, triggered by: 0.10\n",
      "\u001b[32mIt can also be viewed as related to â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1047, tokens None, triggered by: 0.20\n",
      "\u001b[34mfast weightsâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1048, tokens None, triggered by: 0.14\n",
      "\u001b[35m(J.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1049, tokens None, triggered by: 0.16\n",
      "\u001b[31mBa et al. 2016), which connects classical RNNs with the mechanism of linear attention (Schlag, Irie, and Schmidhuber 2021).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1050, tokens None, triggered by: 0.16\n",
      "\u001b[32mHowever, we believe that it is a distinct concept that is worth clarifying.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1051, tokens None, triggered by: 0.24\n",
      "\u001b[34mGating. Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and Schmidhuber 1997) and GRU (J. Chung et al. 2014), or the gated equation (5)n Theorem 1. This was interpreted as a particular mechanism for controlling whether to let an input into the hidden state of an RNN.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1052, tokens None, triggered by: 0.20\n",
      "\u001b[35mIn particular, this aï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1053, tokens None, triggered by: 0.25\n",
      "\u001b[31mects the propagation of signal through time and causes inputs to interact along the sequence length dimension. However, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction (often with an activation function). For example, elementwise multiplicative components of neural network architectures (that do not interact along sequence length) are now commonly referred to as gated architectures (Hua et al. 2022; Mehta et al. 2023), despite a very diï¬ erent meaning than the original RNN sense. Thus we believe the original concept of RNN gating versus the popular usage of multiplicative gating actually have a very diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1054, tokens None, triggered by: 0.25\n",
      "\u001b[32merent semantic meaning.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1055, tokens None, triggered by: 0.12\n",
      "\u001b[34mHypernetworks. Hypernetworks refer to neural networks whose parameters are themselves generated by smaller neural networks.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1056, tokens None, triggered by: 0.12\n",
      "\u001b[35mThe original idea (Ha, Dai, and Quoc V.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1057, tokens None, triggered by: 0.13\n",
      "\u001b[31mLe 2017) used it in a narrow sense to deï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1058, tokens None, triggered by: 0.11\n",
      "\u001b[32mne a large RNN whose recurrent parameters are generated by a smaller RNN.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1059, tokens None, triggered by: 0.22\n",
      "\u001b[34mData-dependence. Similar to hypernetworks, data-dependence can refer to any notion where some parameters of the model depend on the data (Poli et al. 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1060, tokens None, triggered by: 0.18\n",
      "\u001b[35mExample:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1061, tokens None, triggered by: 0.18\n",
      "\u001b[31mGLU Activation.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1062, tokens None, triggered by: 0.27\n",
      "\u001b[32mTo illustrate the issues with these concepts, consider a simple diagonal linear layer ð ¦ = Dð ¥, where D is a diagonal weight parameter. Now suppose that D is itself generated from a linear transformation of ð ¥, with an optional nonlinearity:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1063, tokens None, triggered by: 0.29\n",
      "\u001b[34mD = ð (W ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1064, tokens None, triggered by: 0.29\n",
      "\u001b[35m¥).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1065, tokens None, triggered by: 0.27\n",
      "\u001b[31mSince it is diagonal, the multiplication becomes an elementwise product: ð ¦ = ð (W ð ¥)â ¦ð ¥.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1066, tokens None, triggered by: 0.28\n",
      "\u001b[32mThis is a rather trivial transformation, yet it technically satisï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1067, tokens None, triggered by: 0.24\n",
      "\u001b[34mes the common meanings of gating (since it has a multiplicative â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1068, tokens None, triggered by: 0.15\n",
      "\u001b[35mbranchâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1069, tokens None, triggered by: 0.24\n",
      "\u001b[31m), hypernetworks (since the parameter D is generated by another layer), and data-dependent (since D depends on the data ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1070, tokens None, triggered by: 0.27\n",
      "\u001b[32m¥).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1071, tokens None, triggered by: 0.26\n",
      "\u001b[34mHowever, this in fact simply deï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1072, tokens None, triggered by: 0.14\n",
      "\u001b[35mnes a GLU function, which is so simple that it is often considered just an activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1073, tokens None, triggered by: 0.18\n",
      "\u001b[31mSelection. Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating, hypernetworks, or data-dependence, so can an enormous range of other constructionsâ essentially anything with a multiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) as wellâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1074, tokens None, triggered by: 0.26\n",
      "\u001b[32mand we ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1075, tokens None, triggered by: 0.23\n",
      "\u001b[34mnd it uninformative to think of them as such.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1076, tokens None, triggered by: 0.22\n",
      "\u001b[35mInstead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case (Theorem 1) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization of â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1077, tokens None, triggered by: 0.15\n",
      "\u001b[31m(Funahashi and Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1078, tokens None, triggered by: 0.27\n",
      "\u001b[32mWe also eschew the term â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1079, tokens None, triggered by: 0.21\n",
      "\u001b[34mgatingâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1080, tokens None, triggered by: 0.06\n",
      "\u001b[35min favor of selection to clarify the overloaded use of former. More narrowly, we use selection to refer to the mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence length (Section 3.1). Beyond selective SSMs and gated RNNs, other examples may include input-dependent convolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and Guo 2020; Lutati, Zimerman, and Wolf 2023; Yang et al. 2019) and even attention.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1081, tokens None, triggered by: 0.09\n",
      "\u001b[31m24\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1082, tokens None, triggered by: 0.29\n",
      "\u001b[32m# B Related Work We overview several prior works related to our methods. We mention that some of the most closely related models include recurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet, and RWKV.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1083, tokens None, triggered by: 0.29\n",
      "\u001b[34m# B.1 S4 Variants and Derivatives\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1084, tokens None, triggered by: 0.06\n",
      "\u001b[35mWe describe a brief overview of some structured SSMs from past work, particularly those that have a relation to our method.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1085, tokens None, triggered by: 0.29\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1086, tokens None, triggered by: 0.17\n",
      "\u001b[32m¢ S4 (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021) introduced the ï¬ rst structured SSM, describing diagonal structure and diagonal plus low-rank (DPLR).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1087, tokens None, triggered by: 0.16\n",
      "\u001b[34mIt focused on eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1088, tokens None, triggered by: 0.12\n",
      "\u001b[35mcient convolutional algorithms for DPLR SSMs due to a connection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1089, tokens None, triggered by: 0.26\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1090, tokens None, triggered by: 0.25\n",
      "\u001b[32m¢ DSS (Gupta, Gu, and Berant 2022) ï¬ rst discovered the empirical eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1091, tokens None, triggered by: 0.11\n",
      "\u001b[34mectiveness of diagonal structured SSMs by approximating the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1092, tokens None, triggered by: 0.21\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1093, tokens None, triggered by: 0.16\n",
      "\u001b[31m¢ S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is the ï¬ rst S4 model to be computed recurrently with the parallel scan.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1094, tokens None, triggered by: 0.26\n",
      "\u001b[32mHowever, this required lowering the eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1095, tokens None, triggered by: 0.28\n",
      "\u001b[34mective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) to MIMO (multi-input multi-output) formulation.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1096, tokens None, triggered by: 0.28\n",
      "\u001b[35mOur proposed S6 shares the scan, but diï¬ ers by (i) keeping the SISO dimensions, which provides a larger eï¬ ective recurrent state, (ii) using a hardware-aware algorithm to overcome the computation issue, (iii) adding the selection mechanism. Lu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1097, tokens None, triggered by: 0.09\n",
      "\u001b[31mTheir mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where A is manually set to 0, instead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically to this setting and probe if the model has learned to automatically reset its state on episode boundaries.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1098, tokens None, triggered by: 0.28\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1099, tokens None, triggered by: 0.11\n",
      "\u001b[34m¢ Mega (Ma et al. 2023) introduced a simpliï¬ cation of S4 to be real- instead of complex- valued, giving it an interpretation of being an exponential moving average (EMA). They additionally make an interesting connection of the discretization step of SSMs to an EMA damping term.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1100, tokens None, triggered by: 0.24\n",
      "\u001b[35mContrary to ï¬ ndings in the original S4 papers, this was the ï¬ rst model to show that real-valued SSMs are empirically eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1101, tokens None, triggered by: 0.26\n",
      "\u001b[31mective in certain settings or when combined with diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1102, tokens None, triggered by: 0.22\n",
      "\u001b[32merent architectural components.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1103, tokens None, triggered by: 0.19\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1104, tokens None, triggered by: 0.13\n",
      "\u001b[35m¢ Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this perspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally and close to LTI.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1105, tokens None, triggered by: 0.20\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1106, tokens None, triggered by: 0.25\n",
      "\u001b[32m¢ SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1107, tokens None, triggered by: 0.19\n",
      "\u001b[34mShi, K. A.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1108, tokens None, triggered by: 0.20\n",
      "\u001b[35mWang, and Fox 2023), and Toeplitz Neural Network (Qin, Han, W. Sun, He, et al. 2023) all focus on the convolutional representation of S4 and create global or long convolution kernels with diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1109, tokens None, triggered by: 0.30\n",
      "\u001b[31merent parameterizations.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1110, tokens None, triggered by: 0.10\n",
      "\u001b[32mHowever, these methods cannot do fast autoregressive inference directly. Notably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually strictly LTI (linear time invariant). # B.2 SSM Architectures We use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating one of the previous SSMs as a black box layer.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1111, tokens None, triggered by: 0.24\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1112, tokens None, triggered by: 0.05\n",
      "\u001b[35m¢ GSS (Mehta et al. 2023) was the ï¬ rst gated neural network architecture incorporating SSMs. It is motivated by the gated attention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most importantly, its projection contracts the model dimension to reduce the state size of the SSM, while ours expands the model dimension in order to increase the state size, based on the motivation in Section 3.1.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1113, tokens None, triggered by: 0.24\n",
      "\u001b[31m25 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1114, tokens None, triggered by: 0.24\n",
      "\u001b[32m¢ Mega (Ma et al. 2023) combined the EMA simpliï¬ cation of S4 described above into a hybrid architecture using an eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1115, tokens None, triggered by: 0.22\n",
      "\u001b[34mcient attention approximation.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1116, tokens None, triggered by: 0.27\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1117, tokens None, triggered by: 0.17\n",
      "\u001b[31m¢ H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1118, tokens None, triggered by: 0.10\n",
      "\u001b[32mIt is the ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1119, tokens None, triggered by: 0.12\n",
      "\u001b[34mrst to generalize this formulation of linear attention to more general recurrences, which is also the basis of later architectures.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1120, tokens None, triggered by: 0.24\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1121, tokens None, triggered by: 0.12\n",
      "\u001b[31m¢ Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied on the input.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1122, tokens None, triggered by: 0.14\n",
      "\u001b[32mWhile sharing the â selectionâ name, we consider this an architectural modiï¬ cation that is closer to architectural gating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective Copying task because simply masking out the irrelevant inputs does not aï¬ ect the spacing between the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1123, tokens None, triggered by: 0.23\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1124, tokens None, triggered by: 0.21\n",
      "\u001b[35m¢ RetNet (Y.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1125, tokens None, triggered by: 0.15\n",
      "\u001b[31mSun et al. 2023) is also based on Linear Attention and very similar to H3, but reduces the inner S4 layer to a special case where the state dimension is ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1126, tokens None, triggered by: 0.24\n",
      "\u001b[32m= 1.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1127, tokens None, triggered by: 0.29\n",
      "\u001b[34mAlthough not framed as such, its recurrence can be viewed as a special case of a linear SSM.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1128, tokens None, triggered by: 0.08\n",
      "\u001b[35mIts primary source of improvement is using a linear attention with large head dimension, which can be viewed as another method to perform input-dependent state expansion. Using a larger head dimension in the context of linear attention variants was ï¬ rst done by H3, but not extensively used since this requires a proportional amount of extra computation. RetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention instead of convolutions, made feasible by their particular special case of SSMs which acts as a simple EMA.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1129, tokens None, triggered by: 0.24\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1130, tokens None, triggered by: 0.17\n",
      "\u001b[32m¢ RWKV (B.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1131, tokens None, triggered by: 0.29\n",
      "\u001b[34mPeng et al. 2023) is another recent RNN designed for language modeling.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1132, tokens None, triggered by: 0.19\n",
      "\u001b[35mIt is based on AFT (attention-free Transformer (S. Zhai et al. 2021)), another variant of linear attention.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1133, tokens None, triggered by: 0.09\n",
      "\u001b[31mIts main â WKVâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1134, tokens None, triggered by: 0.19\n",
      "\u001b[32mmechanism involves LTI recurrences and can be seen as the ratio of two SSMs.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1135, tokens None, triggered by: 0.14\n",
      "\u001b[34mWe also highlight the gated attention unit (GAU) from Hua et al. (2022), which was motivated by combining the Transformerâ s MHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining the H3 and MLP blocks. # B.3 Relationship to RNNs RNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state. Several older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016), and simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without time-wise nonlinearities. Because of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs, and are thus more powerful in a sense than the family of LTI structured SSMs above.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1136, tokens None, triggered by: 0.20\n",
      "\u001b[35mThe main diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1137, tokens None, triggered by: 0.19\n",
      "\u001b[31merences are:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1138, tokens None, triggered by: 0.27\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1139, tokens None, triggered by: 0.22\n",
      "\u001b[34m¢ They do not use state expansion (ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1140, tokens None, triggered by: 0.18\n",
      "\u001b[35m= 1) or selective B, C parameters, both of which are important for performance (Section 4.6).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1141, tokens None, triggered by: 0.12\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1142, tokens None, triggered by: 0.28\n",
      "\u001b[32m¢ They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism + discretization (Theorem 1). The connections to principled SSM theory provides better parameterizations and initializations (Section 3.6).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1143, tokens None, triggered by: 0.20\n",
      "\u001b[34mAdditionally, older RNNs famously suï¬ ered from eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1144, tokens None, triggered by: 0.14\n",
      "\u001b[35mciency issues and the vanishing gradients problem (Pascanu, Mikolov, and Bengio 2013), both caused by their sequential nature. The latter could be solved for some of the above RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the former was diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1145, tokens None, triggered by: 0.09\n",
      "\u001b[31mcult without theory later developed for SSMs. For example, modern structured SSMs diï¬ er in more careful parameterization of the recurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson, Goel, et al. 2021; Gu, Johnson, Timalsina, et al. 2023)), or direct analysis (Orvieto et al. 2023)). We also note that there is a long line of work on orthogonal RNNs (Arjovsky, Shah, and Bengio 2016; Henaï¬ , Szlam, and LeCun 2016; Lezcano-Casado and MartÃ nez-Rubio 2019; Mhammedi et al. 2017; Vorontsov et al. 2017)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1146, tokens None, triggered by: 0.05\n",
      "\u001b[32m26\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1147, tokens None, triggered by: 0.29\n",
      "\u001b[34mwhich are motivated by constraining the A transition matrix to be orthogonal or unitary, in order to control its eigenvalues and prevent the vanishing gradient problem. However, these had other limitations; we believe that these stem from the fact that orthogonal/unitary RNNs are also LTI.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1148, tokens None, triggered by: 0.25\n",
      "\u001b[35mFor example, they are almost always evaluated on the Copying task which they can solve perfectly, but observed to struggle on the Selective Copying task (Jing et al. 2019).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1149, tokens None, triggered by: 0.26\n",
      "\u001b[31m# B.4 Linear Attention The Linear Attention (LA) (Katharopoulos et al. 2020) framework is an important result popularizing kernel attention and showing how it relates to recurrent autoregressive models.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1150, tokens None, triggered by: 0.19\n",
      "\u001b[32mMany variants have proposed alternative kernels and other modiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1151, tokens None, triggered by: 0.15\n",
      "\u001b[34mcations.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1152, tokens None, triggered by: 0.15\n",
      "\u001b[35mRandom Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature map to approximate softmax attention (i.e. the exp feature map) using the random Fourier feature approximation of Gaussian kernels (Rahimi and Recht 2007).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1153, tokens None, triggered by: 0.12\n",
      "\u001b[31mPerformer (Choromanski et al. 2021) ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1154, tokens None, triggered by: 0.18\n",
      "\u001b[32mnds an approximation to the exponential kernel involving only positive features, which also allows the softmax normalization term.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1155, tokens None, triggered by: 0.28\n",
      "\u001b[34mTransNormer (Qin, Han, W.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1156, tokens None, triggered by: 0.20\n",
      "\u001b[35mSun, D.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1157, tokens None, triggered by: 0.20\n",
      "\u001b[31mLi, et al. 2022) showed that the LA denominator term can be unstable and proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al. 2022) augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality. Linear Randomized Attention (Zheng, C. Wang, and L.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1158, tokens None, triggered by: 0.27\n",
      "\u001b[32mKong 2022) generalize RFA from the perspective of importance sampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformed numerator).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1159, tokens None, triggered by: 0.22\n",
      "\u001b[34mAside from kernel attention, many other variants of eï¬ cient attention exist; the survey Tay, Dehghani, Bahri, et al. (2022) oï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1160, tokens None, triggered by: 0.30\n",
      "\u001b[35mers an extensive categorization of many of these. # B.5 Long Context Models Long context has become a popular subject, and several recent models have claimed to scale to longer and longer sequences.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1161, tokens None, triggered by: 0.27\n",
      "\u001b[31mHowever, these are often from a computational standpoint and have not been extensively validated.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1162, tokens None, triggered by: 0.20\n",
      "\u001b[32mThese include:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1163, tokens None, triggered by: 0.09\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1164, tokens None, triggered by: 0.08\n",
      "\u001b[35m¢ Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023), a lightweight wrapper around a Transformer backbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization tasks; their main result is similar to our Induction Heads extrapolation experiment (Table 2).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1165, tokens None, triggered by: 0.18\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1166, tokens None, triggered by: 0.25\n",
      "\u001b[32m¢ LongNet (Ding et al. 2023), which claimed to scale to 1B length but only evaluated on length < 100ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1167, tokens None, triggered by: 0.28\n",
      "\u001b[34m¾ for actual tasks.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1168, tokens None, triggered by: 0.17\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1169, tokens None, triggered by: 0.10\n",
      "\u001b[31m¢ Hyena and HyenaDNA (Nguyen, Poli, et al. 2023; Poli et al. 2023), which claimed to leverage up to 1M context. However, their experiments trained on proportionally more data at longer contexts, making it hard to conclude if quality improvements at 1M context are due to context length or due to more data and computation.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1170, tokens None, triggered by: 0.15\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1171, tokens None, triggered by: 0.20\n",
      "\u001b[34m¢ Sparse Transformer (Child et al. 2019) showed a proof-of-concept of using a strided sparse attention Transformer to model audio waveforms of length 220 = 1048576, although did not discuss performance tradeoï¬ s when controlling for computation and model size.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1172, tokens None, triggered by: 0.26\n",
      "\u001b[35mIn contrast, we believe this work presents one of the ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1173, tokens None, triggered by: 0.24\n",
      "\u001b[31mrst approaches to meaningfully demonstrate increasing performance with longer context.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1174, tokens None, triggered by: 0.27\n",
      "\u001b[32m# C Mechanics of Selective SSMs\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1175, tokens None, triggered by: 0.18\n",
      "\u001b[34mProof of Theorem 1. Consider a selective SSM (Algorithm 2) with ð = 1, A = â 1, B = 1, ð â = ð «ð ð ð ¾ð ºð (ð ¥), ð â = ð ð ð ¿ð ð ð ð ð .\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1176, tokens None, triggered by: 0.20\n",
      "\u001b[35mThe corresponding continuous-time SSM (1) is\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1177, tokens None, triggered by: 0.19\n",
      "\u001b[31mâ (ð ¡) = â â (ð ¡) + ð ¥(ð ¡)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1178, tokens None, triggered by: 0.12\n",
      "\u001b[32mwhich is also called a leaky integrator.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1179, tokens None, triggered by: 0.12\n",
      "\u001b[34m27\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1180, tokens None, triggered by: 0.22\n",
      "\u001b[35mThe discretization step size is The discretization step size is\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1181, tokens None, triggered by: 0.30\n",
      "\u001b[31m# â ð ¡ = ð â (ð ¯ð ºð ð ºð ð ¾ð ð ¾ð + ð â (ð ¥ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1182, tokens None, triggered by: 0.30\n",
      "\u001b[32m¡)) = ð ð ð ¿ð ð ð ð ð (ð ¯ð ºð ð ºð ð ¾ð ð ¾ð + ð «ð ð ð ¾ð ºð (ð ¥ð ¡)) = ð ð ð ¿ð ð ð ð ð (ð «ð ð ð ¾ð ºð (ð ¥ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1183, tokens None, triggered by: 0.11\n",
      "\u001b[34m¡))\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1184, tokens None, triggered by: 0.21\n",
      "\u001b[35mwhere we observe that the parameter can be viewed as a learnable bias and folded into the linear projection.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1185, tokens None, triggered by: 0.06\n",
      "\u001b[31mNow applying the zero-order hold (ZOH) discretization formulas:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1186, tokens None, triggered by: 0.28\n",
      "\u001b[32mAð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1187, tokens None, triggered by: 0.25\n",
      "\u001b[34m¡ = exp(â A) = 1 1 + exp(ð «ð ð ð ¾ð ºð (ð ¥ð ¡) = ð (â ð «ð ð ð ¾ð ºð (ð ¥ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1188, tokens None, triggered by: 0.28\n",
      "\u001b[35m¡)) = 1 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1189, tokens None, triggered by: 0.22\n",
      "\u001b[31mð (ð «ð ð ð ¾ð ºð (ð ¥ð ¡)) Bð ¡ = (â A)â 1(exp(â A) â I) â â B = â (exp(â A) â I) = 1 â A = ð (ð «ð ð ð ¾ð ºð (ð ¥ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1190, tokens None, triggered by: 0.21\n",
      "\u001b[32m¡)). Thus the final discrete recurrence (2a) is\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1191, tokens None, triggered by: 0.28\n",
      "\u001b[34mð ð ¡ = ð (ð «ð ð ð ¾ð ºð (ð ¥ð ¡)) â ð ¡ = (1 â ð ð ¡)â ð ¡â 1 + ð ð ¡ð ¥ð ¡\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1192, tokens None, triggered by: 0.21\n",
      "\u001b[35mas desired.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1193, tokens None, triggered by: 0.25\n",
      "\u001b[31m# D Hardware-aware Algorithm For Selective SSMs Without input-dependent selectivity, SSMs can be eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1194, tokens None, triggered by: 0.26\n",
      "\u001b[32mciently implemented as a convolution (Dao, Fu, Saab, et al. 2023; Gu, Goel, and RÃ© 2022), which leverages the fast Fourier transform (FFT) as primitive. With selectivity, SSMs are no-longer equivalent to convolution, but we leverage the parallel associative scan. While SSM scans are theoretically eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1195, tokens None, triggered by: 0.26\n",
      "\u001b[34mcient (ð (ð µð ¿ð ·ð ) FLOPs, scaling linear in ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1196, tokens None, triggered by: 0.22\n",
      "\u001b[35m¿), training foundation models with selective SSMs requires them to be eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1197, tokens None, triggered by: 0.21\n",
      "\u001b[31mcient on modern hardware (GPUs) as well. We describe how we use kernel fusion and recomputation to make SSM scan fast and memory-eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1198, tokens None, triggered by: 0.14\n",
      "\u001b[32mcient.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1199, tokens None, triggered by: 0.23\n",
      "\u001b[34mWe evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5, showing that it is up to 7Ã times faster than attention at sequence length 32K, and is as memory-eï¬ cient as the best attention implementation (FlashAttention).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1200, tokens None, triggered by: 0.27\n",
      "\u001b[35mSpeed.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1201, tokens None, triggered by: 0.30\n",
      "\u001b[31mOn modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by memory-bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This the case with our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to signiï¬ cant speedup compared to a standard implementation. The standard way to implement the scan algorithm in Section 3.2 is to prepare the scan input A, B of size (ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1202, tokens None, triggered by: 0.25\n",
      "\u001b[32mµ, ð ¿, ð ·, ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1203, tokens None, triggered by: 0.30\n",
      "\u001b[34m) in GPU HBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel associative scan implementation to write the scan output of size (ð µ, ð ¿, ð ·, ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1204, tokens None, triggered by: 0.21\n",
      "\u001b[35m) to GPU HBM, then multiply that scan output with C to produce an output of size (ð µ, ð ¿, ð ·).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1205, tokens None, triggered by: 0.24\n",
      "\u001b[31mHowever, this requires the number of memory reads/writes on the order of ð (ð µð ¿ð ·ð ).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1206, tokens None, triggered by: 0.18\n",
      "\u001b[32mWe can instead fuse the discretization step, the scan, and the multiplication with C into one kernel:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1207, tokens None, triggered by: 0.18\n",
      "\u001b[34m1.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1208, tokens None, triggered by: 0.26\n",
      "\u001b[35mWe read in ð (ð µð ¿ð · + ð ·ð ) bytes of memory (â , A, B, C) from slow HBM to fast SRAM.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1209, tokens None, triggered by: 0.20\n",
      "\u001b[31m2.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1210, tokens None, triggered by: 0.25\n",
      "\u001b[32mWe discretize to produce A, B of size (ð µ, ð ¿, ð ·, ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1211, tokens None, triggered by: 0.20\n",
      "\u001b[34m) in SRAM. 3.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1212, tokens None, triggered by: 0.25\n",
      "\u001b[35mWe perform a parallel associative scan, yielding intermediate states of size (ð µ, ð ¿, ð ·, ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1213, tokens None, triggered by: 0.29\n",
      "\u001b[31m) in SRAM.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1214, tokens None, triggered by: 0.18\n",
      "\u001b[32m4.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1215, tokens None, triggered by: 0.26\n",
      "\u001b[34mWe multiply and sum with C, producing outputs of size (ð µ, ð ¿, ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1216, tokens None, triggered by: 0.28\n",
      "\u001b[35m·) and write it to HBM.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1217, tokens None, triggered by: 0.21\n",
      "\u001b[31mThis way, we reduce IOs by a factor of ð (ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1218, tokens None, triggered by: 0.08\n",
      "\u001b[32m) (the state dimension), which in practice speeds up the operation by 20-40 times (Section 4.5).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1219, tokens None, triggered by: 0.14\n",
      "\u001b[34m28\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1220, tokens None, triggered by: 0.21\n",
      "\u001b[35mTable 11: (Induction heads.) Models are trained on sequence length 2Â° = 256, and tested on various sequence lengths of 2Â° = 64 up to 2Â° = 1048576. Y denotes perfect generalization accuracy, while X denotes out of memory. Model Params Test Accuracy (%) at Sequence Length 26 7 28 29 210 gl 212 913 214915216 917918919920 MHA-Abs 137K v 99.6 100.0 58.6 266 188 98 10.9 7.8 X x x x x x MHA-RoPE = 137K v v 100.0 83.6 31.3 184 8.6 9.0 5.5 xX x x x x x MHA-xPos 137K v v 100.0 99.6 67.6 254 7.0 9.0 78 =X x x x x x H3 153K v v 100.0 80.9 39.5 238 148 82 59 66 82 47 82 63 74 Hyena 69M* 977 Vo 100.0 Vv 441 125 66 5.1 70 #59 66 66 59 63 98 Mamba 74K v v 100.0 Vv v v v v v v v v v v v\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1221, tokens None, triggered by: 0.07\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1222, tokens None, triggered by: 0.16\n",
      "\u001b[32mMost of the parameters are in learnable positional encodings.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1223, tokens None, triggered by: 0.18\n",
      "\u001b[34mFor sequence length ð ¿ too long where we cannot ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1224, tokens None, triggered by: 0.21\n",
      "\u001b[35mt the sequence in SRAM (which is much smaller than HBM), we split the sequences into chunks and perform the fused scan on each chunk. As long as we have the intermediate scan states, we can continue the scan with the next chunk.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1225, tokens None, triggered by: 0.30\n",
      "\u001b[31mMemory. We describe how we use the classical technique of recomputation to reduce the total amount of memory required to train selective SSM layers.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1226, tokens None, triggered by: 0.11\n",
      "\u001b[32mFrom the way we fuse the forward pass, we do not save the intermediate states of size (ð µ, ð ¿, ð ·, ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1227, tokens None, triggered by: 0.28\n",
      "\u001b[34m) to avoid memory blowup.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1228, tokens None, triggered by: 0.28\n",
      "\u001b[35mHowever, these intermediate states are necessary for the backward pass to compute gradients. We instead recompute those intermediate states in the backward pass.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1229, tokens None, triggered by: 0.24\n",
      "\u001b[31mSince the inputs â , A, B, C and output gradient read from HBM to SRAM are of size ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1230, tokens None, triggered by: 0.29\n",
      "\u001b[32m(ð µð ¿ð + ð ·ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1231, tokens None, triggered by: 0.29\n",
      "\u001b[34m), and the input gradients are also of size ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1232, tokens None, triggered by: 0.15\n",
      "\u001b[35m(ð µð ¿ð + ð ·ð ), recomputation avoids the cost of reading ð (ð µð ¿ð ð ·) elements from HBM. This means that recomputation of the SSM states in the backward pass speeds up the computation compared to storing them and reading them from HBM. Beyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize the memory requirement of the entire selective SSM block (input projection, convolution, activation, scan, output projection). In particular, we do not save intermediate activations that take a lot of memory but are fast to recompute (e.g. output of activation function or short convolution). As a result, the selective SSM layer has the same memory requirement as an optimized Transformer implementation with FlashAttention. In particular, each attention layer (FlashAttention) stores around 12 bytes of activations per token, an each MLP layer stores around 20 bytes of activations per token, for a total of 32 bytes ((assuming mixed-precision training in FP16 or BF16)). Each selective SSM stores around 16 bytes of activations per token. Hence two layers of selective SSMs have around the same activation memory as an attention layer and an MLP layer.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1233, tokens None, triggered by: 0.28\n",
      "\u001b[31m# E Experimental Details and Additional Results # E.1 Synthetic Tasks\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1234, tokens None, triggered by: 0.14\n",
      "\u001b[32mSelective Copying.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1235, tokens None, triggered by: 0.20\n",
      "\u001b[34mOur setting is on sequences of length 4096, with a vocab size of 16 possible tokens (including the white â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1236, tokens None, triggered by: 0.19\n",
      "\u001b[35mnoiseâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1237, tokens None, triggered by: 0.24\n",
      "\u001b[31mtoken from Figure 2) and requiring models to memorize 16 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1238, tokens None, triggered by: 0.21\n",
      "\u001b[32mdataâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1239, tokens None, triggered by: 0.09\n",
      "\u001b[34mtokens.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1240, tokens None, triggered by: 0.20\n",
      "\u001b[35mWe use 2 layer models with a model dimension of ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1241, tokens None, triggered by: 0.24\n",
      "\u001b[31m· = 64.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1242, tokens None, triggered by: 0.14\n",
      "\u001b[32mModels are trained for 400K steps at a constant learning rate of 0.0001 with a batch size of 64.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1243, tokens None, triggered by: 0.16\n",
      "\u001b[34mInduction Heads.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1244, tokens None, triggered by: 0.14\n",
      "\u001b[35mTraining consists of randomly generating data every step, with a batch size of 8.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1245, tokens None, triggered by: 0.29\n",
      "\u001b[31mWe choose an â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1246, tokens None, triggered by: 0.16\n",
      "\u001b[32mepochâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1247, tokens None, triggered by: 0.29\n",
      "\u001b[34msize of 8192 steps, and track the accuracy on ï¬ xed validation sets (also randomly generated) of each target sequence length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1248, tokens None, triggered by: 0.13\n",
      "\u001b[35mFor the MHA-Abs and Mamba models, results are reported after the 25th epoch (8192 Ã 25 = 204800 steps). For the MHA-RoPE and MHA-xPos models, results are reported after the 50th epoch (8192 Ã 50 = 409600 steps). For the LTI H3 and Hyena models, results are reported after the 10th epoch (81920 steps) because they had converged by then and failed to improve further.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1249, tokens None, triggered by: 0.12\n",
      "\u001b[31m29\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1250, tokens None, triggered by: 0.11\n",
      "\u001b[32mTable 12: (Scaling Law Model Sizes.) Our model sizes and hyperparameters for scaling experiments. (Model dimension and number of heads applies only to Transformer models.)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1251, tokens None, triggered by: 0.17\n",
      "\u001b[34mParams ð _ð ð ð ¢ð ð ð ð _ð ð ð ð ð ð _ð ð ð ð ð / ð _ð ð ð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1252, tokens None, triggered by: 0.14\n",
      "\u001b[35mTraining steps Learning Rate Batch Size Tokens 125M 350M 760M 1.3B 12 24 24 24 768 1024 1536 2048 12 / 64 16 / 64 16 / 96 32 / 64 4800 13500 29000 50000 6e-4 3e-4 2.5e-4 2e-4 0.5M tokens 0.5M tokens 0.5M tokens 0.5M tokens 2.5B 7B 15B 26B We use the Adam optimizer with no weight decay. All models are trained at constant learning rates 2ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1253, tokens None, triggered by: 0.20\n",
      "\u001b[31mâ 4 and 1ð â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1254, tokens None, triggered by: 0.20\n",
      "\u001b[32m3, and the better results are reported for each model (2ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1255, tokens None, triggered by: 0.19\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1256, tokens None, triggered by: 0.28\n",
      "\u001b[35m4 for all models except Mamba).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1257, tokens None, triggered by: 0.18\n",
      "\u001b[31mThe attention and Hyena models did not learn at LR 1ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1258, tokens None, triggered by: 0.25\n",
      "\u001b[32mâ 3.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1259, tokens None, triggered by: 0.19\n",
      "\u001b[34mH3 learned at both LRs, but interestingly generalized better to shorter sequences at the smaller LR of 2ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1260, tokens None, triggered by: 0.19\n",
      "\u001b[35mâ 4.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1261, tokens None, triggered by: 0.18\n",
      "\u001b[31mMamba learned at both LRs, but extrapolated better at the larger LR of 1ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1262, tokens None, triggered by: 0.28\n",
      "\u001b[32mâ 3.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1263, tokens None, triggered by: 0.16\n",
      "\u001b[34m# E.2 Language Modeling # E.2.1 Scaling Law Details\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1264, tokens None, triggered by: 0.25\n",
      "\u001b[35mAll models were trained on the Pile. Model Sizes.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1265, tokens None, triggered by: 0.18\n",
      "\u001b[31mTable 12 speciï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1266, tokens None, triggered by: 0.16\n",
      "\u001b[32mes the model sizes we use for scaling laws.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1267, tokens None, triggered by: 0.09\n",
      "\u001b[34mThis is taken directly from the GPT3 speciï¬ cations (Brown et al. 2020), with very minor modiï¬ cations.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1268, tokens None, triggered by: 0.25\n",
      "\u001b[35mFirst, we changed the batch size of the 1.3B model from 1M tokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch size. Second, we changed the number of training steps and total tokens to roughly match Chinchilla scaling laws (Hoï¬ mann et al. 2022), which specify that training tokens should increase proportionally to model size.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1269, tokens None, triggered by: 0.29\n",
      "\u001b[31mTraining Recipes.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1270, tokens None, triggered by: 0.09\n",
      "\u001b[32mAll models used the AdamW optimizer with\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1271, tokens None, triggered by: 0.17\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1272, tokens None, triggered by: 0.17\n",
      "\u001b[35m¢ gradient clip value 1.0\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1273, tokens None, triggered by: 0.23\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1274, tokens None, triggered by: 0.29\n",
      "\u001b[32m¢ weight decay 0.1\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1275, tokens None, triggered by: 0.14\n",
      "\u001b[34mno dropout linear learning rate warmup with cosine decay By default, the peak learning rate is the GPT3 speciï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1276, tokens None, triggered by: 0.16\n",
      "\u001b[35mcation.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1277, tokens None, triggered by: 0.22\n",
      "\u001b[31mWe give several models an â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1278, tokens None, triggered by: 0.21\n",
      "\u001b[32mimproved recipeâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1279, tokens None, triggered by: 0.14\n",
      "\u001b[34m, inspired by changes adopted by popular large language models such as PaLM (Chowdhery et al. 2023) and LLaMa (Touvron et al. 2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1280, tokens None, triggered by: 0.20\n",
      "\u001b[35mThese include:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1281, tokens None, triggered by: 0.17\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1282, tokens None, triggered by: 0.17\n",
      "\u001b[32m¢ linear learning rate warmup with cosine decay to 1ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1283, tokens None, triggered by: 0.24\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1284, tokens None, triggered by: 0.24\n",
      "\u001b[35m5, with a peak value of 5Ã the GPT3 value\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1285, tokens None, triggered by: 0.27\n",
      "\u001b[31mno linear bias terms\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1286, tokens None, triggered by: 0.12\n",
      "\u001b[32mRMSNorm instead of LayerNorm\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1287, tokens None, triggered by: 0.24\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1288, tokens None, triggered by: 0.05\n",
      "\u001b[35m¢ AdamW hyperparameter ð ½ = (.9, .95) (the GPT3 value) instead of the PyTorch default of ð ½ = (.9, .999)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1289, tokens None, triggered by: 0.24\n",
      "\u001b[31mArchitecture and Training Details.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1290, tokens None, triggered by: 0.25\n",
      "\u001b[32mOur models are: â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1291, tokens None, triggered by: 0.12\n",
      "\u001b[34m¢ Transformer: The standard Transformer based on GPT3 (Table 12).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1292, tokens None, triggered by: 0.25\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1293, tokens None, triggered by: 0.07\n",
      "\u001b[31m¢ Transformer++: A Transformer with an improved architecture, namely rotary positional encodings (Su et al. 2021) and SwiGLU MLP (Shazeer 2020), and the improved training recipe above.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1294, tokens None, triggered by: 0.28\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1295, tokens None, triggered by: 0.22\n",
      "\u001b[34m¢ Hyena: Interleaving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an MLP) with standard MLP blocks. The MLP blocks have expansion factor 2 instead of 4 and the number of layers is correspondingly increased by 1.5Ã\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1296, tokens None, triggered by: 0.17\n",
      "\u001b[35mto preserve parameter count.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1297, tokens None, triggered by: 0.29\n",
      "\u001b[31m30\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1298, tokens None, triggered by: 0.13\n",
      "\u001b[32mâ ¢ H3++: The H3 architecture with a few modiï¬ cations, including (i) using the same â thinâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1299, tokens None, triggered by: 0.14\n",
      "\u001b[34mHyena dimensions above (ii) the improved training recipe above (iii) a linear attention head dimension of 8.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1300, tokens None, triggered by: 0.25\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1301, tokens None, triggered by: 0.26\n",
      "\u001b[31m¢ RWKV: The default RWKV model from B.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1302, tokens None, triggered by: 0.19\n",
      "\u001b[32mPeng et al. (2023), including its modiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1303, tokens None, triggered by: 0.14\n",
      "\u001b[34med MLP block.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1304, tokens None, triggered by: 0.29\n",
      "\u001b[35mWe also used as much of its speciï¬ ed training recipe as possible, such as increasing the learning rates by 2Ã\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1305, tokens None, triggered by: 0.24\n",
      "\u001b[31mor 3Ã\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1306, tokens None, triggered by: 0.22\n",
      "\u001b[32mon certain parameters.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1307, tokens None, triggered by: 0.25\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1308, tokens None, triggered by: 0.21\n",
      "\u001b[35m¢ RetNet: The default RetNet model from Y.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1309, tokens None, triggered by: 0.12\n",
      "\u001b[31mSun et al. (2023).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1310, tokens None, triggered by: 0.11\n",
      "\u001b[32mWe also gave it the improved training recipe above.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1311, tokens None, triggered by: 0.27\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1312, tokens None, triggered by: 0.22\n",
      "\u001b[35m¢ Mamba: The standard Mamba architecture, with the improved training recipe.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1313, tokens None, triggered by: 0.30\n",
      "\u001b[31m# E.2.2 Additional Scaling Law Ablations We perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws in Figure 4 (Left). Mamba Architecture:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1314, tokens None, triggered by: 0.17\n",
      "\u001b[32mInterleaving Blocks.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1315, tokens None, triggered by: 0.14\n",
      "\u001b[34mWe test the eï¬ ect of diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1316, tokens None, triggered by: 0.20\n",
      "\u001b[35merent architectural blocks combined with the Mamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with an extra ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1317, tokens None, triggered by: 0.20\n",
      "\u001b[31m¼ð ð ð â ð ²ð ²ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1318, tokens None, triggered by: 0.14\n",
      "\u001b[32m¬ path added.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1319, tokens None, triggered by: 0.19\n",
      "\u001b[34mThis leads to two natural ablations:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1320, tokens None, triggered by: 0.10\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1321, tokens None, triggered by: 0.18\n",
      "\u001b[31m¢ What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This can also be interpreted as taking Mamba and removing half of the SSMs.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1322, tokens None, triggered by: 0.15\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1323, tokens None, triggered by: 0.23\n",
      "\u001b[34m¢ What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as taking a Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks. Figure 9 (Right) shows these variants compared to the original (homogenous) Mamba architecture.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1324, tokens None, triggered by: 0.23\n",
      "\u001b[35mInterestingly, neither change matters too much.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1325, tokens None, triggered by: 0.15\n",
      "\u001b[31mThe Mamba-MLP architecture is only slightly worse, and still better than all models except Transformer++. The Mamba-MHA architecture is only slightly better, which is somewhat surprising in light of the fact that many recent works have found that combining (LTI) SSMs with Attention can lead to substantial improvements (Dao, Fu, Saab, et al. 2023; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta, and Cui 2023; Zuo et al. 2022). H3 Architecture:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1326, tokens None, triggered by: 0.15\n",
      "\u001b[32mTraining Recipes.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1327, tokens None, triggered by: 0.25\n",
      "\u001b[34mNext we ablate diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1328, tokens None, triggered by: 0.22\n",
      "\u001b[35merences between the Hyena and H3++ models, our weakest and strongest models outside of Transformer++ and Mamba, particularly to isolate the eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1329, tokens None, triggered by: 0.13\n",
      "\u001b[31mect of training recipes.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1330, tokens None, triggered by: 0.28\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1331, tokens None, triggered by: 0.10\n",
      "\u001b[34m¢ Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1332, tokens None, triggered by: 0.27\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1333, tokens None, triggered by: 0.11\n",
      "\u001b[31m¢ Hyena+:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1334, tokens None, triggered by: 0.09\n",
      "\u001b[32mThe same architecture but with the improved training recipe described above.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1335, tokens None, triggered by: 0.17\n",
      "\u001b[34mâ ¢ H3+:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1336, tokens None, triggered by: 0.11\n",
      "\u001b[35mThe same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution kernel.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1337, tokens None, triggered by: 0.23\n",
      "\u001b[31mâ ¢ H3++: The same as H3+, but with a linear attention head dimension of 8.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1338, tokens None, triggered by: 0.15\n",
      "\u001b[32mThis increases computation inside the SSM recurrence but does not increase parameters.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1339, tokens None, triggered by: 0.18\n",
      "\u001b[34mOur general convention is that â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1340, tokens None, triggered by: 0.22\n",
      "\u001b[35mModel+â represents the base model with the improved training recipe, and â Model++â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1341, tokens None, triggered by: 0.22\n",
      "\u001b[31malso allows for architectural changes.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1342, tokens None, triggered by: 0.20\n",
      "\u001b[32mFigure 9 (Right) shows that\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1343, tokens None, triggered by: 0.22\n",
      "\u001b[34mA large improvement is achieved by the improved training recipe, which was used for many of the models in the main Figure 4 (RetNet, H3++, Transformer++, Mamba).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1344, tokens None, triggered by: 0.15\n",
      "\u001b[35mThe choice of the inner LTI SSM does not matter (e.g.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1345, tokens None, triggered by: 0.10\n",
      "\u001b[31mHyena vs.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1346, tokens None, triggered by: 0.18\n",
      "\u001b[32mS4), consistent with ï¬ ndings throughout this\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1347, tokens None, triggered by: 0.11\n",
      "\u001b[34mpaper.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1348, tokens None, triggered by: 0.10\n",
      "\u001b[35mThe head dimension expansion improves performance, consistent with one of our main themes that expanded state dimension improves performance for SSMs (Section 3).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1349, tokens None, triggered by: 0.16\n",
      "\u001b[31m31\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1350, tokens None, triggered by: 0.19\n",
      "\u001b[32mScaling Laws on The Pile (Sequence Length 2048) Scaling Laws on The Pile (Sequence Length 2048) â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1351, tokens None, triggered by: 0.24\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1352, tokens None, triggered by: 0.21\n",
      "\u001b[35mMamba Hyena Mamba-mLp | = â Hyenas â â Members |g â â He a â He 3 Sox!\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1353, tokens None, triggered by: 0.22\n",
      "\u001b[31m= 2104 ext? 5 2S 7x0 Ea 1 1 1 1 10 30 10Â° 10â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1354, tokens None, triggered by: 0.24\n",
      "\u001b[32mFLOPS (log scale) FLOPs (log scale)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1355, tokens None, triggered by: 0.19\n",
      "\u001b[34ms 5 2 3 2 = 3 8\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1356, tokens None, triggered by: 0.23\n",
      "\u001b[35mFigure 9: (Scaling laws: extra ablations.) (Left) Instead of (Right) Instead of\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1357, tokens None, triggered by: 0.21\n",
      "\u001b[31m# E.2.3 Downstream Evaluation Details\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1358, tokens None, triggered by: 0.14\n",
      "\u001b[32mThis pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens. For the 1.3B model, we use a batch size of 1M tokens to be consistent with the GPT3 speciï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1359, tokens None, triggered by: 0.06\n",
      "\u001b[34mcations.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1360, tokens None, triggered by: 0.28\n",
      "\u001b[35mWe report the perplexity on the Pile validation set, and for this metric only compare to models trained on the same dataset and with the same tokenizer, in particular Pythia and RWKV. For downstream evaluation, we use the LM evaluation harness from EleutherAI (L. Gao, Tow, et al. 2021), as done by most work in this area.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1361, tokens None, triggered by: 0.11\n",
      "\u001b[31mWe evaluate on the following tasks/datasets that measure common sense reasoning:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1362, tokens None, triggered by: 0.23\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1363, tokens None, triggered by: 0.23\n",
      "\u001b[34m¢ LAMBADA (Paperno et al. 2016).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1364, tokens None, triggered by: 0.21\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1365, tokens None, triggered by: 0.21\n",
      "\u001b[31m¢ HellaSwag (Zellers et al. 2019).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1366, tokens None, triggered by: 0.25\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1367, tokens None, triggered by: 0.25\n",
      "\u001b[34m¢ PIQA (Bisk et al. 2020).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1368, tokens None, triggered by: 0.28\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1369, tokens None, triggered by: 0.30\n",
      "\u001b[31m¢ ARC-challenge (P.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1370, tokens None, triggered by: 0.16\n",
      "\u001b[32mClark et al. 2018).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1371, tokens None, triggered by: 0.22\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1372, tokens None, triggered by: 0.22\n",
      "\u001b[35m¢ ARC-easy: an easy subset of ARC-challenge.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1373, tokens None, triggered by: 0.20\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1374, tokens None, triggered by: 0.26\n",
      "\u001b[32m¢ WinoGrande (Sakaguchi et al. 2021). We report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence length for HellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these task).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1375, tokens None, triggered by: 0.29\n",
      "\u001b[34m# E.3 DNA Modeling # E.3.1 Pretraining Details We describe the dataset and training procedure of the HG38 pretraining task in more detail. The dataset follows the splits from the prior Enformer work on genomics (Avsec et al. 2021); the training split contains a total of ð = 34021 segments of length 217 = 131072 that cover the genome, for a total of approximately 4.5 billion tokens (DNA base pairs). These segments are pairs of (chromosome number, starting index, ending index), and can be extended if necessary (e.g. to get longer segments). We deviate from HyenaDNA when the training sequence length is not 217. HyenaDNA always takes a ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1376, tokens None, triggered by: 0.29\n",
      "\u001b[35mxed sub-segment (e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length each epoch is ï¬ xed to 34021 samples and doesnâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1377, tokens None, triggered by: 0.29\n",
      "\u001b[31mt necessarily go through the whole genome.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1378, tokens None, triggered by: 0.28\n",
      "\u001b[32mOn the other hand, we use the entire training data: â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1379, tokens None, triggered by: 0.29\n",
      "\u001b[34m¢ When the context length ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1380, tokens None, triggered by: 0.29\n",
      "\u001b[35m¿ is less than (or equal to) 217, we divide up each segment into non-overlapping sub-segments of length ð ¿, so that there are ð Ã 217 ð ¿ total samples and ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1381, tokens None, triggered by: 0.22\n",
      "\u001b[31mÃ 217 â 4.5ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1382, tokens None, triggered by: 0.15\n",
      "\u001b[32mµ tokens per epoch.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1383, tokens None, triggered by: 0.27\n",
      "\u001b[34mâ ¢ When the context length ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1384, tokens None, triggered by: 0.20\n",
      "\u001b[35m¿ is greater than 217, we turn each segment into two samples, one that begins with the prescribed segment and one that ends with the prescribed segment. Thus each epoch has 2ð items and 2ð ð ¿\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1385, tokens None, triggered by: 0.24\n",
      "\u001b[31m32\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1386, tokens None, triggered by: 0.24\n",
      "\u001b[32mtokens per epoch.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1387, tokens None, triggered by: 0.16\n",
      "\u001b[34mFor example, at sequence length 218 = 262144 there are 4Ã as many tokens as the default, and at sequence length 220 there are 16Ã as many tokens.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1388, tokens None, triggered by: 0.24\n",
      "\u001b[35mOther training details generally follow the same protocol as our language modeling experiments (Appendix E.2).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1389, tokens None, triggered by: 0.24\n",
      "\u001b[31mFor example, we use the AdamW with (ð ½1, ð ½2) = (0.9, 0.95), no dropout, weight decay 0.1. We use a cosine learning rate scheduler with linear warmup for 10% of total steps.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1390, tokens None, triggered by: 0.22\n",
      "\u001b[32m# E.3.2 Scaling: Model Size Details Models. The models we consider are: â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1391, tokens None, triggered by: 0.10\n",
      "\u001b[34m¢ Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su et al. 2021). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani et al. 2017).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1392, tokens None, triggered by: 0.14\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1393, tokens None, triggered by: 0.14\n",
      "\u001b[31m¢ HyenaDNA: the Hyena model from Nguyen, Poli, et al. (2023) and Poli et al. (2023), which is roughly a Transformer with the MHA block replaced by an H3 block using a global convolution parameterized by an MLP.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1394, tokens None, triggered by: 0.21\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1395, tokens None, triggered by: 0.25\n",
      "\u001b[34m¢ Mamba: the standard Mamba architecture. Model Sizes. We use the following model sizes. Blocks Model Dimension Params (Approx.) 4 64 250K 700K 1.4M 3.5M 7.0M 19.3M 40.7M 5 96 6 128 7 192 8 256 10 384 12 512 Note that the number of blocks for Mamba is doubled, because one Transformer â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1396, tokens None, triggered by: 0.25\n",
      "\u001b[35mlayerâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1397, tokens None, triggered by: 0.17\n",
      "\u001b[31mincludes both the MHA and MLP blocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1398, tokens None, triggered by: 0.29\n",
      "\u001b[32mTraining.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1399, tokens None, triggered by: 0.19\n",
      "\u001b[34mFor each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across {1ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1400, tokens None, triggered by: 0.27\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1401, tokens None, triggered by: 0.27\n",
      "\u001b[31m3, 2ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1402, tokens None, triggered by: 0.13\n",
      "\u001b[32mâ 3, 4ð â 3, 8ð â 3}.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1403, tokens None, triggered by: 0.15\n",
      "\u001b[34mThe optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal Mamba learning rate was 8e-3; note that Mamba performed better than baselines with matched learning rates (2e-3), but was more stable and improved even more at higher learning rates. (Furthermore, as this LR is on the upper range of the sweep, it is possible that our results are still suboptimal.) Note that, in contrast to standard LM scaling laws (Table 12), our LR held constant across model sizes for simplicity. The optimal LR should go down for larger models, but we didnâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1404, tokens None, triggered by: 0.23\n",
      "\u001b[35mt ï¬ nd a noticeable eï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1405, tokens None, triggered by: 0.30\n",
      "\u001b[31mect at the small model sizes (at most a few million parameters) we considered. E.3.3 Scaling:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1406, tokens None, triggered by: 0.25\n",
      "\u001b[32mContext Length Details We use a total batch size of 224 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1407, tokens None, triggered by: 0.26\n",
      "\u001b[34m16ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1408, tokens None, triggered by: 0.15\n",
      "\u001b[35mtokens per training step, for every sequence length (e.g. at length 220 there are 16 segments per batch and at length 210 there are 16384 segments per batch). This is a large batch size relative to the model size by usual LM standards, but note that a batch size of 223 is the minimum possible on a machine with 8 GPUs and sequence length of 220, and that HyenaDNA used much larger batches of 228. The learning rate used was 0.008 for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same learning rate of 0.002 from the previous section for HyenaDNA, but found that it was unstable at the longest context length. Sequence Length Warmup. Following (Nguyen, Poli, et al. 2023), we use sequence length warmup (SLW) during pretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from 210 = 1024. (Note that because of how data is curated, at the longest sequence lengths more steps and tokens are spent proportionally. In particular, each stage up to length 217 processes the same number of tokens, but 4Ã as many tokens are processed at length 218, 8Ã as many at length 219, and 16Ã as many at length 220.)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1409, tokens None, triggered by: 0.15\n",
      "\u001b[31mUnlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successively halved as the sequence lengths are doubled in each stage.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1410, tokens None, triggered by: 0.15\n",
      "\u001b[32m33\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1411, tokens None, triggered by: 0.24\n",
      "\u001b[34mTable 13: (Great Apes DNA Classification.) Accuracy after fine-tuning on sequences of length 210 = 1024 up to 220 = 1048576 using pretrained models of the same context length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1412, tokens None, triggered by: 0.26\n",
      "\u001b[35mRandom guessing is 20%. Params Accuracy (%) at Sequence Length 210 212 214 216 218 220 28.04 31.47 28.43 27.50 41.17 27.66 42.22 40.72 31.10 42.41 7M 30.00 29.01 31.48 43.73 56.60\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1413, tokens None, triggered by: 0.13\n",
      "\u001b[31mRemark E.1.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1414, tokens None, triggered by: 0.14\n",
      "\u001b[32mWe also note that the schedule was not tuned, and we never experimented with turning off sequence length warmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at similar lengths (Section 4.4), and it is possible that it is not necessary for DNA pretraining either.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1415, tokens None, triggered by: 0.19\n",
      "\u001b[34m# E.3.4 Species (Great Apes) Classification\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1416, tokens None, triggered by: 0.26\n",
      "\u001b[35mModels are causal and therefore only the last element (across the sequence length) of the modelâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1417, tokens None, triggered by: 0.12\n",
      "\u001b[31ms output is used for the classiï¬ cation head.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1418, tokens None, triggered by: 0.21\n",
      "\u001b[32mNote that we control for the total number of elements in the loss function per gradient step. The pretraining objective includes all positions across the sequence length, so that ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1419, tokens None, triggered by: 0.19\n",
      "\u001b[34mð ð ð ð _ð ð ð £ð Ã ð ð ð ð ð ð ð ð _ð ð ð ð ð ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1420, tokens None, triggered by: 0.16\n",
      "\u001b[35mis held constant; in other words, the batch size decreases as the sequence length increases.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1421, tokens None, triggered by: 0.17\n",
      "\u001b[31mHowever, for a classiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1422, tokens None, triggered by: 0.23\n",
      "\u001b[32mcation task, since only the last position enters the loss, the batch size itself is held constant.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1423, tokens None, triggered by: 0.19\n",
      "\u001b[34mNote that this also means that ï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1424, tokens None, triggered by: 0.30\n",
      "\u001b[35mne-tuning models with longer sequence lengths is more computationally expensive.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1425, tokens None, triggered by: 0.24\n",
      "\u001b[31mTraining consists of 10 epochs, each of which has 1024 gradient steps. Each gradient step uses batch size 64, which are all independently randomly drawn by uniformly picking a species, uniformly picking a chromosome, and then uniformly picking a contiguous segment of DNA. Following (Nguyen, Poli, et al. 2023), models with a maximum context length greater than 214 = 16384 use sequence length warmup with 1 epoch at length 214 = 16384, 1 epoch at length 215 = 32768, 1 epoch at length 216 = 65536, and so on up to the maximum sequence length. For example, the model with 220 = 1048576 context undergoes 6 epochs of sequence length warmup before 4 more epochs at its maximum sequence length. The learning rate for all Hyena models is ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1426, tokens None, triggered by: 0.25\n",
      "\u001b[32mºð â ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1427, tokens None, triggered by: 0.25\n",
      "\u001b[34m», while the learning rate for all Mamba models is ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1428, tokens None, triggered by: 0.30\n",
      "\u001b[35m·ð â ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1429, tokens None, triggered by: 0.16\n",
      "\u001b[31mº.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1430, tokens None, triggered by: 0.20\n",
      "\u001b[32mThese were found by performing learning rate sweeps for each model among {1ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1431, tokens None, triggered by: 0.28\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1432, tokens None, triggered by: 0.28\n",
      "\u001b[35m5, 2ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1433, tokens None, triggered by: 0.30\n",
      "\u001b[31mâ 5, 4ð â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1434, tokens None, triggered by: 0.30\n",
      "\u001b[32m5, 1ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1435, tokens None, triggered by: 0.28\n",
      "\u001b[34mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1436, tokens None, triggered by: 0.28\n",
      "\u001b[35m4, 2ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1437, tokens None, triggered by: 0.12\n",
      "\u001b[31mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1438, tokens None, triggered by: 0.12\n",
      "\u001b[32m4} for the smaller sequence lengths (210, 212, 214, 216), and these values were consistently found to be the best for each model. An abridged learning rate sweep was done at length 218, which agreed with these values, and a single run at length 220 was performed (as described above, the computational cost of these experiments is proportional to the sequence length). The learning rate followed a cosine decay schedule with warmup with 5 epochs of linear warmup to the maximum learning rate, and 5 epochs of cosine decay down to 1ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1439, tokens None, triggered by: 0.16\n",
      "\u001b[34mâ 6.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1440, tokens None, triggered by: 0.11\n",
      "\u001b[35mThe unusually long learning rate warmup schedule was chosen because the sequence length warmup was also long (e.g. comprising 6 out of 10 epochs for the model with context length 220); we did not experiment with this choice.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1441, tokens None, triggered by: 0.29\n",
      "\u001b[31mResults for the Species classiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1442, tokens None, triggered by: 0.26\n",
      "\u001b[32mcation task are in Table 13.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1443, tokens None, triggered by: 0.18\n",
      "\u001b[34m# E.4 Audio Details # E.4.1 YouTubeMix Audio Pretraining\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1444, tokens None, triggered by: 0.29\n",
      "\u001b[35mModel. We use a model with 3 blocks per stage (3 Ã 5 = 15 total Mamba blocks), pooling factor ð = 16, and outer dimension ð · = 64, for about 3.5M parameters.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1445, tokens None, triggered by: 0.25\n",
      "\u001b[31mDataset. The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of 256. The dataset consists of clips of up to 1 minute long, or length 960000, which is subsampled and divided into segments of any desired sequence length.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1446, tokens None, triggered by: 0.21\n",
      "\u001b[32mSince the architecture involves two stages of pooling by a factor of 16,\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1447, tokens None, triggered by: 0.26\n",
      "\u001b[34m34\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1448, tokens None, triggered by: 0.12\n",
      "\u001b[35mTable 14:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1449, tokens None, triggered by: 0.13\n",
      "\u001b[31mYouTubeMix length scaling sequence lengths and batch sizes.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1450, tokens None, triggered by: 0.21\n",
      "\u001b[32m468 Ã 2048 = 958464 234 Ã 2048 = 479232 117 Ã 2048 = 239616 59 Ã 2048 = 120832 30 Ã 2048 = 61440 15 Ã 2048 = 30720 8 Ã 2048 = 16384 4 Ã 2048 = 8192 1 2 4 8 16 32 64 128 958464 958464 958464 966656 983040 983040 1048576 1048576\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1451, tokens None, triggered by: 0.23\n",
      "\u001b[34mAudio Waveforms - SSM Parameterization aso â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1452, tokens None, triggered by: 0.30\n",
      "\u001b[35mâ samp â â Mamba (s6) = â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1453, tokens None, triggered by: 0.21\n",
      "\u001b[31msy = sSeaive B/C Â° 1.40 4 â â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1454, tokens None, triggered by: 0.26\n",
      "\u001b[32m-selective A s ras | __Mamba-$4) B 1204 124 108 108 Sequence Length\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1455, tokens None, triggered by: 0.21\n",
      "\u001b[34mAudio Waveforms - SSM Parameterization â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1456, tokens None, triggered by: 0.23\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1457, tokens None, triggered by: 0.23\n",
      "\u001b[31mMamba ($6) 4 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1458, tokens None, triggered by: 0.20\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1459, tokens None, triggered by: 0.29\n",
      "\u001b[34m+complex = Solestive a | (Mamba-S4) 1.35 1.304 1.254 108 108 Sequence Length\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1460, tokens None, triggered by: 0.18\n",
      "\u001b[35m1.48 21404 . Ã© ag\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1461, tokens None, triggered by: 0.27\n",
      "\u001b[31mFigure 10: (Audio Pretraining (YouTubeMix) Ablations.) As a uniformly-sampled â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1462, tokens None, triggered by: 0.17\n",
      "\u001b[32mcontinuousâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1463, tokens None, triggered by: 0.23\n",
      "\u001b[34msignal modality, audio wave- forms actually benefit from LTI models which have matching inductive bias. (Left) Homogenous models (all blocks have the same parameterization) (Right) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1464, tokens None, triggered by: 0.11\n",
      "\u001b[35mPurple line is same as figure on left.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1465, tokens None, triggered by: 0.20\n",
      "\u001b[31mand we want the resulting sequence length to be a a multiple of 8 for hardware eï¬ ciency, the longest possible sequence is 468 Ã 2048 = 958464.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1466, tokens None, triggered by: 0.29\n",
      "\u001b[32mThe rest of our sequence lengths are deï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1467, tokens None, triggered by: 0.17\n",
      "\u001b[34mned by successively halving this and rounding up to the nearest multiple of 2048.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1468, tokens None, triggered by: 0.17\n",
      "\u001b[35mTable 14 lists the speciï¬ cations used in Figure 7.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1469, tokens None, triggered by: 0.13\n",
      "\u001b[31mBeyond the varying batch sizes, the number of valid segments in the training set varied between diï¬ erent sequence lengths (e.g. the number of training steps per epoch was not constant for diï¬ erent points in the graph), which may have contributed to kinks in the scaling curves.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1470, tokens None, triggered by: 0.24\n",
      "\u001b[32mTraining. Models were trained for 200ð ¾ training steps with a maximum learning rate of 0.002, 20ð ¾ (10%) warmup steps, and weight decay 0.1 (similar to our general pretraining recipe across domains).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1471, tokens None, triggered by: 0.26\n",
      "\u001b[34mAdditional Ablations:\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1472, tokens None, triggered by: 0.15\n",
      "\u001b[35mSSM Parameterizations. We investigate SSM parameterizations on long-form audio waveform pretraining in the setting of Figure 7.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1473, tokens None, triggered by: 0.21\n",
      "\u001b[31mThe setting is modiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1474, tokens None, triggered by: 0.29\n",
      "\u001b[32med slightly to use larger models (8 layers and ð · = 64 for 6M params, the SaShiMi default), shorter sequences (211 = 2048 to 218 = 262144 instead of 213 to 220), lower LR (0.001 from 0.002), and shorter training cycles (100K instead of 200K steps).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1475, tokens None, triggered by: 0.25\n",
      "\u001b[34mFigure 10 shows that the change from S4 â S6 (i.e. the selection mechanism) is not always beneï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1476, tokens None, triggered by: 0.19\n",
      "\u001b[35mcial.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1477, tokens None, triggered by: 0.17\n",
      "\u001b[31mOn long-form audio waveforms, it in fact signiï¬ cantly hampers performance, which may be intuitive from the point of view that audio is uniformly sampled and very smooth, and therefore beneï¬ ts from continuous linear time-invariant (LTI) methods.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1478, tokens None, triggered by: 0.11\n",
      "\u001b[32mAfter ablating away the selection mechanism, note that the resulting model is the S4 layer inside the Mamba block. To disambiguate, we call this Mamba-S4 as opposed the default Mamba architecture Mamba-S6. However, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1479, tokens None, triggered by: 0.22\n",
      "\u001b[34mThe performance diï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1480, tokens None, triggered by: 0.26\n",
      "\u001b[35merences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio signal should be LTI, but once they are â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1481, tokens None, triggered by: 0.24\n",
      "\u001b[31mtokenizedâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1482, tokens None, triggered by: 0.24\n",
      "\u001b[32mand compressed by the outer layers, the inner layers no longer need to be LTI.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1483, tokens None, triggered by: 0.06\n",
      "\u001b[34mIn this setting however, the real-valued SSM still underperforms the complex-valued one.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1484, tokens None, triggered by: 0.09\n",
      "\u001b[35m35\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1485, tokens None, triggered by: 0.13\n",
      "\u001b[31m# E.4.2 SC09 Speech Generation Autoregressive training largely followed the autoregressive language modeling protocol, such as\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1486, tokens None, triggered by: 0.21\n",
      "\u001b[32mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1487, tokens None, triggered by: 0.21\n",
      "\u001b[34m¢ Weight decay 0.1\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1488, tokens None, triggered by: 0.11\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1489, tokens None, triggered by: 0.11\n",
      "\u001b[31m¢ Learning rate warmup for 10% of total steps\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1490, tokens None, triggered by: 0.27\n",
      "\u001b[32mâ ¢ AdamW optimizer with ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1491, tokens None, triggered by: 0.23\n",
      "\u001b[34m½ = (0.9, 0.95)\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1492, tokens None, triggered by: 0.14\n",
      "\u001b[35mâ\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1493, tokens None, triggered by: 0.23\n",
      "\u001b[31m¢ Gradient clip value 0.1\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1494, tokens None, triggered by: 0.27\n",
      "\u001b[32mWe used a learning rate of 0.002 and 200000 training steps at a batch size of 16.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1495, tokens None, triggered by: 0.28\n",
      "\u001b[34mThe large Mamba model in Table 4 has 15 layers per stage with an outer dimension of ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1496, tokens None, triggered by: 0.20\n",
      "\u001b[35m· = 96 and pooling factor 4.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1497, tokens None, triggered by: 0.28\n",
      "\u001b[31mWe note that this dataset is small (training went through 100 epochs) and for this large model, there was signiï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1498, tokens None, triggered by: 0.16\n",
      "\u001b[32mcant overï¬\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1499, tokens None, triggered by: 0.12\n",
      "\u001b[34mtting of the BPB or NLL.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1500, tokens None, triggered by: 0.20\n",
      "\u001b[35mHowever, automated metrics of generated samples continually improving throughout training.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1501, tokens None, triggered by: 0.29\n",
      "\u001b[31mThe models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1502, tokens None, triggered by: 0.27\n",
      "\u001b[32m³ = 64 and pooling factor 4. The S4+MLP block has roughly 2ð ·2 + 4ð ·2 parameters (expansion factor 2 in the MLP).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1503, tokens None, triggered by: 0.18\n",
      "\u001b[34mThe Transformer block has 4ð ·2 + 2ð ·2 parameters (expansion factor 1 in the MLP).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1504, tokens None, triggered by: 0.20\n",
      "\u001b[35mThe Mamba block has the usual â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1505, tokens None, triggered by: 0.25\n",
      "\u001b[31m6ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1506, tokens None, triggered by: 0.22\n",
      "\u001b[32m·2 parameters. All models have roughly 6M total parameters.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1507, tokens None, triggered by: 0.20\n",
      "\u001b[34m# E.5 Efficiency Benchmark\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1508, tokens None, triggered by: 0.17\n",
      "\u001b[35mScan Operation. We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3), against convolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost of other operations outside of this core operation, such as computing the convolutional kernel in global-convolution models, or computing the QKV projections in attention. As a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1509, tokens None, triggered by: 0.29\n",
      "\u001b[31mThis requires materializing the parameters A, B, C in HBM. Our scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all the large parameters in HBM.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1510, tokens None, triggered by: 0.29\n",
      "\u001b[32mFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs and the ï¬ lters, multiply them in frequency domain, then performs an inverse FFT to obtain the result.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1511, tokens None, triggered by: 0.11\n",
      "\u001b[34mThe theoretical complexity is ð (ð ¿ log(ð ¿)) for sequence length ð ¿.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1512, tokens None, triggered by: 0.20\n",
      "\u001b[35mFor attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2023)), with causal mask. Note that FlashAttention-2 with causal mask is about 1.7Ã faster than without causal mask, since approximately only half of the attention entries are computed.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1513, tokens None, triggered by: 0.25\n",
      "\u001b[31mWe use batch size of 1 and increase the sequence length from 29 = 512, 210 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1514, tokens None, triggered by: 0.27\n",
      "\u001b[32m1ð ¾, 211 â 2ð\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1515, tokens None, triggered by: 0.30\n",
      "\u001b[34m¾, up to 219 â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1516, tokens None, triggered by: 0.22\n",
      "\u001b[35m500ð ¾ (some of the baselines run out of memory before reaching 500K).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1517, tokens None, triggered by: 0.27\n",
      "\u001b[31mWe use a model dimension of ð · = 1024 and state dimension ð = 16. We measure with BF16 inputs, which is the data type most commonly used for large scale training.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1518, tokens None, triggered by: 0.15\n",
      "\u001b[32mEnd-to-end Inference. We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba 6.9B model, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard Transformer implementation in the Huggingface transformers library.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1519, tokens None, triggered by: 0.27\n",
      "\u001b[34mWe set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16, 32, 64, to 128, and measure time time taken to generate 128 tokens. We then calculate the throughput (tokens/s) as batch size Ã\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1520, tokens None, triggered by: 0.24\n",
      "\u001b[35m128â\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1521, tokens None, triggered by: 0.14\n",
      "\u001b[31mtime taken. We repeat the measurements 3 times and take the average. Measurements are done on an A100 80GB PCIe GPU. Memory Benchmark. The memory usage simply scales proportionally to the size of the activation tensors, as with most deep sequence models. We report measurements of the training memory requirements of 125M models\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1522, tokens None, triggered by: 0.21\n",
      "\u001b[32m36\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1523, tokens None, triggered by: 0.25\n",
      "\u001b[34mTable 15: (Memory benchmark.) Mambaâ s memory footprint is comparable to the most optimized Transformer.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1524, tokens None, triggered by: 0.13\n",
      "\u001b[35mResults for 125M models. Batch size Transformer (w/ FlashAttention-2) Mamba 1 2 4 8 16 32 4.6GB 5.2GB 6.9GB 11.5GB 20.7GB 34.5GB 4.8GB 5.8GB 7.3GB 12.3GB 23.1GB 38.2GB on 1 A100 80GB GPU.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1525, tokens None, triggered by: 0.22\n",
      "\u001b[31mEach batch consists of sequences of length 2048.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1526, tokens None, triggered by: 0.13\n",
      "\u001b[32mWe compare to the most memory-eï¬ cient Transformer implementation we are aware of (with kernel fusion from torch.compile and with FlashAttention-2).\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1527, tokens None, triggered by: 0.10\n",
      "\u001b[34mTable 15 shows that Mambaâ s memory requirement is comparable to a similar-sized Transformer with an extremely optimized implementation, and we expect further improvement in Mambaâ s memory footprint in the future.\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Split 1528, tokens None, triggered by: final split\n",
      "\u001b[35m37\u001b[0m\n",
      "----------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunker.print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f16a0",
   "metadata": {},
   "source": [
    "CUMULATIVE SEMANTIC CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f57b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import CumulativeChunker\n",
    "chunker = CumulativeChunker(encoder=encoder, score_threshold=0.3)\n",
    "chunks = chunker(docs=[content])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
